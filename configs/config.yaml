data:
  path: /home/shawley/datasets/POP909_images_basic  # adjust path as needed
  image_size: 128
  in_channels: 1

model:
  patch_size: 16
  dim: 256
  depth: 4
  heads: 8
  dec_depth: 6   # decoder vit 
  dec_heads: 8

training:
  batch_size: 768  # encoder batch size
  lr: 2e-4
  epochs: 1000
  lambd: 0.3
  max_shift_x: 12   # will increase w/epoch via curriculum
  max_shift_y: 12
  mae_lambda: 0.1
  dec_batch_size: 256   # decoder-training batch size
  dec_epochs: 100
  dec_lr: 5e-5
  gan_warmup: 100000   # number of epochs before adversarial training starts
  enc_ft_lr: 5e-5  # lr for fine-tuning enc via end-to-end training w/ decoder

preencode:
  output_dir: /home/shawley/datasets/POP909_encoded
  num_passes: 10  # number of passes through the dataset

wandb:
  project: vit-midi-rae