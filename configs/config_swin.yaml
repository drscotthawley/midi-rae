data:
  path: /home/shawley/datasets/POP909_images_basic  # adjust path as needed
  image_size: 128
  in_channels: 1

model:
  encoder: swin           # 'vit' or 'swin'
  patch_h: 4
  patch_w: 4
  embed_dim: 8            # doubles each stage: 8 → 16 → 32 → 64 → 128 → 256
  depths: [1, 2, 2, 6, 2, 1]
  num_heads: [1, 1, 2, 4, 8, 16]
  window_size: 8
  mlp_ratio: 4.0
  drop_path_rate: 0.1
  dec_depths: [2, 3, 3, 6, 2, 1] # decoder's a bit heavier than encoder

training:
  batch_size: 380         # encoder batch size
  lr: 2e-4
  epochs: 200
  lambd: 0.3              # lejepa sim-vs-sigreg weight
  max_shift_x: 12         # will increase w/epoch via curriculum
  max_shift_y: 12
  mae_lambda: 10
  mask_ratio: 0.5
  empty_mask_ratio: 0.05  # fraction of mask_ratio applied to empty patches
  lambda_visible: 1.0
  dec_batch_size: 360     # decoder-training batch size
  dec_epochs: 100 # 500
  dec_lr: 2e-3
  gan_warmup: 100000      # number of epochs before adversarial training starts
  enc_ft_lr: -1           # lr for fine-tuning enc via end-to-end training w/ decoder

preencode:
  output_dir: /home/shawley/datasets/POP909_encoded
  num_passes: 10          # number of passes through the dataset

wandb:
  project: swin-midi-rae

