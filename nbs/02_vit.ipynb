{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dc6903",
   "metadata": {},
   "source": [
    "# ViT\n",
    "\n",
    "> Components & defs for ViT-based Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93157b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96051a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10422c",
   "metadata": {},
   "source": [
    "## ViT components \n",
    "\n",
    "used in both encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoPE2D(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        i = torch.arange(0, head_dim // 4)\n",
    "        self.register_buffer('ifreq', 1.0 / (10000 ** (2 * i / head_dim)))\n",
    "        self.num_patches = -1\n",
    "\n",
    "    def calc_pi(self, num_patches, grid_w): \n",
    "        self.num_patches = num_patches\n",
    "        pi = torch.arange(self.num_patches, device=self.ifreq.device)\n",
    "        self.register_buffer('pih', pi // grid_w)\n",
    "        self.register_buffer('piw', pi % grid_w)\n",
    "\n",
    "    def _rotate(self, x, sin, cos):\n",
    "        x_even, x_odd = x[..., 0::2], x[..., 1::2]\n",
    "        x_out = torch.empty_like(x)\n",
    "        x_out[..., 0::2] = x_even * cos - x_odd * sin\n",
    "        x_out[..., 1::2] = x_odd * cos + x_even * sin\n",
    "        return x_out\n",
    "\n",
    "    def forward(self, x, nphw=(16,16)):  # x: (batch, heads, num_patches, head_dim)\n",
    "        num_patches, grid_w, head_dim = x.shape[2], nphw[-1], x.shape[-1]\n",
    "        if num_patches != self.num_patches: self.calc_pi(num_patches, grid_w)\n",
    "        \n",
    "        freqs_h = torch.outer(self.pih.float(), self.ifreq)[None, None]\n",
    "        freqs_w = torch.outer(self.piw.float(), self.ifreq)[None, None]\n",
    "        sin_h, cos_h = torch.sin(freqs_h), torch.cos(freqs_h)\n",
    "        sin_w, cos_w = torch.sin(freqs_w), torch.cos(freqs_w)\n",
    "        \n",
    "        x_h, x_w = x[..., :head_dim//2], x[..., head_dim//2:]\n",
    "        x_h_out = self._rotate(x_h, sin_h, cos_h)\n",
    "        x_w_out = self._rotate(x_w, sin_w, cos_w)\n",
    "        \n",
    "        return torch.cat([x_h_out, x_w_out], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rot_x.shape =  torch.Size([2, 8, 256, 192])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "head_dim = 768//4\n",
    "x = torch.rand((2, 8, 256, head_dim))\n",
    "rope = RoPE2D(head_dim)\n",
    "rot_x = rope(x) \n",
    "print(\"rot_x.shape = \",rot_x.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5498e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_qkv=None):\n",
    "        super().__init__()\n",
    "        if dim_qkv is None: dim_qkv = dim\n",
    "        self.heads, self.dim_qkv  = heads, dim_qkv\n",
    "        self.head_dim = dim_qkv // heads\n",
    "        self.rope = RoPE2D(self.head_dim)\n",
    "        self.qkv = nn.Linear(dim, dim_qkv * 3)\n",
    "        self.proj = nn.Linear(dim_qkv, dim)\n",
    "        \n",
    "    def forward(self, x):  # x: (batch, num_patches, dim)\n",
    "        B, N = x.shape[:2]\n",
    "        # Project and split into q, k, v\n",
    "        qkv = self.qkv(x)  # (B, N, dim_qkv * 3)\n",
    "        qkv = qkv.reshape(B, N, 3, self.heads, self.head_dim)  # split into 3, heads, head_dim\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, heads, N, head_dim)\n",
    "        q, k = self.rope(q), self.rope(k)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v)  # (B, heads, N, head_dim), \"flash attention\"\n",
    "        out = out.transpose(1, 2).reshape(B, N, self.dim_qkv)  # Merge heads\n",
    "        return self.proj(out)  # project back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: a.shape =  torch.Size([2, 256, 768])\n",
      "Done: a2.shape =  torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "x = torch.rand(2, 256, 768)\n",
    "attn = Attention(768, 8) \n",
    "a = attn(x) \n",
    "print(\"Done: a.shape = \",a.shape)\n",
    "\n",
    "attn2 = Attention(768, 8, 64) \n",
    "a2 = attn2(x) \n",
    "print(\"Done: a2.shape = \",a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_qkv=None, hdim=None):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim, heads, dim_qkv)\n",
    "        if hdim is None: hdim = 4*dim\n",
    "        self.lin1, self.lin2 = nn.Linear(dim, hdim), nn.Linear(hdim, dim)\n",
    "        self.norm1, self.norm2 = nn.LayerNorm(dim), nn.LayerNorm(dim)\n",
    "        self.act = nn.GELU() \n",
    "        \n",
    "    def forward(self, x):  # x: (batch, num_patches, dim)\n",
    "        x = x + self.attn(self.norm1(x))   # \"pre-norm\"\n",
    "        x = x + self.lin2(self.act(self.lin1(self.norm2(x))))  \n",
    "        return x  # (batch, num_patches, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7410e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape =  torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "x = torch.randn(2, 256, 768) \n",
    "trans = TransformerBlock(768, 8) \n",
    "out = trans(x) \n",
    "print(\"out.shape = \",out.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57178d7b",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Does patch embedding and then some transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels=1,  # 1 for solo piano, for midi PR's, = # of instruments\n",
    "                patch_size=16,  # assuming square patches, e.g. 16 implies 16x16\n",
    "                dim=768):       # embedding dimension\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):  # x: (batch, channels, height, width)\n",
    "        assert all(s % self.conv.kernel_size[0] == 0 for s in x.shape[-2:]), \\\n",
    "            f\"Image size {x.shape[-2:]} must be divisible by patch_size {self.conv.kernel_size[0]}\"\n",
    "        conv_patches = self.conv(x).flatten(2).permute(0,2,1)\n",
    "        # Check if each patch region in the image is empty\n",
    "        k = self.conv.kernel_size[0]\n",
    "        patches = x.unfold(2, k, k).unfold(3, k, k)  # extract patches\n",
    "        not_empty = (patches.amax(dim=(-1,-2)) > 0.5).squeeze(1).flatten(1)  # (B, num_patches), 0=empty, 1=not\n",
    "        return conv_patches, not_empty  # return pmask: 1=non-empty, 0=empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.shape =  torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "pe = PatchEmbedding()\n",
    "x = torch.randn(2, 1, 256, 256)\n",
    "z, pmask = pe(x) \n",
    "print(\"z.shape = \",z.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32916a79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mViTEncoder\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):",
      "\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Vision Transformer Encoder for piano rolls\"\"\"\u001b[39;00m",
      "\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ",
      "\u001b[32m      4\u001b[39m                 in_channels,  \u001b[38;5;66;03m# \u001b[39;00m",
      "\u001b[32m      5\u001b[39m                 image_size,   \u001b[38;5;66;03m# tuple (H,W), e.g. (256, 256)\u001b[39;00m",
      "\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m                 depth,        \u001b[38;5;66;03m# number of transformerblock layers -- 4? \u001b[39;00m",
      "\u001b[32m      9\u001b[39m                 heads):       \u001b[38;5;66;03m# number of attention heads - 8? \u001b[39;00m",
      "",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"Vision Transformer Encoder for piano rolls\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels,  # \n",
    "                image_size,   # tuple (H,W), e.g. (256, 256)\n",
    "                patch_size,   # assuming square patches, e.g 16\n",
    "                dim,          # embedding dim, e.g. 768\n",
    "                depth,        # number of transformerblock layers -- 4? \n",
    "                heads):       # number of attention heads - 8? \n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels=in_channels,patch_size=patch_size, dim=dim)\n",
    "        self.blocks = nn.ModuleList([ TransformerBlock(dim, heads) for _ in range(depth) ])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "    def forward(self, x, return_cls_only=True):\n",
    "        x, pmask = self.patch_embed(x)                 # x is now patches, pmask is 1 for non-empty patches, 0 for empty\n",
    "        cls = self.cls_token.expand(x.shape[0], -1, -1) # add cls token \n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        pmask = torch.cat([pmask.new_ones(pmask.shape[0], 1), pmask], dim=1)  # (B, 65)\n",
    "        for block in self.blocks:  \n",
    "            x = block(x) \n",
    "            x = torch.where(pmask.unsqueeze(-1), x, x * 1e-3)  # empty patches go to small but nonzero #s\n",
    "        return (x[:, 0] if return_cls_only else x), pmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57761acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "B, C, H, W = 4, 1, 256, 256\n",
    "patch_size, dim, depth, heads = 16, 768, 4, 8 \n",
    "x = torch.randn(B,C,H,W) \n",
    "encoder = ViTEncoder( C, (H,W), patch_size, dim, depth, heads) \n",
    "out, pmask = encoder(x) \n",
    "print(\"out.shape = \",out.shape) \n",
    "\n",
    "out2, pmask = encoder(x, return_cls_only=False) \n",
    "print(\"out2.shape = \",out2.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93515736",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Like the Encoder, only instead of doing \"PatchEmbedding\" on the front end \"UnPatchify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Unpatchify(nn.Module):\n",
    "    \"Take patches and assemble an image\"\n",
    "    def __init__(self, \n",
    "                out_channels=1,  # 1 for solo piano, for midi PR's, = # of instruments\n",
    "                image_size = (128, 128),  # h,w for output image  \n",
    "                patch_size=16,  # assuming square patches, e.g. 16 implies 16x16\n",
    "                dim=768):       # embedding dimension\n",
    "        super().__init__()\n",
    "        self.image_size, self.patch_size, self.out_channels = image_size, patch_size, out_channels\n",
    "        self.npatches_x, self.npatches_y = image_size[0]//patch_size, image_size[1]//patch_size \n",
    "        self.lin = nn.Linear(dim, out_channels * patch_size * patch_size )  # (B, 64, 768) -> (B, 64, 256) \n",
    "        \n",
    "    def forward(self, z):  # z: patch embeddings (batch, num_patches, dim)\n",
    "        out = self.lin(z)  # B, N, D \n",
    "        out = out.reshape(-1, self.npatches_x, self.npatches_y, self.patch_size, self.patch_size, self.out_channels)\n",
    "        out = out.permute(0, 5, 1, 3, 2, 4)  # (B, 1, 8, 16, 8, 16)\n",
    "        out = out.reshape(-1, self.out_channels, self.image_size[0], self.image_size[1])        \n",
    "        return out # (B, out_channels, H, W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false \n",
    "z = torch.randn([3, 64, 768]) \n",
    "unpatch = Unpatchify() # keep the defaults\n",
    "img = unpatch(z) \n",
    "print(\"img.shape = \",img.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ViTDecoder(nn.Module):\n",
    "    \"\"\"Vision Transformer Decoder for piano rolls\"\"\"\n",
    "    def __init__(self, \n",
    "                out_channels,  # \n",
    "                image_size,   # tuple (H,W), e.g. (256, 256)\n",
    "                patch_size,   # assuming square patches, e.g 16\n",
    "                dim,          # embedding dim, e.g. 768\n",
    "                depth=4,        # number of transformerblock layers -- 4? \n",
    "                heads=8):       # number of attention heads - 8? \n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([ TransformerBlock(dim, heads) for _ in range(depth) ])\n",
    "        self.unpatch = Unpatchify(out_channels, image_size, patch_size, dim)\n",
    "        \n",
    "    def forward(self, z, strip_cls_token=True):\n",
    "        for block in self.blocks:  z = block(z)\n",
    "        if strip_cls_token: z = z[:,1:] \n",
    "        img = self.unpatch(z) \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(3, 65, 768)  # batch of 3, with CLS token\n",
    "decoder = ViTDecoder(out_channels=1, image_size=(128,128), patch_size=16, dim=768)\n",
    "img = decoder(z)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
