{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dc6903",
   "metadata": {},
   "source": [
    "# ViT\n",
    "\n",
    "> Components & defs for ViT-based Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93157b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96051a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10422c",
   "metadata": {},
   "source": [
    "## ViT components \n",
    "\n",
    "used in both encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RoPE2D(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        i = torch.arange(0, head_dim // 4)\n",
    "        self.register_buffer('ifreq', 1.0 / (10000 ** (2 * i / head_dim)))\n",
    "        self.num_patches = -1\n",
    "\n",
    "    def calc_pi(self, num_patches, grid_w): \n",
    "        self.num_patches = num_patches\n",
    "        pi = torch.arange(self.num_patches, device=self.ifreq.device)\n",
    "        self.register_buffer('pih', pi // grid_w)\n",
    "        self.register_buffer('piw', pi % grid_w)\n",
    "\n",
    "    def _rotate(self, x, sin, cos):\n",
    "        x_even, x_odd = x[..., 0::2], x[..., 1::2]\n",
    "        x_out = torch.empty_like(x)\n",
    "        x_out[..., 0::2] = x_even * cos - x_odd * sin\n",
    "        x_out[..., 1::2] = x_odd * cos + x_even * sin\n",
    "        return x_out\n",
    "\n",
    "    def forward(self, x, pos=None, nphw=(16,16)):  # x: (batch, heads, num_patches, head_dim)\n",
    "        num_patches, grid_w, head_dim = x.shape[2], nphw[-1], x.shape[-1]\n",
    "        if pos is None: \n",
    "            if num_patches != self.num_patches: self.calc_pi(num_patches, grid_w)\n",
    "            freqs_h = torch.outer(self.pih.float(), self.ifreq)[None, None]\n",
    "            freqs_w = torch.outer(self.piw.float(), self.ifreq)[None, None]\n",
    "        else:\n",
    "            pih, piw = pos[:, 0], pos[:, 1] \n",
    "            freqs_h = torch.outer(pih.float(), self.ifreq)[None, None]\n",
    "            freqs_w = torch.outer(piw.float(), self.ifreq)[None, None]        \n",
    "\n",
    "        sin_h, cos_h = torch.sin(freqs_h), torch.cos(freqs_h)\n",
    "        sin_w, cos_w = torch.sin(freqs_w), torch.cos(freqs_w)\n",
    "        \n",
    "        x_h, x_w = x[..., :head_dim//2], x[..., head_dim//2:]\n",
    "        x_h_out = self._rotate(x_h, sin_h, cos_h)\n",
    "        x_w_out = self._rotate(x_w, sin_w, cos_w)\n",
    "        \n",
    "        return torch.cat([x_h_out, x_w_out], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rot_x.shape =  torch.Size([2, 8, 256, 192])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "head_dim = 768//4\n",
    "x = torch.rand((2, 8, 256, head_dim))\n",
    "rope = RoPE2D(head_dim)\n",
    "rot_x = rope(x) \n",
    "print(\"rot_x.shape = \",rot_x.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5498e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_qkv=None):\n",
    "        super().__init__()\n",
    "        if dim_qkv is None: dim_qkv = dim\n",
    "        self.heads, self.dim_qkv  = heads, dim_qkv\n",
    "        self.head_dim = dim_qkv // heads\n",
    "        self.rope = RoPE2D(self.head_dim)\n",
    "        self.qkv = nn.Linear(dim, dim_qkv * 3)\n",
    "        self.proj = nn.Linear(dim_qkv, dim)\n",
    "        \n",
    "    def forward(self, x, pos=None):  # x: (batch, num_patches, dim)\n",
    "        B, N = x.shape[:2]\n",
    "        # Project and split into q, k, v\n",
    "        qkv = self.qkv(x)  # (B, N, dim_qkv * 3)\n",
    "        qkv = qkv.reshape(B, N, 3, self.heads, self.head_dim)  # split into 3, heads, head_dim\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, heads, N, head_dim)\n",
    "        q, k = self.rope(q, pos=pos), self.rope(k, pos=pos)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v)  # (B, heads, N, head_dim), \"flash attention\"\n",
    "        out = out.transpose(1, 2).reshape(B, N, self.dim_qkv)  # Merge heads\n",
    "        return self.proj(out)  # project back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: a.shape =  torch.Size([2, 256, 768])\n",
      "Done: a2.shape =  torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "x = torch.rand(2, 256, 768)\n",
    "attn = Attention(768, 8) \n",
    "a = attn(x) \n",
    "print(\"Done: a.shape = \",a.shape)\n",
    "\n",
    "attn2 = Attention(768, 8, 64) \n",
    "a2 = attn2(x) \n",
    "print(\"Done: a2.shape = \",a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_qkv=None, hdim=None):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(dim, heads, dim_qkv)\n",
    "        if hdim is None: hdim = 4*dim\n",
    "        self.lin1, self.lin2 = nn.Linear(dim, hdim), nn.Linear(hdim, dim)\n",
    "        self.norm1, self.norm2 = nn.LayerNorm(dim), nn.LayerNorm(dim)\n",
    "        self.act = nn.GELU() \n",
    "        \n",
    "    def forward(self, x, pos=None):  # x: (batch, num_patches, dim)\n",
    "        x = x + self.attn(self.norm1(x), pos=pos)   # \"pre-norm\"\n",
    "        x = x + self.lin2(self.act(self.lin1(self.norm2(x))))  \n",
    "        return x  # (batch, num_patches, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7410e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape =  torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "x = torch.randn(2, 256, 768) \n",
    "trans = TransformerBlock(768, 8) \n",
    "out = trans(x) \n",
    "print(\"out.shape = \",out.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57178d7b",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Does patch embedding and then some transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels=1,  # 1 for solo piano, for midi PR's, = # of instruments\n",
    "                patch_size=16,  # assuming square patches, e.g. 16 implies 16x16\n",
    "                dim=768):       # embedding dimension\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):  # x: (batch, channels, height, width)\n",
    "        assert all(s % self.conv.kernel_size[0] == 0 for s in x.shape[-2:]), \\\n",
    "            f\"Image size {x.shape[-2:]} must be divisible by patch_size {self.conv.kernel_size[0]}\"\n",
    "        conv_patches = self.conv(x).flatten(2).permute(0,2,1)\n",
    "        # Check if each patch region in the image is empty\n",
    "        k = self.conv.kernel_size[0]\n",
    "        patches = x.unfold(2, k, k).unfold(3, k, k)  # extract patches\n",
    "        pmask = (patches.amax(dim=(-1,-2)) > 0.2).squeeze(1).flatten(1) # (B, num_patches), 0=empty, 1=not\n",
    "\n",
    "        # save patch position \"coordinates\" (for masking later)\n",
    "        H, W = x.shape[-2] // k, x.shape[-1] // k  # grid dimensions\n",
    "        rows = torch.arange(H, device=x.device).repeat_interleave(W)\n",
    "        cols = torch.arange(W, device=x.device).repeat(H)\n",
    "        pos = torch.stack([rows, cols], dim=-1)  # (num_patches, 2)\n",
    "        return conv_patches, pmask, pos  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.shape, pmask.shape, pos.shape =  torch.Size([2, 256, 768]) torch.Size([2, 256]) torch.Size([256, 2])\n",
      "pos = \n",
      " [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11], [0, 12], [0, 13], [0, 14], [0, 15], [1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [1, 13], [1, 14], [1, 15], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11], [2, 12], [2, 13], [2, 14], [2, 15], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [3, 10], [3, 11], [3, 12], [3, 13], [3, 14], [3, 15], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [4, 10], [4, 11], [4, 12], [4, 13], [4, 14], [4, 15], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9], [5, 10], [5, 11], [5, 12], [5, 13], [5, 14], [5, 15], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 6], [6, 7], [6, 8], [6, 9], [6, 10], [6, 11], [6, 12], [6, 13], [6, 14], [6, 15], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 7], [7, 8], [7, 9], [7, 10], [7, 11], [7, 12], [7, 13], [7, 14], [7, 15], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 8], [8, 9], [8, 10], [8, 11], [8, 12], [8, 13], [8, 14], [8, 15], [9, 0], [9, 1], [9, 2], [9, 3], [9, 4], [9, 5], [9, 6], [9, 7], [9, 8], [9, 9], [9, 10], [9, 11], [9, 12], [9, 13], [9, 14], [9, 15], [10, 0], [10, 1], [10, 2], [10, 3], [10, 4], [10, 5], [10, 6], [10, 7], [10, 8], [10, 9], [10, 10], [10, 11], [10, 12], [10, 13], [10, 14], [10, 15], [11, 0], [11, 1], [11, 2], [11, 3], [11, 4], [11, 5], [11, 6], [11, 7], [11, 8], [11, 9], [11, 10], [11, 11], [11, 12], [11, 13], [11, 14], [11, 15], [12, 0], [12, 1], [12, 2], [12, 3], [12, 4], [12, 5], [12, 6], [12, 7], [12, 8], [12, 9], [12, 10], [12, 11], [12, 12], [12, 13], [12, 14], [12, 15], [13, 0], [13, 1], [13, 2], [13, 3], [13, 4], [13, 5], [13, 6], [13, 7], [13, 8], [13, 9], [13, 10], [13, 11], [13, 12], [13, 13], [13, 14], [13, 15], [14, 0], [14, 1], [14, 2], [14, 3], [14, 4], [14, 5], [14, 6], [14, 7], [14, 8], [14, 9], [14, 10], [14, 11], [14, 12], [14, 13], [14, 14], [14, 15], [15, 0], [15, 1], [15, 2], [15, 3], [15, 4], [15, 5], [15, 6], [15, 7], [15, 8], [15, 9], [15, 10], [15, 11], [15, 12], [15, 13], [15, 14], [15, 15]]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "pe = PatchEmbedding()\n",
    "x = torch.randn(2, 1, 256, 256)\n",
    "z, pmask, pos = pe(x) \n",
    "print(\"z.shape, pmask.shape, pos.shape = \",z.shape, pmask.shape, pos.shape) \n",
    "print(\"pos = \\n\",pos.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_mae_mask(pmask, ratio=0.75, has_cls_token=True):\n",
    "    \"Apply token masking for MAE training. 1=keep, 0=masked\"\n",
    "    mae_mask = pmask[0] & (torch.rand(pmask.shape[1], device=pmask.device) > ratio)  #  (B, N), 1=visible, 0=masked\n",
    "    if has_cls_token: mae_mask[0] = True  \n",
    "    return mae_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_mae_mask(x, pos, pmask, mae_mask):\n",
    "    \"Apply token masking for MAE training. 1=keep, 0=masked\"\n",
    "    return x[:, mae_mask, :], pos[mae_mask, :], pmask[:,mae_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32916a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"Vision Transformer Encoder for piano rolls, keeps track of empty patches (pmask) and supports masking\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels,  # \n",
    "                image_size,   # tuple (H,W), e.g. (256, 256)\n",
    "                patch_size,   # assuming square patches, e.g 16\n",
    "                dim,          # embedding dim, e.g. 768\n",
    "                depth,        # number of transformerblock layers -- 4? \n",
    "                heads):       # number of attention heads - 8? \n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(in_channels=in_channels,patch_size=patch_size, dim=dim)\n",
    "        self.blocks = nn.ModuleList([ TransformerBlock(dim, heads) for _ in range(depth) ])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "    def forward(self, x, return_cls_only=True, mask_ratio=0.75, mae_mask=None):\n",
    "        x, pmask, pos = self.patch_embed(x)      # x is now patches, pmask=1 for non-empty patches, 0 for empty\n",
    "\n",
    "        # tack on cls token \n",
    "        cls = self.cls_token.expand(x.shape[0], -1, -1) \n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        cls_pos = torch.tensor([[-1, -1]], device=pos.device)\n",
    "        pos = torch.cat([cls_pos, pos], dim=0)  # (num_patches+1, 2)\n",
    "        pmask = torch.cat([pmask.new_ones(pmask.shape[0], 1), pmask], dim=1)  # (B, 65)\n",
    "        if mae_mask is None: mae_mask = make_mae_mask(pmask, ratio=mask_ratio if self.training else 0.0)\n",
    "        x, pos_visible, pmask_visible = apply_mae_mask(x, pos, pmask, mae_mask)\n",
    "\n",
    "        for block in self.blocks:  \n",
    "            x = block(x, pos=pos_visible) \n",
    "            x = torch.where(pmask_visible.unsqueeze(-1), x, x * 1e-3)  # empty patches go to small but nonzero #s\n",
    "        return (x[:, 0] if return_cls_only else x), pmask, pos, mae_mask  # return full pos, not pos_visible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57761acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "B, C, H, W = 4, 1, 256, 256\n",
    "patch_size, dim, depth, heads = 16, 768, 4, 8 \n",
    "x = torch.randn(B,C,H,W) \n",
    "encoder = ViTEncoder( C, (H,W), patch_size, dim, depth, heads) \n",
    "out, pmask, pos, mae_mask = encoder(x) \n",
    "print(\"out.shape = \",out.shape) \n",
    "\n",
    "out2, pmask2, pos2, mae_mask = encoder(x, return_cls_only=False, mae_mask=mae_mask) \n",
    "print(\"out2.shape = \",out2.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93515736",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Like the Encoder, only instead of doing \"PatchEmbedding\" on the front end \"UnPatchify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Unpatchify(nn.Module):\n",
    "    \"Take patches and assemble an image\"\n",
    "    def __init__(self, \n",
    "                out_channels=1,  # 1 for solo piano, for midi PR's, = # of instruments\n",
    "                image_size = (128, 128),  # h,w for output image  \n",
    "                patch_size=16,  # assuming square patches, e.g. 16 implies 16x16\n",
    "                dim=768):       # embedding dimension\n",
    "        super().__init__()\n",
    "        self.image_size, self.patch_size, self.out_channels = image_size, patch_size, out_channels\n",
    "        self.npatches_x, self.npatches_y = image_size[0]//patch_size, image_size[1]//patch_size \n",
    "        self.lin = nn.Linear(dim, out_channels * patch_size * patch_size )  # (B, 64, 768) -> (B, 64, 256) \n",
    "        \n",
    "    def forward(self, z):  # z: patch embeddings (batch, num_patches, dim)\n",
    "        out = self.lin(z)  # B, N, D \n",
    "        out = out.reshape(-1, self.npatches_x, self.npatches_y, self.patch_size, self.patch_size, self.out_channels)\n",
    "        out = out.permute(0, 5, 1, 3, 2, 4)  # (B, 1, 8, 16, 8, 16)\n",
    "        out = out.reshape(-1, self.out_channels, self.image_size[0], self.image_size[1])        \n",
    "        return out # (B, out_channels, H, W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false \n",
    "z = torch.randn([3, 64, 768]) \n",
    "unpatch = Unpatchify() # keep the defaults\n",
    "img = unpatch(z) \n",
    "print(\"img.shape = \",img.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ViTDecoder(nn.Module):\n",
    "    \"\"\"Vision Transformer Decoder for piano rolls\"\"\"\n",
    "    def __init__(self, \n",
    "                out_channels,  # \n",
    "                image_size,   # tuple (H,W), e.g. (256, 256)\n",
    "                patch_size,   # assuming square patches, e.g 16\n",
    "                dim,          # embedding dim, e.g. 768\n",
    "                depth=4,        # number of transformerblock layers -- 4? \n",
    "                heads=8):       # number of attention heads - 8? \n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([ TransformerBlock(dim, heads) for _ in range(depth) ])\n",
    "        self.unpatch = Unpatchify(out_channels, image_size, patch_size, dim)\n",
    "        \n",
    "    def forward(self, z, strip_cls_token=True):\n",
    "        for block in self.blocks:  z = block(z)\n",
    "        if strip_cls_token: z = z[:,1:] \n",
    "        img = self.unpatch(z) \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false \n",
    "z = torch.randn(3, 65, 768)  # batch of 3, with CLS token\n",
    "decoder = ViTDecoder(out_channels=1, image_size=(128,128), patch_size=16, dim=768)\n",
    "img = decoder(z)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LightweightMAEDecoder(nn.Module):\n",
    "    \"\"\"Simple decoder for MAE pretraining - reconstructs masked patches\n",
    "     loss should compare `output[:, ~mae_mask]` against original masked patch pixels.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size=16, dim=256, depth=2, heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(dim, heads) for _ in range(depth)])\n",
    "        self.proj = nn.Linear(dim, patch_size * patch_size)  # output pixels per patch\n",
    "        \n",
    "    def forward(self, z,  # (B, N_vis, dim),\n",
    "                pos_full, # original set of positions w/o mae masking\n",
    "                mae_mask, # 1=visible, 0=masked out\n",
    "                ):   \n",
    "        B, N_full = z.shape[0], pos_full.shape[0]\n",
    "        z_full = self.mask_token.expand(B, N_full, -1).clone()\n",
    "        if z.ndim < 3: z = z.reshape(B, -1, z.shape[-1])          #  # (B*N_vis, dim) -> (B, N_vis, dim)\n",
    "        z_full[:, mae_mask, :] = z  # insert visible tokens\n",
    "        for block in self.blocks: z_full = block(z_full, pos=pos_full)\n",
    "        return self.proj(z_full)  # (B, N_full, patch_size^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
