{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dc6903",
   "metadata": {},
   "source": [
    "# losses\n",
    "\n",
    "> LeJEPA, GAN discriminator, ...aand more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93157b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96051a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239f356",
   "metadata": {},
   "source": [
    "## Safe Mean\n",
    "\n",
    "Turns out zero element tensors will yield NaN when you try to run `.mean()`, so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c62704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def safe_mean(t, dim=None): \n",
    "    \"\"\"safe replacement for torch.mean( ).  can't be used as a suffix though\"\"\"\n",
    "    return t.mean(dim=dim) if t.numel() > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366f3eb",
   "metadata": {},
   "source": [
    "## LeJEPA Loss\n",
    "\n",
    "For an interactive overview of LeJEPA, see https://www.scotthawley.com/ssltoy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def SIGReg(x, global_step, num_slices=256):\n",
    "    \"\"\"SIGReg with Epps-Pulley statistic. x is (N, K) tensor.\"\"\"\n",
    "    device = x.device\n",
    "    g = torch.Generator(device=device).manual_seed(global_step)\n",
    "    proj_shape = (x.size(1), num_slices)\n",
    "    A = torch.randn(proj_shape, generator=g, device=device)\n",
    "    A = A / (A.norm(dim=0, keepdim=True) + 1e-10)  # normalize columns\n",
    "    \n",
    "    # Epps-Pulley statistic\n",
    "    t = torch.linspace(-5, 5, 17, device=device) # values used in LeJEPA paper, worked for SSLtoy\n",
    "    exp_f = torch.exp(-0.5 * t**2)  # theoretical CF for N(0,1)\n",
    "    x_t = (x @ A).unsqueeze(2) * t  # (N, M, T)\n",
    "    ecf = (torch.exp(1j * x_t).mean(dim=0)).abs()  # empirical CF\n",
    "    diff = (ecf - exp_f).abs().square().mul(exp_f)  # weighted L2 distance\n",
    "    #N = x.size(0)  # With respect to Yann: Don't scale by N because then if you change the batch size you have to retune lambd by hand ugh\n",
    "    T = torch.trapz(diff, t, dim=1).sum() #* N  # sum here is over num slices, not data points\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795594dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGReg loss: 2.9035\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test SIGReg with random embeddings\n",
    "batch_size, embed_dim = 32, 64\n",
    "x = torch.randn(batch_size, embed_dim)\n",
    "loss = SIGReg(x, global_step=0, num_slices=256)\n",
    "print(f\"SIGReg loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def attraction_loss(z1, z2,  # embeddings of two \"views\" of the same thing (in batches)\n",
    "                    deltas=None,   # optional/TBD: info on semantic 'distance' between z1 & z2\n",
    "                    tau = 100.0):    # inverse strength of fall-off for delta distances, big=slower\n",
    "    \"How we pull similar 'views' together\"\n",
    "    if deltas is None: return safe_mean( (z1 - z2).square() )\n",
    "    delta_diag = (deltas**2).sum(dim=1)\n",
    "    delta_fac = torch.exp(-delta_diag / tau) # less attraction for more 'distant' views\n",
    "    #delta_fac = 1/(1 + delta_diag/tau)  # longer tail than exp\n",
    "    return safe_mean( (z1 - z2).square() * delta_fac.unsqueeze(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624570b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def LeJEPA(z1, z2, global_step, lambd=0.5, deltas=None): \n",
    "    \"Main LeJEPA loss function\"\n",
    "    sim = attraction_loss(z1, z2, deltas=deltas)\n",
    "    sigreg = SIGReg( torch.cat((z1, z2), dim=0), global_step ) * 1 # normalize to similar scale as sim\n",
    "    return {'loss': (1-lambd)*sim + lambd*sigreg, 'sim':sim.item(), 'sigreg':sigreg.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeJEPA loss: 1.8238\n",
      "  Attraction: 2.0526\n",
      "  SIGReg: 1.5950\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test LeJEPA loss\n",
    "batch_size, embed_dim = 32, 64\n",
    "z1 = torch.randn(batch_size, embed_dim)\n",
    "z2 = torch.randn(batch_size, embed_dim)\n",
    "\n",
    "loss = LeJEPA(z1, z2, global_step=0, lambd=0.5)\n",
    "print(f\"LeJEPA loss: {loss['loss'].item():.4f}\")\n",
    "print(f\"  Attraction: {attraction_loss(z1, z2).item():.4f}\")\n",
    "print(f\"  SIGReg: {SIGReg(torch.cat((z1, z2), dim=0), global_step=0).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0da96b-409d-4715-b7e2-e302f5f6739a",
   "metadata": {},
   "source": [
    "## Masked (Auto)Encoder Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# def calc_mae_loss(recon_patches, img, enc_out, patch_size=16, lambda_visible=0.1):\n",
    "#     \"BCE loss on reconstructed patches, with optional downweighting of visible patches\"\n",
    "#     mae_mask = enc_out.mae_mask[1:]  # strip CLS\n",
    "#     img_patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "#     img_patches = img_patches.flatten(2, 3).flatten(-2, -1).squeeze(1)  # (B, N, ps*ps)\n",
    "#     weights = torch.ones_like(recon_patches)\n",
    "#     weights[:, mae_mask, :] = lambda_visible\n",
    "#     loss = (F.binary_cross_entropy_with_logits(recon_patches, img_patches, reduction='none') * weights).mean()\n",
    "#     return loss\n",
    "\n",
    "def calc_mae_loss(recon_patches, img, enc_out, lambda_visible=0.1):\n",
    "    \"BCE loss on reconstructed patches, with optional downweighting of visible patches\"\n",
    "    mae_mask = enc_out.mae_mask\n",
    "    patch_size = int(recon_patches.shape[-1] ** 0.5)\n",
    "    if mae_mask.dim() == 1: mae_mask = mae_mask[1:]  # strip CLS (ViT only)\n",
    "    img_patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    img_patches = img_patches.flatten(2, 3).flatten(-2, -1).squeeze(1)  # (B, N, ps*ps)\n",
    "    weights = torch.ones_like(recon_patches)\n",
    "    weights[mae_mask] = lambda_visible  # works for both (N,) and (B,N)\n",
    "    loss = (F.binary_cross_entropy_with_logits(recon_patches, img_patches, reduction='none') * weights).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e863863",
   "metadata": {},
   "source": [
    "## Full Encoder Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa01fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def anchor_loss(z1, z2):\n",
    "    \"Anchor embeddings of empty patches to the origin\"\n",
    "    return safe_mean( z1.square() ) + safe_mean( z2.square() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_enc_loss(z1, z2, global_step, deltas=None, lambd=0.5, non_emptys=(None,None), lambda_anchor=0.05):\n",
    "    \"Main loss function for Encoder\"\n",
    "    non_empty1, non_empty2 = non_emptys\n",
    "    non_empty = non_empty1 & non_empty2  # both non-empty\n",
    "    valid = non_empty.view(-1).bool()\n",
    "    loss_dict = LeJEPA(z1[valid], z2[valid], global_step, deltas=deltas[valid], lambd=lambd)\n",
    "    aloss = anchor_loss(z1[~non_empty1.view(-1).bool()], z2[~non_empty2.view(-1).bool()])\n",
    "    loss_dict['anchor'] = aloss\n",
    "    loss_dict['loss'] = loss_dict['loss'] + lambda_anchor * aloss\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "@torch.compiler.disable\n",
    "def calc_enc_loss_multiscale(z1, z2,  # lists of embeddings at each swin hierarchy level, 0=coarsest, -1=finest\n",
    "                             global_step, img_size, deltas=None,\n",
    "                             lambd=0.5, non_emptys=None, lambda_anchor=0.05):\n",
    "    \"\"\"Compute encoder loss at each hierarchy level, weighted by patch overlap fraction.\n",
    "    Levels where delta/shift exceeds patch size (resulting in zero overlap) are skipped entirely.\n",
    "    Non-empty patch masks focus the sim/anchor losses on musically active regions.\"\"\"\n",
    "    if not isinstance(z1, list): # regular vit \n",
    "        d_exp = deltas.repeat_interleave(z1.shape[0] // deltas.shape[0], dim=0)\n",
    "        return calc_enc_loss(z1, z2, global_step, deltas=d_exp, lambd=lambd, non_emptys=non_emptys, lambda_anchor=lambda_anchor)\n",
    "    total = {}\n",
    "    abs_deltas = deltas.abs()\n",
    "    n_levels = len(z1)\n",
    "    for i, (z1_l, z2_l, ne) in enumerate(zip(z1, z2, non_emptys)):\n",
    "        if z1_l.shape[1] == 1: continue  # skip degenerate 1-token level\n",
    "        B, N, D = z1_l.shape\n",
    "        grid = int(N ** 0.5)\n",
    "        patch_w, patch_h = img_size // grid, img_size // grid\n",
    "        dx, dy = abs_deltas[:, 0], abs_deltas[:, 1]\n",
    "        # hinge-style weight to avoid comparing patches if the deltas (aka shift) exceed patch size. \n",
    "        w = (((patch_w - dx) / patch_w).clamp(min=0) * ((patch_h - dy) / patch_h).clamp(min=0)).mean()\n",
    "        if w < 1e-8: continue\n",
    "\n",
    "        num_tokens = N\n",
    "        d_exp = deltas.repeat_interleave(num_tokens, dim=0)\n",
    "        z1_flat, z2_flat = z1_l.reshape(-1, D),  z2_l.reshape(-1, D)\n",
    "        #ne_flat = (ne[0].reshape(-1), ne[1].reshape(-1))\n",
    "        ne_flat = (ne[0].reshape(-1).bool(), ne[1].reshape(-1).bool())\n",
    "\n",
    "        ld = calc_enc_loss(z1_flat, z2_flat, global_step, deltas=d_exp,\n",
    "                           lambd=lambd, non_emptys=ne_flat, lambda_anchor=lambda_anchor)\n",
    "        for k, v in ld.items():  total[k] = total.get(k, 0) + v * w\n",
    "\n",
    "    if not total: return {'loss': torch.tensor(0.0, device=deltas.device)}\n",
    "    return {k: v / n_levels for k, v in total.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1dec6",
   "metadata": {},
   "source": [
    "## Adversarial Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade84ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=64, n_layers=3, use_spectral_norm=True):\n",
    "        super().__init__()\n",
    "        norm = nn.utils.spectral_norm if use_spectral_norm else (lambda x: x)\n",
    "        layers = [norm(nn.Conv2d(in_ch, base_ch, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, True)]\n",
    "        ch = base_ch\n",
    "        for i in range(1, n_layers):\n",
    "            ch_next = min(ch * 2, 512)  # double channels each layer, but cap at 512 to limit params\n",
    "            layers += [norm(nn.Conv2d(ch, ch_next, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, True)]\n",
    "            ch = ch_next\n",
    "        layers.append(norm(nn.Conv2d(ch, 1, kernel_size=4, stride=1, padding=1)))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
