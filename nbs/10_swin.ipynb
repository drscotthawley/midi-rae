{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# swin\n",
    "\n",
    "> Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b291f",
   "metadata": {},
   "source": [
    "## Design Overview\n",
    "\n",
    "### What this module does\n",
    "\n",
    "`SwinEncoder` is a drop-in replacement for `ViTEncoder` that uses the **Swin Transformer V2**\n",
    "architecture. It takes a piano roll image `(B, 1, 128, 128)` and returns an `EncoderOutput`\n",
    "with hierarchical multi-scale patch states.\n",
    "\n",
    "### Why Swin V2?\n",
    "\n",
    "- **Hierarchical representation**: 7 levels from finest (64×64 grid, dim=4) down to a single\n",
    "  CLS-like token (1×1, dim=256), compared to ViT's flat single-scale output\n",
    "- **Efficient attention**: Windowed attention with shifted windows — O(N) instead of O(N²)\n",
    "- **V2 improvements**: Cosine attention with learned log-scale temperature, continuous position\n",
    "  bias via CPB MLP, res-post-norm for training stability\n",
    "\n",
    "### Architecture\n",
    "\n",
    "| Stage | Grid | Patch covers | Dim | Depths | Heads |\n",
    "|-------|------|-------------|-----|--------|-------|\n",
    "| 0 | 64×64 | 2×2 | 4 | 1 | 1 |\n",
    "| 1 | 32×32 | 4×4 | 8 | 1 | 1 |\n",
    "| 2 | 16×16 | 8×8 | 16 | 2 | 1 |\n",
    "| 3 | 8×8 | 16×16 | 32 | 2 | 2 |\n",
    "| 4 | 4×4 | 32×32 | 64 | 6 | 4 |\n",
    "| 5 | 2×2 | 64×64 | 128 | 2 | 8 |\n",
    "| 6 | 1×1 | 128×128 | 256 | 1 | 16 |\n",
    "\n",
    "Config is in `configs/config_swin.yaml`.\n",
    "\n",
    "### Implementation approach\n",
    "\n",
    "We use **timm's `SwinTransformerV2Stage` directly** — no copied or modified Swin internals.\n",
    "Our `SwinEncoder` wrapper handles only:\n",
    "\n",
    "1. **Patch embedding** — `Conv2d(1, 4, kernel_size=2, stride=2)` + LayerNorm\n",
    "2. **Empty patch detection** — patches where all pixels are black get a learnable `empty_token`\n",
    "3. **MAE masking** (SimMIM-style) — masked patches get a learnable `mask_token`, grid stays\n",
    "   intact so windowed attention works unmodified. Two-rate sampling: non-empty patches masked\n",
    "   at `mask_ratio`, empty patches at `mask_ratio × empty_mask_ratio` (default 5%)\n",
    "4. **Hierarchical output** — collects each stage's output into `HierarchicalPatchState`\n",
    "   (coarsest-first), packaged as `EncoderOutput`\n",
    "\n",
    "### Key differences from ViTEncoder\n",
    "\n",
    "- No CLS token (stage 6's single 1×1 token serves as a global summary)\n",
    "- No RoPE (Swin V2 uses its own continuous position bias)\n",
    "- MAE masking keeps all tokens (SimMIM-style) — no compute savings but preserves spatial grid\n",
    "- `empty_mask_ratio` controls how often trivial-to-reconstruct empty patches are masked\n",
    "\n",
    "### TODOs\n",
    "\n",
    "- `HierarchicalPatchState` could store `window_size` per level\n",
    "- `EncoderOutput` could store scale metadata (downsample factors per level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, Set, Type, Union\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.swin_transformer_v2 import SwinTransformerV2Stage\n",
    "from timm.layers import trunc_normal_, to_2tuple, calculate_drop_path_rates\n",
    "from midi_rae.core import PatchState, HierarchicalPatchState, EncoderOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "class SwinEncoder(nn.Module):\n",
    "    \"Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder.  (Wrapper for timm routines)\"\n",
    "    def __init__(self,\n",
    "                 img_height:int,           # Input image height in pixels (e.g. 128)\n",
    "                 img_width:int,            # Input image width in pixels (e.g. 128)\n",
    "                 patch_h:int=2,            # Patch height for initial embedding\n",
    "                 patch_w:int=2,            # Patch width for initial embedding\n",
    "                 in_chans:int=1,           # Number of input channels (1 for piano roll)\n",
    "                 embed_dim:int=4,          # Base embedding dimension (doubles each stage)\n",
    "                 depths:tuple=(1,1,2,2,6,2,1),   # Number of transformer blocks per stage\n",
    "                 num_heads:tuple=(1,1,1,2,4,8,16),# Attention heads per stage\n",
    "                 window_size:int=8,        # Window size for windowed attention\n",
    "                 mlp_ratio:float=4.,       # MLP hidden dim = embed_dim * mlp_ratio\n",
    "                 qkv_bias:bool=True,       # Add bias to QKV projections\n",
    "                 drop_rate:float=0.,       # Dropout after patch embedding\n",
    "                 proj_drop_rate:float=0.,  # Dropout after attention projection\n",
    "                 attn_drop_rate:float=0.,  # Dropout on attention weights\n",
    "                 drop_path_rate:float=0.1, # Stochastic depth rate\n",
    "                 norm_layer:type=nn.LayerNorm, # Normalization layer class\n",
    "                 mae_ratio:float=0.,       # Fraction of non-empty patches to mask (0=no masking)\n",
    "                 empty_mask_ratio:float=0.05): # Mask rate for empty patches relative to mae_ratio\n",
    "        super().__init__()\n",
    "        self.num_stages, self.embed_dim = len(depths), embed_dim\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_stages - 1))\n",
    "        self.patch_h, self.patch_w, self.grid_size = patch_h, patch_w,  (img_height // patch_h, img_width // patch_w)\n",
    "        self.mae_ratio, self.empty_mask_ratio = mae_ratio, empty_mask_ratio\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))\n",
    "        self.patch_norm = norm_layer(embed_dim)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Learnable replacement tokens\n",
    "        self.empty_token, self.mask_token = nn.Parameter(torch.zeros(embed_dim)), nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "        # Build stages using timm's SwinTransformerV2Stage\n",
    "        embed_dims = [int(embed_dim * 2 ** i) for i in range(self.num_stages)]\n",
    "        dpr = calculate_drop_path_rates(drop_path_rate, list(depths), stagewise=True)\n",
    "        self.stages = nn.ModuleList()\n",
    "        in_dim, scale = embed_dims[0], 1\n",
    "        for i in range(self.num_stages):\n",
    "            out_dim = embed_dims[i]\n",
    "            self.stages.append(SwinTransformerV2Stage(\n",
    "                dim=in_dim, out_dim=out_dim, depth=depths[i], num_heads=num_heads[i],\n",
    "                input_resolution=(self.grid_size[0] // scale, self.grid_size[1] // scale),\n",
    "                window_size=window_size, downsample=(i > 0), mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, proj_drop=proj_drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer))\n",
    "            in_dim = out_dim\n",
    "            if i > 0: scale *= 2\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.apply(self._init_weights)\n",
    "        for stage in self.stages: stage._init_respostnorm()\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self) -> Set[str]:\n",
    "        nod = {'empty_token', 'mask_token'}\n",
    "        for n, _ in self.named_parameters():\n",
    "            if any(kw in n for kw in ('cpb_mlp', 'logit_scale')): nod.add(n)\n",
    "        return nod\n",
    "\n",
    "    def _compute_non_empty(self, img):\n",
    "        \"Detect which patches have content (non-black).\"\n",
    "        patches = img.unfold(2, self.patch_h, self.patch_h).unfold(3, self.patch_w, self.patch_w)\n",
    "        return (patches.amax(dim=(-1, -2)) > 0.2).squeeze(1).flatten(1)  # (B, N)\n",
    "\n",
    "    def _make_mae_mask(self, non_empty, device):\n",
    "        \"Two-rate MAE mask: non-empty at mae_ratio, empty at mae_ratio*empty_mask_ratio. Returns (N,) bool, True=visible.\"\n",
    "        B, N = non_empty.shape\n",
    "        is_nonempty = non_empty[0]  # use first sample as representative\n",
    "        rand = torch.rand(N, device=device)\n",
    "        threshold = torch.where(is_nonempty.bool(),\n",
    "            torch.full_like(rand, 1.0 - self.mae_ratio),\n",
    "            torch.full_like(rand, 1.0 - self.mae_ratio * self.empty_mask_ratio))\n",
    "        return rand < threshold\n",
    "\n",
    "    def _make_grid_pos(self, h, w, device):\n",
    "        \"Build (h*w, 2) grid positions.\"\n",
    "        return torch.stack(torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij'), dim=-1).reshape(-1, 2)\n",
    "\n",
    "    def forward(self, x, mask_ratio:float=0., mae_mask:Optional[torch.Tensor]=None) -> EncoderOutput:\n",
    "        \"x: (B,C,H,W) piano roll. mask_ratio overrides self.mae_ratio. mae_mask: (N,) bool, True=visible.\"\n",
    "        B, device = x.shape[0], x.device\n",
    "        grid_h, grid_w = self.grid_size\n",
    "        N_full = grid_h * grid_w\n",
    "\n",
    "        non_empty = self._compute_non_empty(x)                              # (B, N_full)\n",
    "        x = self.patch_embed(x)                                             # (B, C, H', W')\n",
    "        x = self.patch_norm(x.permute(0, 2, 3, 1).contiguous())            # → (B, H', W', C) NHWC\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        # Replace empty patches with learned empty_token\n",
    "        ne4d = non_empty.view(B, H, W, 1)\n",
    "        x = torch.where(ne4d, x, self.empty_token.view(1, 1, 1, -1).expand_as(x))\n",
    "\n",
    "        # MAE masking: replace masked positions with learned mask_token\n",
    "        effective_ratio = mask_ratio if mask_ratio > 0 else self.mae_ratio\n",
    "        if mae_mask is None and effective_ratio > 0:\n",
    "            mae_mask = self._make_mae_mask(non_empty, device)\n",
    "        if mae_mask is not None:\n",
    "            m4d = mae_mask.view(1, H, W, 1).expand(B, -1, -1, -1)\n",
    "            x = torch.where(m4d, x, self.mask_token.view(1, 1, 1, -1).expand_as(x))\n",
    "        else:\n",
    "            mae_mask = torch.ones(N_full, device=device, dtype=torch.bool)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Run stages, collect intermediates\n",
    "        intermediates = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "            intermediates.append(x)\n",
    "        intermediates[-1] = self.norm(intermediates[-1])\n",
    "\n",
    "        # Build HierarchicalPatchState (coarsest first)\n",
    "        levels = []\n",
    "        for feat in reversed(intermediates):\n",
    "            Bf, Hf, Wf, Cf = feat.shape\n",
    "            n = Hf * Wf\n",
    "            levels.append(PatchState(\n",
    "                emb=feat.reshape(Bf, n, Cf), pos=self._make_grid_pos(Hf, Wf, device),\n",
    "                non_empty=torch.ones(Bf, n, device=device),\n",
    "                mae_mask=torch.ones(n, device=device, dtype=torch.bool)))\n",
    "\n",
    "        return EncoderOutput(patches=HierarchicalPatchState(levels=levels),\n",
    "            full_pos=self._make_grid_pos(grid_h, grid_w, device),\n",
    "            full_non_empty=non_empty, mae_mask=mae_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5babacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae_mask:        torch.Size([4096])\n",
      "full_pos:        torch.Size([4096, 2])\n",
      "full_non_empty:  torch.Size([2, 4096])\n",
      "num levels:      7\n",
      "  level 0: emb=torch.Size([2, 1, 256]), pos=torch.Size([1, 2])  — grid 1×1 (128×128 patch)\n",
      "  level 1: emb=torch.Size([2, 4, 128]), pos=torch.Size([4, 2])  — grid 2×2 (64×64 patches)\n",
      "  level 2: emb=torch.Size([2, 16, 64]), pos=torch.Size([16, 2])  — grid 4×4 (32×32 patches)\n",
      "  level 3: emb=torch.Size([2, 64, 32]), pos=torch.Size([64, 2])  — grid 8×8 (16×16 patches)\n",
      "  level 4: emb=torch.Size([2, 256, 16]), pos=torch.Size([256, 2])  — grid 16×16 (8×8 patches)\n",
      "  level 5: emb=torch.Size([2, 1024, 8]), pos=torch.Size([1024, 2])  — grid 32×32 (4×4 patches)\n",
      "  level 6: emb=torch.Size([2, 4096, 4]), pos=torch.Size([4096, 2])  — grid 64×64 (2×2 patches)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test: verify SwinEncoder output shapes\n",
    "B, C, H, W = 2, 1, 128, 128\n",
    "enc = SwinEncoder(img_height=H, img_width=W)\n",
    "x = torch.randn(B, C, H, W)\n",
    "out = enc(x)\n",
    "\n",
    "print(f'mae_mask:        {out.mae_mask.shape}')\n",
    "print(f'full_pos:        {out.full_pos.shape}')\n",
    "print(f'full_non_empty:  {out.full_non_empty.shape}')\n",
    "print(f'num levels:      {len(out.patches.levels)}')\n",
    "for i, ps in enumerate(out.patches.levels):\n",
    "    g = int(ps.pos.shape[0]**0.5)\n",
    "    p = H // g\n",
    "    print(f'  level {i}: emb={ps.emb.shape}, pos={ps.pos.shape}  — grid {g}×{g} ({p}×{p} patch{\"es\" if i>0 else \"\"})')\n",
    "\n",
    "# Expected hierarchy (coarsest first), 128×128 image, 2×2 patches:\n",
    "#   level 0 (coarsest): emb=(1, 1,    256) — grid 1×1  (CLS-like)\n",
    "#   level 1:            emb=(1, 4,    128) — grid 2×2\n",
    "#   level 2:            emb=(1, 16,    64) — grid 4×4\n",
    "#   level 3:            emb=(1, 64,    32) — grid 8×8\n",
    "#   level 4:            emb=(1, 256,   16) — grid 16×16\n",
    "#   level 5:            emb=(1, 1024,   8) — grid 32×32\n",
    "#   level 6 (finest):   emb=(1, 4096,   4) — grid 64×64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
