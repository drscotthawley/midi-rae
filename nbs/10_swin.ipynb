{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# swin\n",
    "\n",
    "> Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4",
   "metadata": {},
   "source": [
    "## Implementation Plan: `SwinEncoder` — Drop-in Replacement for `ViTEncoder`\n",
    "\n",
    "### Goal\n",
    "\n",
    "Create a `SwinEncoder` class in `midi_rae.swin` that can replace `ViTEncoder` with **zero changes**\n",
    "to calling code. It must accept the same constructor args and return an `EncoderOutput` with the\n",
    "same structure. Import dataclasses from `midi_rae.core`.\n",
    "\n",
    "**Note any upgrades or changes to the PatchState / HierarchicalPatchState / EncoderOutput dataclasses**\n",
    "that would be beneficial for the Swin architecture (e.g. storing window sizes, extra scale metadata),\n",
    "but do NOT implement them yet — just leave TODO comments describing what could change.\n",
    "\n",
    "---\n",
    "\n",
    "### Source Material\n",
    "\n",
    "**Copy** (don't import) the needed components from the timm Swin V2 file at:\n",
    "```\n",
    "/app/data/pytorch-image-models/timm/models/swin_transformer_v2.py\n",
    "```\n",
    "\n",
    "Copy these 5 components into `#| export` cells in this notebook:\n",
    "\n",
    "1. `window_partition(x, window_size)` and `window_reverse(windows, window_size, H, W)` — free functions\n",
    "2. `WindowAttention` — the V2 version with log-scale cosine attention and continuous position bias (CPB) MLP\n",
    "3. `SwinTransformerV2Block` — one transformer block with windowed or shifted-windowed attention\n",
    "4. `PatchMerging` — merges 2×2 patches and projects to higher dim (the downsampling layer between stages)\n",
    "5. `SwinTransformerV2Stage` — a full stage = optional downsample + N blocks\n",
    "\n",
    "**Do NOT copy** the top-level `SwinTransformerV2` class — we will write our own `SwinEncoder` wrapper.\n",
    "\n",
    "---\n",
    "\n",
    "### Handling timm Imports\n",
    "\n",
    "The timm file uses these imports that need replacing:\n",
    "\n",
    "| timm import | Replacement |\n",
    "|---|---|\n",
    "| `from timm.layers import DropPath` | `from torch.nn import Identity` and implement: `DropPath` = stochastic depth. Simple impl: during training, randomly drop the whole residual path with probability `drop_prob`. Or just copy timm's `DropPath` — it's ~15 lines. |\n",
    "| `from timm.layers import Mlp` | Copy or rewrite: it's just `Linear → Act → Dropout → Linear → Dropout`. |\n",
    "| `from timm.layers import to_2tuple` | `def to_2tuple(x): return (x, x) if isinstance(x, int) else tuple(x)` |\n",
    "| `from timm.layers import _assert` | Replace with plain `assert` statements |\n",
    "| `from timm.layers import PatchEmbed` | We do NOT need this — our `SwinEncoder` receives an already-patchified image (see forward spec below) OR we write our own simple conv-based patch embed. |\n",
    "| `from timm.layers import ClassifierHead` | Not needed — we have no classification head |\n",
    "| `from timm.data import IMAGENET_DEFAULT_MEAN/STD` | Not needed |\n",
    "| All model registry decorators (`@register_model`, `generate_default_cfgs`, etc.) | Remove entirely |\n",
    "\n",
    "**Alternatively**, if `timm` is installed in the environment, you may import these utilities directly\n",
    "(`from timm.layers import DropPath, Mlp, to_2tuple`). Check with `import timm` first. If it works,\n",
    "prefer importing over copying for `DropPath` and `Mlp` to reduce code. But `_assert` should still\n",
    "become plain `assert`.\n",
    "\n",
    "---\n",
    "\n",
    "### Existing Interface to Match\n",
    "\n",
    "From `midi_rae/core.py`:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class PatchState:\n",
    "    emb: torch.Tensor       # (B, N, dim) patch embeddings\n",
    "    pos: torch.Tensor       # (N, 2) grid coordinates (row, col)\n",
    "    non_empty: torch.Tensor # (B, N) content mask — 1 where patch has content\n",
    "    mae_mask: torch.Tensor  # (N,) MAE visibility mask — 1=visible, 0=masked\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalPatchState:\n",
    "    levels: list  # list of PatchState, coarsest-first\n",
    "\n",
    "@dataclass\n",
    "class EncoderOutput:\n",
    "    latent: torch.Tensor              # (B, N, dim) final encoder output\n",
    "    patch_state: PatchState            # final-scale patch state\n",
    "    hierarchical: HierarchicalPatchState  # multi-scale states\n",
    "```\n",
    "\n",
    "(Double-check these against the actual `midi_rae/core.py` — the field names above are from memory\n",
    "and may need adjustment. Read the file to confirm.)\n",
    "\n",
    "From `midi_rae/vit.py`, the `ViTEncoder` has roughly this interface:\n",
    "\n",
    "```python\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_height, img_width, patch_h, patch_w,\n",
    "                 in_chans=1, embed_dim=256, depth=6, num_heads=8,\n",
    "                 mlp_ratio=4.0, drop_rate=0.0, mae_ratio=0.0, ...):\n",
    "    def forward(self, x) -> EncoderOutput:\n",
    "        # x: (B, 1, H, W) piano roll image\n",
    "        # returns EncoderOutput\n",
    "```\n",
    "\n",
    "(Again, read the actual file to confirm exact args. The key point: it takes an image and returns `EncoderOutput`.)\n",
    "\n",
    "---\n",
    "\n",
    "### `SwinEncoder` Class Design\n",
    "\n",
    "```python\n",
    "class SwinEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        img_height: int,          # e.g. 128 (MIDI pitch range)\n",
    "        img_width: int,           # e.g. 256 (time steps)\n",
    "        patch_h: int = 4,         # patch height\n",
    "        patch_w: int = 4,         # patch width\n",
    "        in_chans: int = 1,        # piano rolls are single-channel\n",
    "        embed_dim: int = 96,      # Swin default; ViT used 256\n",
    "        depths: tuple = (2, 2, 6, 2),   # blocks per stage\n",
    "        num_heads: tuple = (3, 6, 12, 24),  # heads per stage\n",
    "        window_size: int = 7,     # attention window size\n",
    "        mlp_ratio: float = 4.0,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        mae_ratio: float = 0.0,   # kept for interface compat; NOT used in Swin path\n",
    "    ):\n",
    "```\n",
    "\n",
    "#### Constructor should:\n",
    "\n",
    "1. **Patch embedding**: Use a `nn.Conv2d(in_chans, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))` followed by a `nn.LayerNorm(embed_dim)`. This replaces timm's `PatchEmbed`.\n",
    "   - After conv: reshape from `(B, C, H', W')` to `(B, H'*W', C)` for the transformer.\n",
    "   - Store `self.grid_size = (img_height // patch_h, img_width // patch_w)` — needed by stages.\n",
    "\n",
    "2. **Build stages**: Create `nn.ModuleList` of `SwinTransformerV2Stage` instances.\n",
    "   - Stage 0: no downsampling, dim=embed_dim\n",
    "   - Stages 1+: downsample=True (PatchMerging), dim doubles each stage\n",
    "   - Use stochastic depth with linearly increasing drop path rates across all blocks.\n",
    "   - `input_resolution` for stage 0 = `self.grid_size`; halves each subsequent stage.\n",
    "\n",
    "3. **Final norm**: `nn.LayerNorm(final_dim)` where `final_dim = embed_dim * 2^(num_stages-1)`.\n",
    "\n",
    "4. Store `self.num_stages = len(depths)`.\n",
    "\n",
    "#### Forward should:\n",
    "\n",
    "1. **Patch embed**: `x = self.patch_embed(img)` → reshape to `(B, N, C)`\n",
    "2. **Apply dropout** if configured: `x = self.pos_drop(x)`\n",
    "3. **Run through stages**, collecting intermediate outputs:\n",
    "   ```python\n",
    "   intermediates = []\n",
    "   for stage in self.stages:\n",
    "       x = stage(x)\n",
    "       intermediates.append(x)  # (B, N_i, C_i) at each scale\n",
    "   ```\n",
    "4. **Apply final norm**: `x = self.norm(x)`\n",
    "5. **Build HierarchicalPatchState**: For each intermediate, create a `PatchState`:\n",
    "   - `emb`: the intermediate tensor `(B, N_i, C_i)`\n",
    "   - `pos`: grid coordinates for that scale. Stage i has grid `(H_i, W_i)` where `H_i = grid_H // 2^i`, etc.\n",
    "     Generate with: `torch.stack(torch.meshgrid(torch.arange(H_i), torch.arange(W_i), indexing='ij'), dim=-1).reshape(-1, 2)`\n",
    "   - `non_empty`: `torch.ones(B, N_i, device=x.device)` — we skip empty-patch handling for now\n",
    "   - `mae_mask`: `torch.ones(N_i, device=x.device)` — no MAE masking in Swin path for now\n",
    "   - **Order**: `levels` list should be **coarsest-first** (i.e. reverse of stage order, since stage 0 is finest)\n",
    "6. **Return** `EncoderOutput(latent=x, patch_state=<finest PatchState>, hierarchical=<HierarchicalPatchState>)`\n",
    "\n",
    "---\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- **No CLS token**: Swin doesn't use one — the spatial output IS the representation.\n",
    "- **No RoPE**: Swin V2 uses its own continuous position bias (CPB) MLP. Do not add RoPE.\n",
    "- **No MAE masking during Swin stages**: The windowed attention makes token-level masking complex.\n",
    "  Keep `mae_ratio` in the constructor for interface compatibility but ignore it.\n",
    "  Leave a `# TODO: MAE masking not yet supported in Swin path` comment.\n",
    "- **Window size vs grid size**: If `grid_size` in any dimension is smaller than `window_size`,\n",
    "  the timm code handles this (it adjusts). But document the constraint.\n",
    "- **Data format**: timm's V2 stages expect `(B, H*W, C)` input (NHWC flattened). They internally\n",
    "  reshape to `(B, H, W, C)` for windowing. Check `SwinTransformerV2Stage.forward()` — it takes\n",
    "  `(B, N, C)` and needs to know the spatial dims. The stage stores `input_resolution`.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Cell Order\n",
    "\n",
    "1. Title markdown (already done)\n",
    "2. `#| default_exp swin` (already done)\n",
    "3. `#| hide` + nbdev import (already done)\n",
    "4. **This instruction note** (not exported)\n",
    "5. `#| export` — Imports cell: `torch`, `torch.nn`, `torch.nn.functional`, `math`, `functools.partial`, `from midi_rae.core import PatchState, HierarchicalPatchState, EncoderOutput`\n",
    "6. `#| export` — Helper utilities: `to_2tuple`, `DropPath` (or import from timm), `Mlp` (or import from timm)\n",
    "7. `#| export` — `window_partition` and `window_reverse`\n",
    "8. `#| export` — `WindowAttention`\n",
    "9. `#| export` — `SwinTransformerV2Block`\n",
    "10. `#| export` — `PatchMerging`\n",
    "11. `#| export` — `SwinTransformerV2Stage`\n",
    "12. `#| export` — `SwinEncoder`\n",
    "13. Test cell (not exported): instantiate with typical MIDI piano roll dims and verify output shapes\n",
    "\n",
    "---\n",
    "\n",
    "### Test Cell\n",
    "\n",
    "```python\n",
    "# Test: verify SwinEncoder is a drop-in for ViTEncoder\n",
    "B, C, H, W = 2, 1, 128, 256  # typical piano roll\n",
    "enc = SwinEncoder(\n",
    "    img_height=H, img_width=W,\n",
    "    patch_h=4, patch_w=4,\n",
    "    in_chans=C, embed_dim=96,\n",
    "    depths=(2, 2, 6, 2),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    window_size=8,\n",
    ")\n",
    "x = torch.randn(B, C, H, W)\n",
    "out = enc(x)\n",
    "\n",
    "print(f'latent:       {out.latent.shape}')        # expect (2, N_final, 768)\n",
    "print(f'patch_state:  {out.patch_state.emb.shape}')  # same as latent\n",
    "print(f'num levels:   {len(out.hierarchical.levels)}')\n",
    "for i, ps in enumerate(out.hierarchical.levels):\n",
    "    print(f'  level {i}: emb={ps.emb.shape}, pos={ps.pos.shape}, non_empty={ps.non_empty.shape}')\n",
    "\n",
    "# Expected hierarchy (coarsest first):\n",
    "#   level 0 (coarsest): emb=(2, 4*8, 768),  pos=(32, 2)  — grid 4×8\n",
    "#   level 1:            emb=(2, 8*16, 384),  pos=(128, 2) — grid 8×16\n",
    "#   level 2:            emb=(2, 16*32, 192), pos=(512, 2) — grid 16×32\n",
    "#   level 3 (finest):   emb=(2, 32*64, 96),  pos=(2048, 2) — grid 32×64\n",
    "```\n",
    "\n",
    "Grid sizes above assume `img=128×256`, `patch=4×4` → initial grid `32×64`,\n",
    "then halved at each stage after stage 0."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
