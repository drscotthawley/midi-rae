{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# swin\n",
    "\n",
    "> Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b291f",
   "metadata": {},
   "source": [
    "## Design Overview\n",
    "\n",
    "### What this module does\n",
    "\n",
    "`SwinEncoder` is a drop-in replacement for `ViTEncoder` that uses the **Swin Transformer V2**\n",
    "architecture. It takes a piano roll image `(B, 1, 128, 128)` and returns an `EncoderOutput`\n",
    "with hierarchical multi-scale patch states.\n",
    "\n",
    "### Why Swin V2?\n",
    "\n",
    "- **Hierarchical representation**: 7 levels from finest (64×64 grid, dim=4) down to a single\n",
    "  CLS-like token (1×1, dim=256), compared to ViT's flat single-scale output\n",
    "- **Efficient attention**: Windowed attention with shifted windows — O(N) instead of O(N²)\n",
    "- **V2 improvements**: Cosine attention with learned log-scale temperature, continuous position\n",
    "  bias via CPB MLP, res-post-norm for training stability\n",
    "\n",
    "### Architecture\n",
    "\n",
    "| Stage | Grid | Patch covers | Dim | Depths | Heads |\n",
    "|-------|------|-------------|-----|--------|-------|\n",
    "| 0 | 64×64 | 2×2 | 4 | 1 | 1 |\n",
    "| 1 | 32×32 | 4×4 | 8 | 1 | 1 |\n",
    "| 2 | 16×16 | 8×8 | 16 | 2 | 1 |\n",
    "| 3 | 8×8 | 16×16 | 32 | 2 | 2 |\n",
    "| 4 | 4×4 | 32×32 | 64 | 6 | 4 |\n",
    "| 5 | 2×2 | 64×64 | 128 | 2 | 8 |\n",
    "| 6 | 1×1 | 128×128 | 256 | 1 | 16 |\n",
    "\n",
    "Config is in `configs/config_swin.yaml`.\n",
    "\n",
    "### Implementation approach\n",
    "\n",
    "We use **timm's `SwinTransformerV2Stage` directly** — no copied or modified Swin internals.\n",
    "Our `SwinEncoder` wrapper handles only:\n",
    "\n",
    "1. **Patch embedding** — `Conv2d(1, 4, kernel_size=2, stride=2)` + LayerNorm\n",
    "2. **Empty patch detection** — patches where all pixels are black get a learnable `empty_token`\n",
    "3. **MAE masking** (SimMIM-style) — masked patches get a learnable `mask_token`, grid stays\n",
    "   intact so windowed attention works unmodified. Two-rate sampling: non-empty patches masked\n",
    "   at `mask_ratio`, empty patches at `mask_ratio × empty_mask_ratio` (default 5%)\n",
    "4. **Hierarchical output** — collects each stage's output into `HierarchicalPatchState`\n",
    "   (coarsest-first), packaged as `EncoderOutput`\n",
    "\n",
    "### Key differences from ViTEncoder\n",
    "\n",
    "- No CLS token (stage 6's single 1×1 token serves as a global summary)\n",
    "- No RoPE (Swin V2 uses its own continuous position bias)\n",
    "- MAE masking keeps all tokens (SimMIM-style) — no compute savings but preserves spatial grid\n",
    "- `empty_mask_ratio` controls how often trivial-to-reconstruct empty patches are masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, Set, Type, Union\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.swin_transformer_v2 import SwinTransformerV2Stage\n",
    "from timm.layers import trunc_normal_, to_2tuple, calculate_drop_path_rates\n",
    "from midi_rae.core import PatchState, HierarchicalPatchState, EncoderOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "class SwinEncoder(nn.Module):\n",
    "    \"\"\"Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder.\n",
    "\n",
    "    Uses timm's SwinTransformerV2Stage directly (no copied/modified code).\n",
    "    Adds custom patch embedding, learnable empty/mask tokens, and packages\n",
    "    multi-scale output as EncoderOutput.\n",
    "\n",
    "    Key differences from ViTEncoder:\n",
    "    - No CLS token — Swin uses spatial features directly\n",
    "    - Hierarchical multi-scale output (7 levels by default)\n",
    "    - Windowed attention with shifted windows (no global attention, no RoPE)\n",
    "    - V2 features: cosine attention, log-scale temperature, continuous position bias\n",
    "\n",
    "    # TODO: HierarchicalPatchState could store window_size per level\n",
    "    # TODO: EncoderOutput could store scale metadata (downsample factors per level)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        img_height: int,          # e.g. 128 (MIDI pitch range)\n",
    "        img_width: int,           # e.g. 256 (time steps)\n",
    "        patch_h: int = 2,\n",
    "        patch_w: int = 2,\n",
    "        in_chans: int = 1,        # piano rolls are single-channel\n",
    "        embed_dim: int = 4,       # 2×2 patch → 4 dims; doubles each stage to 256\n",
    "        depths: tuple = (1, 1, 2, 2, 6, 2, 1),\n",
    "        num_heads: tuple = (1, 1, 1, 2, 4, 8, 16),\n",
    "        window_size: int = 8,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        proj_drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.1,\n",
    "        norm_layer: type = nn.LayerNorm,\n",
    "        mae_ratio: float = 0.0,\n",
    "        empty_mask_ratio: float = 0.05,  # fraction of mae_ratio applied to empty patches\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_stages = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_stages - 1))\n",
    "        self.patch_h, self.patch_w = patch_h, patch_w\n",
    "        self.grid_size = (img_height // patch_h, img_width // patch_w)\n",
    "        self.mae_ratio = mae_ratio\n",
    "        self.empty_mask_ratio = empty_mask_ratio\n",
    "\n",
    "        # --- Patch embedding: Conv2d + LayerNorm (replaces timm PatchEmbed) ---\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w),\n",
    "        )\n",
    "        self.patch_norm = norm_layer(embed_dim)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # --- Learnable replacement tokens ---\n",
    "        self.empty_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.mask_token = nn.Parameter(torch.zeros(embed_dim))\n",
    "        # TODO: MAE masking in Swin keeps all tokens (SimMIM-style) — no compute\n",
    "        # savings, but preserves the regular spatial grid that windowed attention needs.\n",
    "\n",
    "        # --- Build stages using timm's SwinTransformerV2Stage ---\n",
    "        embed_dims = [int(embed_dim * 2 ** i) for i in range(self.num_stages)]\n",
    "        dpr = calculate_drop_path_rates(drop_path_rate, list(depths), stagewise=True)\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        in_dim = embed_dims[0]\n",
    "        scale = 1\n",
    "        for i in range(self.num_stages):\n",
    "            out_dim = embed_dims[i]\n",
    "            self.stages.append(SwinTransformerV2Stage(\n",
    "                dim=in_dim,\n",
    "                out_dim=out_dim,\n",
    "                input_resolution=(self.grid_size[0] // scale, self.grid_size[1] // scale),\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size,\n",
    "                downsample=(i > 0),\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                proj_drop=proj_drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "            ))\n",
    "            in_dim = out_dim\n",
    "            if i > 0:\n",
    "                scale *= 2\n",
    "\n",
    "        # --- Final norm ---\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "\n",
    "        # --- Weight init (matches timm SwinTransformerV2) ---\n",
    "        self.apply(self._init_weights)\n",
    "        for stage in self.stages:\n",
    "            stage._init_respostnorm()\n",
    "\n",
    "    def _init_weights(self, m: nn.Module) -> None:\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self) -> Set[str]:\n",
    "        nod = {'empty_token', 'mask_token'}\n",
    "        for n, _ in self.named_parameters():\n",
    "            if any(kw in n for kw in ('cpb_mlp', 'logit_scale')):\n",
    "                nod.add(n)\n",
    "        return nod\n",
    "\n",
    "    def _compute_non_empty(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Detect which patches have content (non-black). Matches PatchEmbedding in vit.py.\"\"\"\n",
    "        patches = img.unfold(2, self.patch_h, self.patch_h).unfold(3, self.patch_w, self.patch_w)\n",
    "        non_empty = (patches.amax(dim=(-1, -2)) > 0.2).squeeze(1).flatten(1)  # (B, N)\n",
    "        return non_empty\n",
    "\n",
    "    def _make_mae_mask(self, non_empty: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Generate MAE mask with two-rate sampling.\n",
    "\n",
    "        Non-empty patches masked at mae_ratio; empty patches masked at\n",
    "        mae_ratio * empty_mask_ratio (much lower — they're trivial to reconstruct).\n",
    "\n",
    "        Returns: (N,) bool — True=visible, False=masked for reconstruction.\n",
    "        \"\"\"\n",
    "        B, N = non_empty.shape\n",
    "        is_nonempty = non_empty[0]  # (N,) — use first sample as representative\n",
    "        rand = torch.rand(N, device=device)\n",
    "        threshold = torch.where(\n",
    "            is_nonempty.bool(),\n",
    "            torch.full_like(rand, 1.0 - self.mae_ratio),\n",
    "            torch.full_like(rand, 1.0 - self.mae_ratio * self.empty_mask_ratio),\n",
    "        )\n",
    "        return rand < threshold  # True = visible\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask_ratio: float = 0.0,\n",
    "                mae_mask: Optional[torch.Tensor] = None) -> EncoderOutput:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, H, W) piano roll image\n",
    "            mask_ratio: override self.mae_ratio for this call (0 = no masking)\n",
    "            mae_mask: (N,) bool, True=visible. If None, generated when ratio > 0.\n",
    "\n",
    "        Returns:\n",
    "            EncoderOutput matching ViTEncoder interface.\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        device = x.device\n",
    "        grid_h, grid_w = self.grid_size\n",
    "        N_full = grid_h * grid_w\n",
    "\n",
    "        # --- Detect empty patches from raw input ---\n",
    "        non_empty = self._compute_non_empty(x)  # (B, N_full)\n",
    "\n",
    "        # --- Patch embed ---\n",
    "        x = self.patch_embed(x)                        # (B, C, H', W')\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()        # → (B, H', W', C)  NHWC\n",
    "        x = self.patch_norm(x)\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        # --- Replace empty patches with learned empty_token ---\n",
    "        non_empty_4d = non_empty.view(B, H, W, 1)\n",
    "        x = torch.where(non_empty_4d, x, self.empty_token.view(1, 1, 1, -1).expand_as(x))\n",
    "\n",
    "        # --- MAE masking: replace masked positions with learned mask_token ---\n",
    "        effective_ratio = mask_ratio if mask_ratio > 0 else self.mae_ratio\n",
    "        if mae_mask is None and effective_ratio > 0:\n",
    "            mae_mask = self._make_mae_mask(non_empty, device)\n",
    "        if mae_mask is not None:\n",
    "            mae_mask_4d = mae_mask.view(1, H, W, 1).expand(B, -1, -1, -1)\n",
    "            x = torch.where(mae_mask_4d, x, self.mask_token.view(1, 1, 1, -1).expand_as(x))\n",
    "        else:\n",
    "            mae_mask = torch.ones(N_full, device=device, dtype=torch.bool)\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # --- Run through stages, collect intermediates in NHWC ---\n",
    "        intermediates = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)                               # (B, H_i, W_i, C_i)\n",
    "            intermediates.append(x)\n",
    "\n",
    "        # --- Apply final norm to last (coarsest) stage ---\n",
    "        intermediates[-1] = self.norm(intermediates[-1])\n",
    "\n",
    "        # --- Build full-resolution grid positions ---\n",
    "        full_pos = torch.stack(torch.meshgrid(\n",
    "            torch.arange(grid_h, device=device),\n",
    "            torch.arange(grid_w, device=device),\n",
    "            indexing='ij',\n",
    "        ), dim=-1).reshape(-1, 2)  # (N_full, 2)\n",
    "\n",
    "        # --- Build HierarchicalPatchState (coarsest first) ---\n",
    "        levels = []\n",
    "        for feat in reversed(intermediates):\n",
    "            Bf, Hf, Wf, Cf = feat.shape\n",
    "            emb = feat.reshape(Bf, Hf * Wf, Cf)\n",
    "            pos = torch.stack(torch.meshgrid(\n",
    "                torch.arange(Hf, device=device),\n",
    "                torch.arange(Wf, device=device),\n",
    "                indexing='ij',\n",
    "            ), dim=-1).reshape(-1, 2)\n",
    "            n = Hf * Wf\n",
    "            levels.append(PatchState(\n",
    "                emb=emb,\n",
    "                pos=pos,\n",
    "                non_empty=torch.ones(Bf, n, device=device),\n",
    "                mae_mask=torch.ones(n, device=device, dtype=torch.bool),\n",
    "            ))\n",
    "\n",
    "        return EncoderOutput(\n",
    "            patches=HierarchicalPatchState(levels=levels),\n",
    "            full_pos=full_pos,\n",
    "            full_non_empty=non_empty,\n",
    "            mae_mask=mae_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5babacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: verify SwinEncoder output shapes\n",
    "B, C, H, W = 2, 1, 128, 128\n",
    "enc = SwinEncoder(img_height=H, img_width=W)\n",
    "x = torch.randn(B, C, H, W)\n",
    "out = enc(x)\n",
    "\n",
    "print(f'mae_mask:        {out.mae_mask.shape}')\n",
    "print(f'full_pos:        {out.full_pos.shape}')\n",
    "print(f'full_non_empty:  {out.full_non_empty.shape}')\n",
    "print(f'num levels:      {len(out.patches.levels)}')\n",
    "for i, ps in enumerate(out.patches.levels):\n",
    "    print(f'  level {i}: emb={ps.emb.shape}, pos={ps.pos.shape}')\n",
    "\n",
    "# Expected hierarchy (coarsest first), 128×128 image, 2×2 patches:\n",
    "#   level 0 (coarsest): emb=(1, 1,    256) — grid 1×1  (CLS-like)\n",
    "#   level 1:            emb=(1, 4,    128) — grid 2×2\n",
    "#   level 2:            emb=(1, 16,    64) — grid 4×4\n",
    "#   level 3:            emb=(1, 64,    32) — grid 8×8\n",
    "#   level 4:            emb=(1, 256,   16) — grid 16×16\n",
    "#   level 5:            emb=(1, 1024,   8) — grid 32×32\n",
    "#   level 6 (finest):   emb=(1, 4096,   4) — grid 64×64"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
