{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dc6903",
   "metadata": {},
   "source": [
    "# viz\n",
    "\n",
    "> vizualization routines\n",
    "\n",
    "\n",
    "**NOTE:** Lazy imports throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93157b0",
   "metadata": {
    "time_run": "2026-02-07T02:57:12.464323+00:00"
   },
   "outputs": [],
   "source": [
    "#| default_exp viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcbb",
   "metadata": {
    "time_run": "2026-02-07T02:57:12.468810+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96051a7",
   "metadata": {
    "time_run": "2026-02-07T02:57:12.519040+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cae58",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c279",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.263961+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cpu_umap_project(embeddings, n_components=3, n_neighbors=15, min_dist=0.1, random_state=42):\n",
    "    \"Project embeddings to n_components dimensions via UMAP (on CPU)\"\n",
    "    import umap\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = embeddings.detach().cpu().numpy()\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
    "    return reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d07d94",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.269872+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cuml_umap_project(embeddings, n_components=3, n_neighbors=15, min_dist=0.1, random_state=42):\n",
    "    \"Project embeddings to n_components dimensions via cuML UMAP (GPU)\"\n",
    "    from cuml import UMAP\n",
    "    import cupy as cp\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = cp.from_dlpack(embeddings.detach())\n",
    "    reducer = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
    "    coords = reducer.fit_transform(embeddings)\n",
    "    del reducer\n",
    "    return cp.asnumpy(coords)  # back to numpy for plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf6f27",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.274348+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def umap_project(embeddings, **kwargs): \n",
    "    \"Calls one of two preceding UMAP routines based on device availability.\"\n",
    "    try:\n",
    "        coords = cuml_umap_project(embeddings, **kwargs)\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        coords = cpu_umap_project(embeddings, **kwargs)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28e033",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fef620",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.278993+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cuml_pca_project(embeddings, n_components=3):\n",
    "    \"Project embeddings to n_components dimensions via cuML PCA (GPU)\"\n",
    "    from cuml import PCA\n",
    "    import cupy as cp\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = cp.from_dlpack(embeddings.detach())\n",
    "    coords = PCA(n_components=n_components).fit_transform(embeddings)\n",
    "    return cp.asnumpy(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d5a1d",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.283564+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cpu_pca_project(embeddings, n_components=3):\n",
    "    \"Project embeddings to n_components dimensions via sklearn PCA (CPU)\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = embeddings.detach().cpu().numpy()\n",
    "    return PCA(n_components=n_components).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52f98b",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.287831+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def pca_project(embeddings, **kwargs):\n",
    "    \"Calls GPU or CPU PCA based on availability\"\n",
    "    try:\n",
    "        return cuml_pca_project(embeddings, **kwargs)\n",
    "    except:\n",
    "        return cpu_pca_project(embeddings, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f9308",
   "metadata": {},
   "source": [
    "## 3D Plotly Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d9cc8",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.295426+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_embeddings_3d(coords, num_tokens, color_by='pairs', file_idx=None, title='Embeddings', debug=False):\n",
    "    \"3D scatter plot of embeddings. color_by: 'none', 'file', or 'pair'\"\n",
    "    import plotly.graph_objects as go\n",
    "    n = len(coords)\n",
    "    if debug: print(\" plot_embeddings_3d: n =\",n)\n",
    "    \n",
    "    if color_by == 'none':     colors = ['blue'] * n\n",
    "    elif color_by == 'file':   colors = file_idx.tolist() if file_idx is not None else ['blue'] * n\n",
    "    elif color_by == 'pairs':\n",
    "        n_pairs = n // 2\n",
    "        pair_colors = [f'rgb({np.random.randint(0,256)},{np.random.randint(0,256)},{np.random.randint(0,256)})' for _ in range(n_pairs)]\n",
    "        colors = [pair_colors[i % n_pairs] for i in range(n)]\n",
    "    else: raise ValueError(f\"Unknown color_by: {color_by}\")\n",
    "\n",
    "    hover_text = [f\"fileid: {int(fid)}\" for fid in file_idx] if file_idx is not None else None\n",
    "    \n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=coords[:,0], y=coords[:,1], z=coords[:,2],\n",
    "        mode='markers', \n",
    "        marker=dict(size=4, color=colors, colorscale='Viridis' if color_by != 'pairs' else None, opacity=0.8),\n",
    "        hovertext=hover_text,\n",
    "        hoverinfo='text' if hover_text else 'x+y+z'\n",
    "    )])\n",
    "    title = title + f', n={n}'\n",
    "    fig.update_layout(title=title, margin=dict(l=0, r=0, b=0, t=30))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1f471",
   "metadata": {},
   "source": [
    "## Main Routine \n",
    "\n",
    "Calls the preceding routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1feb8ca",
   "metadata": {
    "time_run": "2026-02-07T03:16:27.512517+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def _make_emb_viz(zs, num_tokens, epoch=-1, title='Embeddings', do_umap=True, file_idx=None):\n",
    "    \"visualize embeddings, projected\"\n",
    "    fig = None\n",
    "    if do_umap:\n",
    "        coords = umap_project(zs)\n",
    "        umap_fig = plot_embeddings_3d(coords, num_tokens, title=title+f' (UMAP), epoch {epoch}', file_idx=file_idx)\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize() # cleanup before PCA or else you get CUDA errors\n",
    "    gc.collect()\n",
    "    coords = pca_project(zs)\n",
    "    pca_fig = plot_embeddings_3d(coords, num_tokens, title=title+f' (PCA), epoch {epoch}', file_idx=file_idx)\n",
    "    if wandb.run is not None: \n",
    "        if do_umap:\n",
    "            wandb.log({f\"{title} UMAP\": wandb.Html(umap_fig.to_html()), f\"{title} PCA\": wandb.Html(pca_fig.to_html())}, step=epoch)\n",
    "        else:\n",
    "            wandb.log({f\"{title} PCA\": wandb.Html(pca_fig.to_html())}, step=epoch)\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize() # cleanup again\n",
    "    gc.collect()\n",
    "    return pca_fig \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618d453",
   "metadata": {
    "time_run": "2026-02-07T03:08:46.076930+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def _subsample(data, indices, max_points):\n",
    "    \"Subsample data and indices together\"\n",
    "    perm = torch.randperm(len(data))[:max_points]\n",
    "    return data[perm], indices[perm] if indices is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818edfa",
   "metadata": {
    "time_run": "2026-02-07T03:13:00.890996+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_emb_viz(zs,  num_tokens, epoch=-1, model=None, title='Embeddings', max_points=8192, pmask=None, file_idx=None, do_umap=True):\n",
    "    \"this is the main routine, showing different groups of embeddings\"\n",
    "    device = zs.device\n",
    "    if model is not None: model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # CLS tokens\n",
    "    cls_tokens = zs[::num_tokens]\n",
    "    cls_file_idx = file_idx[::num_tokens] if file_idx is not None else None\n",
    "    _make_emb_viz(cls_tokens, num_tokens, epoch=epoch, title='CLS Tokens'+title, file_idx=cls_file_idx, do_umap=do_umap)\n",
    "    \n",
    "    # Patches (non-CLS)\n",
    "    patch_mask = torch.arange(len(zs)) % num_tokens != 0\n",
    "    patch_only = zs[patch_mask]\n",
    "    patch_file_idx = file_idx[patch_mask] if file_idx is not None else None\n",
    "    \n",
    "    if pmask is not None:\n",
    "        patch_pmask = pmask[:, 1:].flatten().bool()\n",
    "        print(f\"Non-empty patches: {patch_pmask.sum()}/{len(patch_pmask)} ({patch_pmask.float().mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Non-empty patches\n",
    "        valid_patches, valid_file_idx = patch_only[patch_pmask], (patch_file_idx[patch_pmask] if patch_file_idx is not None else None)\n",
    "        rnd_patches, rnd_file_idx = _subsample(valid_patches, valid_file_idx, max_points)\n",
    "        pca_fig = _make_emb_viz(rnd_patches, num_tokens, epoch=epoch, title='RND Patches'+title, file_idx=rnd_file_idx, do_umap=do_umap)\n",
    "        \n",
    "        # Empty patches\n",
    "        empty_patches, empty_file_idx = patch_only[~patch_pmask], (patch_file_idx[~patch_pmask] if patch_file_idx is not None else None)\n",
    "        rnd_empty, rnd_empty_idx = _subsample(empty_patches, empty_file_idx, max_points)\n",
    "        pca_fig = _make_emb_viz(rnd_empty, num_tokens, epoch=epoch, title='RND Empty Patches'+title, do_umap=False, file_idx=rnd_empty_idx)\n",
    "    \n",
    "    if model is not None: model.to(device)\n",
    "    return pca_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b458db6",
   "metadata": {
    "time_run": "2026-02-07T02:58:53.887199+00:00"
   },
   "source": [
    "Testing visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6451ed3",
   "metadata": {
    "time_run": "2026-02-07T03:21:49.709887+00:00"
   },
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "bs, num_tokens, dim = 32, 65, 256\n",
    "z1 = torch.randn([bs, num_tokens, dim])\n",
    "z2 = z1 +  0.1*torch.randn([bs, num_tokens, dim])\n",
    "zs = torch.cat([z1, z2], dim=0).view(-1, dim)  # flatten to [64*65, 256]\n",
    "#file_idx = torch.arange(bs).repeat_interleave(num_tokens)\n",
    "file_idx = torch.arange(bs).repeat(2).repeat_interleave(num_tokens)\n",
    "\n",
    "pmask = torch.ones([2*bs, num_tokens])\n",
    "pmask[:, 30:] = 0  # mark roughly half the patches as empty\n",
    "pca_fig = make_emb_viz(zs,  num_tokens, title='testing', pmask=pmask, file_idx=file_idx, do_umap=False) \n",
    "#pca_fig.to_html()  # Sadly, SolveIt will not render this, so you have to check this on another Jupyter Notebook system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225e230",
   "metadata": {
    "solveit_ai": true,
    "time_run": "2026-02-07T03:22:22.603052+00:00"
   },
   "source": [
    "We're not seeing the plotly plot appear, instead I'm just seeing a bunch of raw HTML code. How can we make the plotly plot visible and solve it? This is testing code to make sure that the interactivity works and that the mouse over labeling of the file indices work. We can't use a static image and we can't use matplotlib. How do we get this working?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "**Context note:** Some earlier messages were truncated from the dialog, so I don't have the full history of what you've tried.\n",
    "\n",
    "For Plotly in solveit, try using IPython's HTML display:\n",
    "\n",
    "```python\n",
    "from IPython.display import HTML\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x=[1,2,3], y=[1,4,9])\n",
    "HTML(fig.to_html(include_plotlyjs='cdn'))\n",
    "```\n",
    "\n",
    "This renders the plot as interactive HTML with mouse-over labels intact. Give that a try and let me know what you see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde7b2",
   "metadata": {
    "time_run": "2026-02-07T02:57:15.320475+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
