{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dc6903",
   "metadata": {},
   "source": [
    "# viz\n",
    "\n",
    "> vizualization routines\n",
    "\n",
    "\n",
    "**NOTE:** Lazy imports throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93157b0",
   "metadata": {
    "time_run": "2026-02-11T02:51:45.442809+00:00"
   },
   "outputs": [],
   "source": [
    "#| default_exp viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcbb",
   "metadata": {
    "time_run": "2026-02-11T02:51:45.470457+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96051a7",
   "metadata": {
    "time_run": "2026-02-11T02:51:45.521968+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cae58",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c279",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.243680+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cpu_umap_project(embeddings, n_components=3, n_neighbors=15, min_dist=0.1, random_state=42):\n",
    "    \"Project embeddings to n_components dimensions via UMAP (on CPU)\"\n",
    "    import umap\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = embeddings.detach().cpu().numpy()\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
    "    return reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d07d94",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.248291+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cuml_umap_project(embeddings, n_components=3, n_neighbors=15, min_dist=0.1, random_state=42):\n",
    "    \"Project embeddings to n_components dimensions via cuML UMAP (GPU)\"\n",
    "    from cuml import UMAP\n",
    "    import cupy as cp\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = cp.from_dlpack(embeddings.detach())\n",
    "    reducer = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=random_state)\n",
    "    coords = reducer.fit_transform(embeddings)\n",
    "    del reducer\n",
    "    return cp.asnumpy(coords)  # back to numpy for plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf6f27",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.252499+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def umap_project(embeddings, **kwargs): \n",
    "    \"Calls one of two preceding UMAP routines based on device availability.\"\n",
    "    try:\n",
    "        coords = cuml_umap_project(embeddings, **kwargs)\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        coords = cpu_umap_project(embeddings, **kwargs)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28e033",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fef620",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.259186+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cuml_pca_project(embeddings, n_components=3):\n",
    "    \"Project embeddings to n_components dimensions via cuML PCA (GPU)\"\n",
    "    from cuml import PCA\n",
    "    import cupy as cp\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = cp.from_dlpack(embeddings.detach())\n",
    "    coords = PCA(n_components=n_components).fit_transform(embeddings)\n",
    "    return cp.asnumpy(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d5a1d",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.263805+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def cpu_pca_project(embeddings, n_components=3):\n",
    "    \"Project embeddings to n_components dimensions via sklearn PCA (CPU)\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    if isinstance(embeddings, torch.Tensor): embeddings = embeddings.detach().cpu().numpy()\n",
    "    return PCA(n_components=n_components).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52f98b",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.268317+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def pca_project(embeddings, **kwargs):\n",
    "    \"Calls GPU or CPU PCA based on availability\"\n",
    "    try:\n",
    "        return cuml_pca_project(embeddings, **kwargs)\n",
    "    except:\n",
    "        return cpu_pca_project(embeddings, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f9308",
   "metadata": {},
   "source": [
    "## 3D Plotly Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d9cc8",
   "metadata": {
    "time_run": "2026-02-11T03:16:34.664365+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_embeddings_3d(coords, num_tokens, color_by='pairs', file_idx=None, title='Embeddings', debug=False):\n",
    "    \"3D scatter plot of embeddings. color_by: 'none', 'file', or 'pair'\"\n",
    "    import plotly.graph_objects as go\n",
    "    n = len(coords)\n",
    "    if debug: print(\" plot_embeddings_3d: n =\",n)\n",
    "    \n",
    "    if color_by == 'none':     colors = ['blue'] * n\n",
    "    elif color_by == 'file':   colors = file_idx.tolist() if file_idx is not None else ['blue'] * n\n",
    "    elif color_by == 'pairs':\n",
    "        n_pairs = n // 2\n",
    "        pair_colors = [f'rgb({np.random.randint(0,256)},{np.random.randint(0,256)},{np.random.randint(0,256)})' for _ in range(n_pairs)]\n",
    "        colors = [pair_colors[i % n_pairs] for i in range(n)]  # pairs separated by num_tokens \n",
    "    else: raise ValueError(f\"Unknown color_by: {color_by}\")\n",
    "\n",
    "    hover_text = [f\"file_id: {int(fid)}\" for fid in file_idx] if file_idx is not None else None\n",
    "    if color_by == 'pairs':\n",
    "        hover_text = [f\"pair {i%n_pairs}\" for i in range(n)] if hover_text is None else [f\"{s}, pair {i%n_pairs}\" for i, s in enumerate(hover_text)]\n",
    "\n",
    "    \n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=coords[:,0], y=coords[:,1], z=coords[:,2],\n",
    "        mode='markers', \n",
    "        marker=dict(size=4, color=colors, colorscale='Viridis' if color_by != 'pairs' else None, opacity=0.8),\n",
    "        hovertext=hover_text, hoverinfo='x+y+z+text' if hover_text else 'x+y+z'\n",
    "    )])\n",
    "    title = title + f', n={n}'\n",
    "    fig.update_layout(title=title, margin=dict(l=0, r=0, b=0, t=30))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1f471",
   "metadata": {},
   "source": [
    "## Main Routine \n",
    "\n",
    "Calls the preceding routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1feb8ca",
   "metadata": {
    "time_run": "2026-02-11T02:51:48.280969+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def _make_emb_viz(zs, num_tokens, epoch=-1, title='Embeddings', do_umap=True, file_idx=None):\n",
    "    \"visualize embeddings, projected\"\n",
    "    umap_fig = None\n",
    "    if do_umap:\n",
    "        coords = umap_project(zs)\n",
    "        umap_fig = plot_embeddings_3d(coords, num_tokens, title=title+f' (UMAP), epoch {epoch}', file_idx=file_idx)\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize() # cleanup before PCA or else you get CUDA errors\n",
    "    gc.collect()\n",
    "    coords = pca_project(zs)\n",
    "    pca_fig = plot_embeddings_3d(coords, num_tokens, title=title+f' (PCA), epoch {epoch}', file_idx=file_idx)\n",
    "    if wandb.run is not None: \n",
    "        if do_umap:\n",
    "            wandb.log({f\"{title} UMAP\": wandb.Html(umap_fig.to_html()), f\"{title} PCA\": wandb.Html(pca_fig.to_html())}, step=epoch)\n",
    "        else:\n",
    "            wandb.log({f\"{title} PCA\": wandb.Html(pca_fig.to_html())}, step=epoch)\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize() # cleanup again\n",
    "    gc.collect()\n",
    "    return pca_fig, umap_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618d453",
   "metadata": {
    "time_run": "2026-02-11T03:13:27.062171+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def _subsample(data, indices, max_points, debug=False):\n",
    "    \"Subsample data and indices together, in pairs\"\n",
    "    perm1 = torch.randperm(len(data)//2)[:max_points//2]\n",
    "    perm2 = perm1 + len(data)//2\n",
    "    perm = torch.cat([perm1,perm2])\n",
    "    return data[perm], indices[perm] if indices is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442994a5",
   "metadata": {},
   "source": [
    "Testing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7cdbf",
   "metadata": {
    "time_run": "2026-02-11T03:13:50.447396+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zs.shape =  torch.Size([10, 1])\n",
      "zs = \n",
      " tensor([[  0],\n",
      "        [200],\n",
      "        [400],\n",
      "        [600],\n",
      "        [800],\n",
      "        [  1],\n",
      "        [201],\n",
      "        [401],\n",
      "        [601],\n",
      "        [801]])\n",
      "indices = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "data_perm.shape =  torch.Size([6, 1]) , data_perm = \n",
      " tensor([[  0],\n",
      "        [200],\n",
      "        [400],\n",
      "        [  1],\n",
      "        [201],\n",
      "        [401]])\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "n_pairs, dim = 5, 1  # data points\n",
    "z1 = 200*torch.arange(n_pairs).unsqueeze(-1).unsqueeze(-1)\n",
    "z2 = z1 + 1 \n",
    "zs = torch.cat([z1, z2], dim=0).view(-1, dim)\n",
    "print(\"zs.shape = \",zs.shape)\n",
    "indices = torch.arange(2*n_pairs)\n",
    "print(\"zs = \\n\",zs)\n",
    "print(\"indices =\",indices)\n",
    "data_perm, indices2 = _subsample(zs, indices, max_points=2*(n_pairs-2), debug=True)\n",
    "print(\"data_perm.shape = \",data_perm.shape,\", data_perm = \\n\",data_perm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c3a13",
   "metadata": {},
   "source": [
    "Yes. That does what I expect. Moving on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2818edfa",
   "metadata": {
    "time_run": "2026-02-11T03:15:56.782492+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_emb_viz(zs,  \n",
    "                num_tokens, epoch=-1, \n",
    "                model=None, \n",
    "                title='Embeddings', \n",
    "                max_points=5000, \n",
    "                pmask=None, \n",
    "                file_idx=None, \n",
    "                do_umap=True):\n",
    "    \"this is the main routine, showing different groups of embeddings\"\n",
    "    device = zs.device\n",
    "    if model is not None: model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if file_idx is not None and file_idx.shape[0] < zs.shape[0]:\n",
    "        file_idx = file_idx.repeat(2).repeat_interleave(num_tokens).to(device)\n",
    "\n",
    "    # CLS tokens\n",
    "    cls_tokens = zs[::num_tokens]\n",
    "    cls_file_idx = file_idx[::num_tokens] if file_idx is not None else None\n",
    "\n",
    "    cls_pca_fig, cls_umap_fig = _make_emb_viz(cls_tokens, num_tokens, epoch=epoch, title='CLS Tokens '+title, file_idx=cls_file_idx, do_umap=do_umap)\n",
    "    \n",
    "    # Patches (non-CLS)\n",
    "    patch_mask = torch.arange(len(zs)) % num_tokens != 0\n",
    "    patch_only = zs[patch_mask]\n",
    "    patch_file_idx = file_idx[patch_mask] if file_idx is not None else None\n",
    "    \n",
    "    if pmask is not None:\n",
    "        patch_pmask = pmask[:, 1:].flatten().bool()\n",
    "        print(f\"Non-empty patches: {patch_pmask.sum()}/{len(patch_pmask)} ({patch_pmask.float().mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Non-empty patches\n",
    "        valid_patches, valid_file_idx = patch_only[patch_pmask], (patch_file_idx[patch_pmask] if patch_file_idx is not None else None)\n",
    "        rnd_patches, rnd_file_idx = _subsample(valid_patches, valid_file_idx, max_points)\n",
    "        patch_pca_fig, patch_umap_fig = _make_emb_viz(rnd_patches, num_tokens, epoch=epoch, title='RND Patches '+title, file_idx=rnd_file_idx, do_umap=do_umap)\n",
    "        \n",
    "        # Empty patches\n",
    "        empty_patches, empty_file_idx = patch_only[~patch_pmask], (patch_file_idx[~patch_pmask] if patch_file_idx is not None else None)\n",
    "        rnd_empty, rnd_empty_idx = _subsample(empty_patches, empty_file_idx, max_points)\n",
    "        empty_pca_fig = _make_emb_viz(rnd_empty, num_tokens, epoch=epoch, title='RND Empty Patches '+title, do_umap=False, file_idx=rnd_empty_idx)\n",
    "    \n",
    "    if model is not None: model.to(device)\n",
    "    figs = {'cls_pca_fig':cls_pca_fig, 'cls_umap_fig':cls_umap_fig, 'patch_pca_fig':patch_pca_fig, 'patch_umap_fig':patch_umap_fig, 'empty_pca_fig': empty_pca_fig}\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b458db6",
   "metadata": {},
   "source": [
    "Testing visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6451ed3",
   "metadata": {
    "time_run": "2026-02-11T03:16:42.195118+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zs.shape =  torch.Size([4160, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty patches: 1856/4096 (45.3%)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "bs, num_tokens, dim = 32, 65, 256\n",
    "z1 = torch.randn([bs, num_tokens, dim])\n",
    "file_idx = torch.arange(bs)\n",
    "z2 = z1 +  0.1*torch.randn([bs, num_tokens, dim]) # z2 is slightly shifted from z1\n",
    "\n",
    "zs = torch.cat([z1, z2], dim=0).view(-1, dim)  # combine z's one after the other & flatten to [64*65, 256]\n",
    "print(\"zs.shape = \",zs.shape) \n",
    "file_idx = file_idx.repeat(2).repeat_interleave(num_tokens) \n",
    "\n",
    "pmask = torch.ones([2*bs, num_tokens])  # all ones\n",
    "pmask[:, 30:] = 0  # mark roughly half the patches as empty\n",
    "\n",
    "figs = make_emb_viz(zs,  num_tokens, title='testing', pmask=pmask, file_idx=file_idx, do_umap=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb5eb0",
   "metadata": {
    "skipped": true
   },
   "outputs": [],
   "source": [
    "#| eval: false \n",
    "figs['patch_pca_fig'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde7b2",
   "metadata": {
    "time_run": "2026-02-11T02:51:51.226611+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
