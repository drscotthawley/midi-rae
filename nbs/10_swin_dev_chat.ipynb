{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# swin\n",
    "\n",
    "> Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b291f",
   "metadata": {},
   "source": [
    "## Design Overview\n",
    "\n",
    "### What this module does\n",
    "\n",
    "`SwinEncoder` is a drop-in replacement for `ViTEncoder` that uses the **Swin Transformer V2**\n",
    "architecture. It takes a piano roll image `(B, 1, 128, 128)` and returns an `EncoderOutput`\n",
    "with hierarchical multi-scale patch states.\n",
    "\n",
    "### Why Swin V2?\n",
    "\n",
    "- **Hierarchical representation**: 7 levels from finest (64×64 grid, dim=4) down to a single\n",
    "  CLS-like token (1×1, dim=256), compared to ViT's flat single-scale output\n",
    "- **Efficient attention**: Windowed attention with shifted windows — O(N) instead of O(N²)\n",
    "- **V2 improvements**: Cosine attention with learned log-scale temperature, continuous position\n",
    "  bias via CPB MLP, res-post-norm for training stability\n",
    "\n",
    "### Architecture\n",
    "\n",
    "| Stage | Grid | Patch covers | Dim | Depths | Heads |\n",
    "|-------|------|-------------|-----|--------|-------|\n",
    "| 0 | 64×64 | 2×2 | 4 | 1 | 1 |\n",
    "| 1 | 32×32 | 4×4 | 8 | 1 | 1 |\n",
    "| 2 | 16×16 | 8×8 | 16 | 2 | 1 |\n",
    "| 3 | 8×8 | 16×16 | 32 | 2 | 2 |\n",
    "| 4 | 4×4 | 32×32 | 64 | 6 | 4 |\n",
    "| 5 | 2×2 | 64×64 | 128 | 2 | 8 |\n",
    "| 6 | 1×1 | 128×128 | 256 | 1 | 16 |\n",
    "\n",
    "Config is in `configs/config_swin.yaml`.\n",
    "\n",
    "### Implementation approach\n",
    "\n",
    "We use **timm's `SwinTransformerV2Stage` directly** — no copied or modified Swin internals.\n",
    "Our `SwinEncoder` wrapper handles only:\n",
    "\n",
    "1. **Patch embedding** — `Conv2d(1, 4, kernel_size=2, stride=2)` + LayerNorm\n",
    "2. **Empty patch detection** — patches where all pixels are black get a learnable `empty_token`\n",
    "3. **MAE masking** (SimMIM-style) — masked patches get a learnable `mask_token`, grid stays\n",
    "   intact so windowed attention works unmodified. Two-rate sampling: non-empty patches masked\n",
    "   at `mask_ratio`, empty patches at `mask_ratio × empty_mask_ratio` (default 5%)\n",
    "4. **Hierarchical output** — collects each stage's output into `HierarchicalPatchState`\n",
    "   (coarsest-first), packaged as `EncoderOutput`\n",
    "\n",
    "### Key differences from ViTEncoder\n",
    "\n",
    "- No CLS token (stage 6's single 1×1 token serves as a global summary)\n",
    "- No RoPE (Swin V2 uses its own continuous position bias)\n",
    "- MAE masking keeps all tokens (SimMIM-style) — no compute savings but preserves spatial grid\n",
    "- `empty_mask_ratio` controls how often trivial-to-reconstruct empty patches are masked\n",
    "\n",
    "### TODOs\n",
    "\n",
    "- `HierarchicalPatchState` could store `window_size` per level\n",
    "- `EncoderOutput` could store scale metadata (downsample factors per level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, Set, Type, Union\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.swin_transformer_v2 import SwinTransformerV2Stage\n",
    "from timm.layers import trunc_normal_, to_2tuple, calculate_drop_path_rates\n",
    "from midi_rae.core import PatchState, HierarchicalPatchState, EncoderOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "class SwinEncoder(nn.Module):\n",
    "    \"Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder.  (Wrapper for timm routines)\"\n",
    "    def __init__(self,\n",
    "                 img_height:int,           # Input image height in pixels (e.g. 128)\n",
    "                 img_width:int,            # Input image width in pixels (e.g. 128)\n",
    "                 patch_h:int=2,            # Patch height for initial embedding\n",
    "                 patch_w:int=2,            # Patch width for initial embedding\n",
    "                 in_chans:int=1,           # Number of input channels (1 for piano roll)\n",
    "                 embed_dim:int=4,          # Base embedding dimension (doubles each stage)\n",
    "                 depths:tuple=(1,1,2,2,6,2,1),   # Number of transformer blocks per stage\n",
    "                 num_heads:tuple=(1,1,1,2,4,8,16),# Attention heads per stage\n",
    "                 window_size:int=8,        # Window size for windowed attention\n",
    "                 mlp_ratio:float=4.,       # MLP hidden dim = embed_dim * mlp_ratio\n",
    "                 qkv_bias:bool=True,       # Add bias to QKV projections\n",
    "                 drop_rate:float=0.,       # Dropout after patch embedding\n",
    "                 proj_drop_rate:float=0.,  # Dropout after attention projection\n",
    "                 attn_drop_rate:float=0.,  # Dropout on attention weights\n",
    "                 drop_path_rate:float=0.1, # Stochastic depth rate\n",
    "                 norm_layer:type=nn.LayerNorm, # Normalization layer class\n",
    "                 mae_ratio:float=0.,       # Fraction of non-empty patches to mask (0=no masking)\n",
    "                 empty_mask_ratio:float=0.05): # Mask rate for empty patches relative to mae_ratio\n",
    "        super().__init__()\n",
    "        self.num_stages, self.embed_dim = len(depths), embed_dim\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_stages - 1))\n",
    "        self.patch_h, self.patch_w, self.grid_size = patch_h, patch_w,  (img_height // patch_h, img_width // patch_w)\n",
    "        self.mae_ratio, self.empty_mask_ratio = mae_ratio, empty_mask_ratio\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))\n",
    "        self.patch_norm = norm_layer(embed_dim)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Learnable replacement tokens\n",
    "        self.empty_token, self.mask_token = nn.Parameter(torch.zeros(embed_dim)), nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "        # Build stages using timm's SwinTransformerV2Stage\n",
    "        embed_dims = [int(embed_dim * 2 ** i) for i in range(self.num_stages)]\n",
    "        dpr = calculate_drop_path_rates(drop_path_rate, list(depths), stagewise=True)\n",
    "        self.stages = nn.ModuleList()\n",
    "        in_dim, scale = embed_dims[0], 1\n",
    "        for i in range(self.num_stages):\n",
    "            out_dim = embed_dims[i]\n",
    "            self.stages.append(SwinTransformerV2Stage(\n",
    "                dim=in_dim, out_dim=out_dim, depth=depths[i], num_heads=num_heads[i],\n",
    "                input_resolution=(self.grid_size[0] // scale, self.grid_size[1] // scale),\n",
    "                window_size=window_size, downsample=(i > 0), mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, proj_drop=proj_drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer))\n",
    "            in_dim = out_dim\n",
    "            if i > 0: scale *= 2\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.apply(self._init_weights)\n",
    "        for stage in self.stages: stage._init_respostnorm()\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self) -> Set[str]:\n",
    "        nod = {'empty_token', 'mask_token'}\n",
    "        for n, _ in self.named_parameters():\n",
    "            if any(kw in n for kw in ('cpb_mlp', 'logit_scale')): nod.add(n)\n",
    "        return nod\n",
    "\n",
    "    def _compute_non_empty(self, img):\n",
    "        \"Detect which patches have content (non-black).\"\n",
    "        patches = img.unfold(2, self.patch_h, self.patch_h).unfold(3, self.patch_w, self.patch_w)\n",
    "        return (patches.amax(dim=(-1, -2)) > 0.2).squeeze(1).flatten(1)  # (B, N)\n",
    "\n",
    "    def _make_mae_mask(self, non_empty, device):\n",
    "        \"Two-rate MAE mask: non-empty at mae_ratio, empty at mae_ratio*empty_mask_ratio. Returns (N,) bool, True=visible.\"\n",
    "        B, N = non_empty.shape\n",
    "        is_nonempty = non_empty[0]  # use first sample as representative\n",
    "        rand = torch.rand(N, device=device)\n",
    "        threshold = torch.where(is_nonempty.bool(),\n",
    "            torch.full_like(rand, 1.0 - self.mae_ratio),\n",
    "            torch.full_like(rand, 1.0 - self.mae_ratio * self.empty_mask_ratio))\n",
    "        return rand < threshold\n",
    "\n",
    "    def _make_grid_pos(self, h, w, device):\n",
    "        \"Build (h*w, 2) grid positions.\"\n",
    "        return torch.stack(torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij'), dim=-1).reshape(-1, 2)\n",
    "\n",
    "    def forward(self, x, mask_ratio:float=0., mae_mask:Optional[torch.Tensor]=None) -> EncoderOutput:\n",
    "        \"x: (B,C,H,W) piano roll. mask_ratio overrides self.mae_ratio. mae_mask: (N,) bool, True=visible.\"\n",
    "        B, device = x.shape[0], x.device\n",
    "        grid_h, grid_w = self.grid_size\n",
    "        N_full = grid_h * grid_w\n",
    "\n",
    "        non_empty = self._compute_non_empty(x)                              # (B, N_full)\n",
    "        x = self.patch_embed(x)                                             # (B, C, H', W')\n",
    "        x = self.patch_norm(x.permute(0, 2, 3, 1).contiguous())            # → (B, H', W', C) NHWC\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        # Replace empty patches with learned empty_token\n",
    "        ne4d = non_empty.view(B, H, W, 1)\n",
    "        x = torch.where(ne4d, x, self.empty_token.view(1, 1, 1, -1).expand_as(x))\n",
    "\n",
    "        # MAE masking: replace masked positions with learned mask_token\n",
    "        effective_ratio = mask_ratio if mask_ratio > 0 else self.mae_ratio\n",
    "        if mae_mask is None and effective_ratio > 0:\n",
    "            mae_mask = self._make_mae_mask(non_empty, device)\n",
    "        if mae_mask is not None:\n",
    "            m4d = mae_mask.view(1, H, W, 1).expand(B, -1, -1, -1)\n",
    "            x = torch.where(m4d, x, self.mask_token.view(1, 1, 1, -1).expand_as(x))\n",
    "        else:\n",
    "            mae_mask = torch.ones(N_full, device=device, dtype=torch.bool)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Run stages, collect intermediates\n",
    "        intermediates = []\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "            intermediates.append(x)\n",
    "        intermediates[-1] = self.norm(intermediates[-1])\n",
    "\n",
    "        # Build non-empty masks for each scale via max-pool cascade\n",
    "        ne = non_empty.view(B, 1, grid_h, grid_w).float()\n",
    "        ne_scales = []\n",
    "        for feat in intermediates:\n",
    "            Hf, Wf = feat.shape[1], feat.shape[2]\n",
    "            while ne.shape[2] > Hf:\n",
    "                ne = F.max_pool2d(ne, 2)\n",
    "            ne_scales.append(ne.view(B, -1))\n",
    "\n",
    "        # Build HierarchicalPatchState (coarsest first)\n",
    "        levels = []\n",
    "        for feat, ne_s in zip(reversed(intermediates), reversed(ne_scales)):\n",
    "            Bf, Hf, Wf, Cf = feat.shape\n",
    "            n = Hf * Wf\n",
    "            levels.append(PatchState(\n",
    "                emb=feat.reshape(Bf, n, Cf), pos=self._make_grid_pos(Hf, Wf, device),\n",
    "                non_empty=ne_s,\n",
    "                mae_mask=torch.ones(n, device=device, dtype=torch.bool)))\n",
    "\n",
    "        return EncoderOutput(patches=HierarchicalPatchState(levels=levels),\n",
    "            full_pos=self._make_grid_pos(grid_h, grid_w, device),\n",
    "            full_non_empty=non_empty, mae_mask=mae_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5babacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae_mask:        torch.Size([4096])\n",
      "full_pos:        torch.Size([4096, 2])\n",
      "full_non_empty:  torch.Size([2, 4096])\n",
      "num levels:      7\n",
      "  level 0: emb=torch.Size([2, 1, 256]), pos=torch.Size([1, 2])  — grid 1×1 (128×128 patch)\n",
      "  level 1: emb=torch.Size([2, 4, 128]), pos=torch.Size([4, 2])  — grid 2×2 (64×64 patches)\n",
      "  level 2: emb=torch.Size([2, 16, 64]), pos=torch.Size([16, 2])  — grid 4×4 (32×32 patches)\n",
      "  level 3: emb=torch.Size([2, 64, 32]), pos=torch.Size([64, 2])  — grid 8×8 (16×16 patches)\n",
      "  level 4: emb=torch.Size([2, 256, 16]), pos=torch.Size([256, 2])  — grid 16×16 (8×8 patches)\n",
      "  level 5: emb=torch.Size([2, 1024, 8]), pos=torch.Size([1024, 2])  — grid 32×32 (4×4 patches)\n",
      "  level 6: emb=torch.Size([2, 4096, 4]), pos=torch.Size([4096, 2])  — grid 64×64 (2×2 patches)\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test: verify SwinEncoder output shapes\n",
    "B, C, H, W = 2, 1, 128, 128\n",
    "enc = SwinEncoder(img_height=H, img_width=W)\n",
    "x = torch.randn(B, C, H, W)\n",
    "out = enc(x)\n",
    "\n",
    "print(f'mae_mask:        {out.mae_mask.shape}')\n",
    "print(f'full_pos:        {out.full_pos.shape}')\n",
    "print(f'full_non_empty:  {out.full_non_empty.shape}')\n",
    "print(f'num levels:      {len(out.patches.levels)}')\n",
    "for i, ps in enumerate(out.patches.levels):\n",
    "    g = int(ps.pos.shape[0]**0.5)\n",
    "    p = H // g\n",
    "    print(f'  level {i}: emb={ps.emb.shape}, pos={ps.pos.shape}  — grid {g}×{g} ({p}×{p} patch{\"es\" if i>0 else \"\"})')\n",
    "\n",
    "# Expected hierarchy (coarsest first), 128×128 image, 2×2 patches:\n",
    "#   level 0 (coarsest): emb=(1, 1,    256) — grid 1×1  (CLS-like)\n",
    "#   level 1:            emb=(1, 4,    128) — grid 2×2\n",
    "#   level 2:            emb=(1, 16,    64) — grid 4×4\n",
    "#   level 3:            emb=(1, 64,    32) — grid 8×8\n",
    "#   level 4:            emb=(1, 256,   16) — grid 16×16\n",
    "#   level 5:            emb=(1, 1024,   8) — grid 32×32\n",
    "#   level 6 (finest):   emb=(1, 4096,   4) — grid 64×64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28b475",
   "metadata": {},
   "source": [
    "Testing code to check for non-empty patches: Green equals non-empty, red equals empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 91 val files from ~/datasets/POP909_images/...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x270 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from midi_rae.data import PRPairDataset\n",
    "\n",
    "# Load one image from the dataset\n",
    "ds = PRPairDataset(split='val')\n",
    "img_tensor = ds[0]['img1'][:1]\n",
    "x = img_tensor.unsqueeze(0)\n",
    "\n",
    "# Run empty patch detection\n",
    "enc = SwinEncoder(img_height=128, img_width=128)\n",
    "non_empty = enc._compute_non_empty(x)\n",
    "ne = non_empty[0].reshape(1, 1, 64, 64).float()\n",
    "\n",
    "# Build hierarchy via max-pool cascade\n",
    "levels = [ne[0, 0].cpu()]  # 64×64\n",
    "while levels[-1].shape[0] > 1:\n",
    "    ne = F.max_pool2d(ne, 2)\n",
    "    levels.append(ne[0, 0].cpu())\n",
    "\n",
    "# Plot: original image + all levels\n",
    "fig, axes = plt.subplots(1, len(levels) + 1, figsize=(16, 2.7))\n",
    "axes[0].imshow(img_tensor[0].cpu(), cmap='gray', origin='lower', aspect='auto')\n",
    "axes[0].set_title('Original\\n128×128 px')\n",
    "for i, grid in enumerate(levels):\n",
    "    g = grid.shape[0]\n",
    "    axes[i+1].imshow(grid.numpy(), cmap='RdYlGn', origin='lower', aspect='auto', vmin=0, vmax=1)\n",
    "    p = 128 // g\n",
    "    axes[i+1].set_title(f'Level {i}\\n{g}×{g} ({p}×{p}px)')\n",
    "    axes[i+1].axis('off')\n",
    "axes[0].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cd765",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Okay, that visualization code we just did, I noticed you've got you added this code that says build hierarchy via max pool cascade.  we're probably gonna need something like that built into our encoder itself, though, aren't we? Or do we not need to keep track of non-empty tokens inside the encoder?\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Great observation! Look at what's happening in your `forward` method when you build the `HierarchicalPatchState` — for every level you're setting:\n",
    "\n",
    "```python\n",
    "non_empty=torch.ones(Bf, n, device=device)\n",
    "```\n",
    "\n",
    "So right now the coarser levels all claim every patch is non-empty, which isn't correct. The downstream decoder presumably uses `non_empty` to know which patches matter, so it needs accurate masks at each scale.\n",
    "\n",
    "The max-pool cascade from the visualization is exactly the right idea — a coarse patch is non-empty if *any* of its constituent fine patches are non-empty. You'd want to compute that inside `forward` and store the correct mask per level.\n",
    "\n",
    "Same issue applies to `mae_mask` at coarser levels, by the way — currently also just `torch.ones`. Do you want to tackle `non_empty` first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003751f",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Yeah, so go back into the encoder. And add the logic for that non-empty cascade to accurately track non-empty pixels. You can probably edit the cell in place, or if you're worried, write to a new code cell, and then we can delete the old cell.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "I'll edit the cell in place. The change is in the hierarchy-building section of `forward` — I'll add a max-pool cascade to compute `non_empty` at each scale, then use those instead of `torch.ones`.\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>msg_str_replace(id='_1347f17a', old_str='<…>…erarchicalPatchState (coarsest…</…>', new_str='<…>…n-empty masks for each scale v…</…>')</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01H8o7ELkLckGmqQfLingk6J\",\n",
    "  \"call\": {\n",
    "    \"function\": \"msg_str_replace\",\n",
    "    \"arguments\": {\n",
    "      \"id\": \"_1347f17a\",\n",
    "      \"old_str\": \"# Build HierarchicalPatchState (coarsest first)\\n        levels = []\\n        for feat in reversed(intermediates):\\n            Bf, Hf, Wf, Cf = feat.shape\\n            n = Hf * Wf\\n            levels.append(PatchState(\\n                emb=feat.reshape(Bf, n, Cf), pos=self._make_grid_pos(Hf, Wf, device),\\n                non_empty=torch.ones(Bf, n, device=device),\\n                mae_mask=torch.ones(n, device=device, dtype=torch.bool)))\",\n",
    "      \"new_str\": \"# Build non-empty masks for each scale via max-pool cascade\\n        ne = non_empty.view(B, 1, grid_h, grid_w).float()\\n        ne_scales = []\\n        for feat in intermediates:\\n            Hf, Wf = feat.shape[1], feat.shape[2]\\n            while ne.shape[2] > Hf:\\n                ne = F.max_pool2d(ne, 2)\\n            ne_scales.append(ne.view(B, -1))\\n\\n        # Build HierarchicalPatchState (coarsest first)\\n        levels = []\\n        for feat, ne_s in zip(reversed(intermediates), reversed(ne_scales)):\\n            Bf, Hf, Wf, Cf = feat.shape\\n            n = Hf * Wf\\n            levels.append(PatchState(\\n                emb=feat.reshape(Bf, n, Cf), pos=self._make_grid_pos(Hf, Wf, device),\\n                non_empty=ne_s,\\n                mae_mask=torch.ones(n, device=device, dtype=torch.bool)))\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"@@ -120,5 +120,14 @@\\n \\n+        # Build non-empty masks for each scale via max-pool cascade\\n+        ne = non_empty.view(B, 1, grid_h, grid_w).float()\\n+        ne_scales = []\\n+        for feat in intermediates:\\n+            Hf, Wf = feat.shape[1], feat.shape[2]\\n+            while ne.shape[2] > Hf:\\n+                ne = F.max_pool2d(ne, 2)\\n+            ne_scales.append(ne.view(B, -1))\\n+\\n         # Build HierarchicalPatchState (coarsest first)\\n         levels = []\\n-        for feat in reversed(intermediates):\\n+        for feat, ne_s in zip(reversed(intermediates), reversed(ne_scales)):\\n             Bf, Hf, Wf, Cf = feat.shape\\n@@ -127,3 +136,3 @@\\n                 emb=feat.reshape(Bf, n, Cf), pos=self._make_grid_pos(Hf, Wf, device),\\n-                non_empty=torch.ones(Bf, n, device=device),\\n+                non_empty=ne_s,\\n                 mae_mask=torch.ones(n, device=device, dtype=torch.bool)))\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Done. The logic:\n",
    "\n",
    "1. Reshapes the finest-level `non_empty` to `(B, 1, 64, 64)` \n",
    "2. Walks through intermediates (finest→coarsest), max-pooling by 2 whenever the grid is larger than the current stage's resolution\n",
    "3. Each level now gets its real `non_empty` mask — a coarse patch is non-empty if *any* of its fine patches were\n",
    "\n",
    "You can re-run your visualization test to verify the masks match. And as I mentioned, `mae_mask` has the same `torch.ones` issue at coarser levels — want to tackle that next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca11a3",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Well for the MAE mask actually to me it kind of makes sense that it might be a little random and salt and peppery because then that forces the model to be able to fill in different levels of the hierarchy and thereby learn robust representations, right?\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "That's a really good point. The masking happens at the finest level, and those masked tokens flow *through* all the Swin stages. So coarser representations are already built from a mixture of real and masked-token inputs — the model is forced to learn to aggregate robustly even with partial information feeding up from below.\n",
    "\n",
    "So `torch.ones` at coarser levels is actually a reasonable choice — it's saying \"we don't consider these coarse patches themselves to be masked.\" The one scenario where you'd want accurate coarse-level masks is if you ever add a **reconstruction loss at coarser scales** (e.g. predicting downsampled piano rolls). But if reconstruction is only at the finest level, you're fine as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3ff1f",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Oh no, I'm sorry, my mistake. I misunderstood. I think courser tokens should get masked out as well. But I thought that that was just going to happen randomly. That, you know, given all patches at all levels of the hierarchy, we would be randomly masking some of them.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Ah I see — so you're thinking of independent random masking at each level of the hierarchy in the output. So the `mae_mask` in each `PatchState` wouldn't be `torch.ones` or propagated from the finest level, but its own random mask.\n",
    "\n",
    "Just to make sure I understand: this would be for the **decoder side** — telling it which patches to reconstruct at each scale? The encoder itself would still only do SimMIM-style masking at the input (finest level), but then the `HierarchicalPatchState` would have per-level masks that the decoder uses for its reconstruction targets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bef98",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I mean, I guess you're a little more sophisticated than me. In my mind, we need to mask at all levels of the hierarchy so that the model is forced to learn to reconstruct, to learn robust, interdependent tokens so that if the one of the coarser tokens gets masked, thenen that can get filled in by using information from the other tokens. But maybe I'm just mixed up in the head.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "No, you're not mixed up at all — that's a solid intuition! If a coarse token gets masked, the model has to reconstruct it from its neighbors at the same scale *and* from the finer-scale tokens below it and/or from the coarser tokens above it. That forces the hierarchy to be genuinely interdependent rather than redundant.\n",
    "\n",
    "So the masking would happen in the `PatchState` outputs — each level gets its own random mask, and the decoder is responsible for filling in the gaps using cross-scale information. The encoder just marks which ones are masked; it doesn't need to do anything differently internally.\n",
    "\n",
    "Want me to add per-level random masking to the hierarchy-building section?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c3939",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Well, the fact that you're asking me all this, makes me imagine that what I'm proposing is non-standard. Do other people who do the swin MAE type stuff, like the swin mae code we scraped earlier that's in the `/tmp/Swin-MAE/` directory, do they not mask at all levels? Do they only mask at the finest level? I mean, I guess you could do that, but that seems pretty weak if we're talking about representation learning. Some of those models, though, were probably for medical imagery, where they were mainly just interested in image segmentation and not truly robust multi-scale representation learning, which I am.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Good question — let me check what the Swin-MAE code actually does.\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>view(path='/tmp/Swin-MAE/')</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_0176kt8ocWd4ZdCcEkRH88KV\",\n",
    "  \"call\": {\n",
    "    \"function\": \"view\",\n",
    "    \"arguments\": {\n",
    "      \"path\": \"/tmp/Swin-MAE/\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"Directory contents of /tmp/Swin-MAE:\\n/tmp/Swin-MAE/visualize.py (2.6k)\\n/tmp/Swin-MAE/requirements.txt (0.1k)\\n/tmp/Swin-MAE/LICENSE (1.7k)\\n/tmp/Swin-MAE/train.py (5.8k)\\n/tmp/Swin-MAE/utils \\ud83d\\udcc1\\n/tmp/Swin-MAE/README.md (1.4k)\\n/tmp/Swin-MAE/swin_mae.py (9.9k)\\n/tmp/Swin-MAE/swin_unet.py (17.1k)\\n/tmp/Swin-MAE/utils/lr_sched.py (0.8k)\\n/tmp/Swin-MAE/utils/pos_embed.py (4.0k)\\n/tmp/Swin-MAE/utils/engine_pretrain.py (2.5k)\\n/tmp/Swin-MAE/utils/misc.py (12.2k)\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>view(path='/tmp/Swin-MAE/swin_mae.py')</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01CUyMNnKQRxb8S9xTEUvcHA\",\n",
    "  \"call\": {\n",
    "    \"function\": \"view\",\n",
    "    \"arguments\": {\n",
    "      \"path\": \"/tmp/Swin-MAE/swin_mae.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"<TRUNCATED>\\u2026ools import partial\\n\\nimport torch\\nimport torch.nn as nn\\nimport numpy as np\\nfrom einops import rearrange\\n\\nfrom swin_unet import PatchEmbedding, BasicBlock, PatchExpanding, BasicBlockUp\\nfrom utils.pos_embed import get_2d_sincos_pos_embed\\n\\n\\nclass SwinMAE(nn.Module):\\n    \\\"\\\"\\\"\\n    Masked Auto Encoder with Swin Transformer backbone\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, img_size: int = 224, patch_size: int = 4, mask_ratio: float = 0.75, in_chans: int = 3,\\n                 decoder_embed_dim=512, norm_pix_loss=False,\\n                 depths: tuple = (2, 2, 6, 2), embed_dim: int = 96, num_heads: tuple = (3, 6, 12, 24),\\n                 window_size: int = 7, qkv_bias: bool = True, mlp_ratio: float = 4.,\\n                 drop_path_rate: float = 0.1, drop_rate: float = 0., attn_drop_rate: float = 0.,\\n                 norm_layer=None, patch_norm: bool = True):\\n        super().__init__()\\n        self.mask_ratio = mask_ratio\\n        assert img_size % patch_size == 0\\n        self.num_patch\\u2026</TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "So you're right — this Swin-MAE implementation **only masks at the finest level**. The `window_masking` method operates on the patch-embedded tokens before they enter any stages, replacing masked patches with `mask_token`. Then the encoder stages run normally, and the decoder reconstructs just the original pixel-level patches.\n",
    "\n",
    "There's no per-level masking at all. The loss is only computed at the finest scale too — `forward_loss` compares `pred` to `patchify(imgs)`.\n",
    "\n",
    "So what you're proposing — independent masking at each level of the hierarchy — would be genuinely novel compared to this. It makes sense for your use case: you care about learning robust multi-scale representations, not just pixel reconstruction. These medical imaging folks just needed the encoder for downstream segmentation.\n",
    "\n",
    "Want to go ahead and add per-level random masking to the `PatchState` outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772fd29",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Well, hang on. It makes sense to do the masking on the finest level because that's where your reconstruction occurs. There's no straight-up reconstruction signal for for the embeddings further up in the hierarchy because they're more abstract. So, without some other pre-trained neural network to compare embeddings against. Maybe the finest level is actually the only thing that we can use the masking on. I don't know. I just thought there ought to be some kind of internal consistency. Maybe I'm too influenced by multi-grid methods with their prolongation and interpolation operators, which obviously isn't the same thing as this. I don't know, I just imagine that people have dealt with this before us, right?\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "You're absolutely right to question it — you *do* need a target signal to train against, and at coarser levels there's no obvious ground truth. Let me see what's out there on this.\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Great finds. There are two relevant threads here:\n",
    "\n",
    "1. [*](https://deepai.org/publication/point-m2ae-multi-scale-masked-autoencoders-for-hierarchical-point-cloud-pre-training \"Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training | DeepAI\") **Point-M2AE** (NeurIPS 2022) proposes a Multi-scale MAE pre-training framework for hierarchical self-supervised learning. They modify the encoder and decoder into pyramid architectures to progressively model spatial geometries, and design a multi-scale masking strategy to generate consistent visible regions across scales.\n",
    "\n",
    "2. [*](https://arxiv.org/html/2405.10642v1 \"Hi-GMAE: Hierarchical Graph Masked Autoencoders\") **Hi-GMAE** (for graphs) does \"Coarse-to-Fine masking with Recovery\" — designed to maintain the consistency of masked subgraphs across all scales, starting with random masking at the coarsest graph and [*](https://arxiv.org/html/2405.10642v1 \"Hi-GMAE: Hierarchical Graph Masked Autoencoders\") projecting the mask matrix back to finer scales using an unpooling operation.\n",
    "\n",
    "3. The **self-distillation** approach (iBOT, MaskCLIP, etc.) solves the \"what's the target?\" problem directly: [*](https://arxiv.org/html/2408.06687v1 \"Masked Image Modeling: A Survey\") a self-distillation masked autoencoder framework where an input image is divided into patches, some randomly masked. Two encoder-decoder networks (teacher and student) reconstruct the original image, with the objective of minimizing the distance between their predictions. The teacher's weights are computed as an exponential moving average of the student's.\n",
    "\n",
    "So your intuition about internal consistency is sound — and self-distillation is probably the cleanest answer to your \"what's the target?\" concern. The EMA teacher gives you a target at *every* level of abstraction without needing ground-truth pixels. Want to dig into Point-M2AE or the self-distillation approach more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475f045",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Well, actually number two sounded the most appealing to me. Why did that one not stick out to you as the obvious choice?  I mean, yeah, I'm not sure about masking at the coarsest level., if that's the way to go. Again, I was just sort of imagining masking randomly throughout the entire hierarchy. which sometimes would include the coarsest level and sometimes would not.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "You're right to call me out — Hi-GMAE's approach is actually the closest to what you're describing. The reason I didn't jump on it is that it goes coarse-to-fine (mask at the top, project downward), whereas you're describing something more like independent random masking at every level. Those are subtly different.\n",
    "\n",
    "But here's the thing: if you mask independently at each level, then a coarse token might be masked while all its children are visible. What should the model reconstruct that coarse token *from*? The fine tokens below it can provide the answer — and that's essentially a **self-consistency** objective: \"the coarse token should be whatever you'd get by aggregating the fine tokens.\" You don't need an external target because the hierarchy *is* the target.\n",
    "\n",
    "Does that match your intuition from the multi-grid world — that prolongation/restriction should be internally consistent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a969244",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "> But here's the thing: if you mask independently at each level, then a coarse token might be masked while all its children are visible. \n",
    "\n",
    "Yeah, no shit. I get that. And then the job of the network is to reconstruct the course representation from its visible children. But I'm not exactly sure what you mean by internally consistent. I mean, there's going to be some kind of nonlinear mapping between the different levels because the hierarchical, the coarser levels are going to involve more abstract representations covering longer time periods and more pitches. But that's built into the swing transformer architecture, isn't it?  ..... Oh wait. Okay, hang on. So right now, all of the information flow goes from the finer resolution patches up to the coarse ones.  We don't have anything that maps back the other way.  Maybe that's what you were talking about when you were talking about the decoder. But in my mind that wasn't necessarily a decoder thing, that was just a way of Yeah, kinda doing the masking.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Exactly — you've hit the key issue. The Swin encoder is strictly bottom-up: fine→coarse. So if you mask a coarse token in the *output*, nothing in the current architecture can reconstruct it from the fine tokens, because the fine tokens already *produced* it on the way up.\n",
    "\n",
    "For multi-scale masking to actually create a training signal, you need something that can move information in *both* directions across scales — fine→coarse AND coarse→fine. That's what a U-Net-style decoder with skip connections would give you, and it's exactly what the Swin-MAE code uses (`PatchExpanding` + `BasicBlockUp`).\n",
    "\n",
    "So it's not really a \"masking\" question — it's an architecture question about whether you have bidirectional information flow across scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddecef99",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Okay, maybe I'm overthinking all this. I really just naively thought we were gonna randomly knock out patches here and there, you know, like we were basically doing dropout only with patches. \n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Oh! That's a much simpler idea than what we've been discussing — and it's a good one. You're basically talking about **patch dropout**: just randomly zero out or replace patches at each level of the hierarchical output as a regularization technique. No reconstruction loss needed at those positions. The decoder just has to cope with missing information at every scale.\n",
    "\n",
    "That's totally different from MAE-style \"mask and reconstruct.\" It's closer to standard dropout, and it would absolutely encourage robust, non-redundant representations across scales.\n",
    "\n",
    "Is that what you had in mind?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a52da",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "> The decoder just has to cope with missing information at every scale.\n",
    "\n",
    "Well, but the encoder does too. The encoder's job is going to be to produce reasonable encodings even in the face of masked out empty patches along the way or whatever.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Right, good correction. If the masking happens *between* stages inside the encoder — not just on the output — then the later stages have to produce good coarse representations from incomplete finer representations. That's where the regularization pressure actually lives.\n",
    "\n",
    "So you'd zero out (or replace with a mask token) some patches at each stage transition, before they feed into the next downsampling step. The encoder learns to be robust at every level, not just at the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19f76a",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Well, that's what I've been naively assuming, but if that's not a standard thing to do, then maybe I've been assuming too much.  This isn't a principled targeted design by which I'm trying to be innovative, it's just what I thought was involved.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "It's not crazy — it's a natural extension of ideas that *are* standard. You already have `drop_path_rate` in your config, which randomly drops entire transformer blocks during training. What you're describing is the same intuition applied at the patch level between stages rather than the block level within stages.\n",
    "\n",
    "The closest established technique is probably **patch dropout** (Beymer et al., Liu et al.), but that's usually applied at the input for compute savings, not between stages for regularization. Doing it inter-stage in a hierarchical encoder isn't widely adopted, but it's principled — it's just dropout at a coarser granularity.\n",
    "\n",
    "Want to try it as a simple experiment? It'd be a small change — just drop some patches between stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303e58f",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "I guess I'm still a little bit lost on why this is such a huge distinction. It's just masking, right? We're randomly blocking out parts of the representation and then the network is going to have to fill them in. To me, that's masking. And it's not just on the decoder end. The idea is to force a robust set of representations that are descriptive about what's going on musically.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "You're right, and I was overcomplicating it. It *is* just masking. The only distinction that actually matters is: do you add a reconstruction loss at the masked positions, or do you just let the network cope? And you've already answered that — the reconstruction loss is at the finest level (pixels), and everything else is just \"deal with it.\"\n",
    "\n",
    "So: mask some patches at every stage, reconstruct at the pixel level, and the encoder is forced to build robust representations throughout the hierarchy. Simple. Shall we add it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75dc77",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "So but it sounds like what we don't have is a way for a non-masked Coarse patch to inform the embeddings of the masked finer patches. Unless you mean inside the decoder when we're at the very last finest level next to the reconstruction output. Nah, fuck it. Yeah, you're right. When we train this thing end to end, because we're going to have a little bit of reconstruction loss, that is going to propagate all the way through at all scales. And yeah, the network's going to just have to learn to deal with it. That's kind of the thought.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Exactly. The reconstruction loss at the bottom sends gradients all the way back through every stage. If stage 4 had some patches masked, stages 5 and 6 still had to produce something useful from what was left — and the gradient signal from pixel reconstruction will tell them whether they did a good job. The whole hierarchy learns to be resilient.\n",
    "\n",
    "Shall we add the inter-stage masking to the encoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba20b1",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Okay, I'm going to make a copy of this dialogue and call it something like Swin Design Chat so that we can keep the main Swin dialogue clean and lean. So, hang on a second while I do that.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "Sure, take your time! I'll be here when you're ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b31150",
   "metadata": {
    "solveit_ai": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
