{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b916516",
   "metadata": {},
   "source": [
    "# train_enc\n",
    "\n",
    "> Encoder training script for midi_rae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905c98",
   "metadata": {
    "time_run": "2026-02-15T20:40:46.736852+00:00"
   },
   "outputs": [],
   "source": [
    "#| default_exp train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d15b2",
   "metadata": {
    "time_run": "2026-02-15T20:40:46.741459+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edfbef",
   "metadata": {
    "time_run": "2026-02-15T20:40:46.795933+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "from midi_rae.vit import ViTEncoder, LightweightMAEDecoder\n",
    "from midi_rae.data import PRPairDataset\n",
    "from midi_rae.losses import calc_enc_loss, calc_mae_loss\n",
    "from midi_rae.utils import save_checkpoint\n",
    "from midi_rae.viz import make_emb_viz, viz_mae_recon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b6854",
   "metadata": {},
   "source": [
    "## Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4f354",
   "metadata": {
    "time_run": "2026-02-15T20:40:49.513622+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def curr_learn(shared_ct_dict, epoch, interval=100, verbose=False): \n",
    "    \"UNUSED/UNNECESSARY: curriculum learning: increase difficulty with epoch\"\n",
    "    if epoch < interval: return shared_ct_dict['training']\n",
    "    training = shared_ct_dict['training']\n",
    "    training['max_shift_x'] = min(12, 6 + epoch // interval)\n",
    "    training['max_shift_y'] = min(12, 6 + epoch // interval)\n",
    "    if verbose: \n",
    "        print(f\"curr_learn: max_shift_x = {training['max_shift_x']}, max_shift_y = {training['max_shift_y']}\")\n",
    "    return training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5526a",
   "metadata": {},
   "source": [
    "## Compute Loss On Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6162a",
   "metadata": {
    "time_run": "2026-02-15T20:54:06.638754+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_batch_loss(batch, encoder, cfg, global_step, mae_decoder=None): \n",
    "    \"Compute loss and return other exal auxiliary variables (for train or val)\"\n",
    "    device = next(encoder.parameters()).device\n",
    "    img1, img2, deltas = batch['img1'].to(device), batch['img2'].to(device), batch['deltas'].to(device)\n",
    "    z1, pmask1, pos1, mae_mask1 = encoder(img1, return_cls_only=False) \n",
    "    z2, pmask2, pos2, mae_mask2 = encoder(img2, return_cls_only=False, mae_mask=mae_mask1) # same mask for both\n",
    "    loss_dict = {} \n",
    "    if mae_decoder is not None:\n",
    "        recon_patches = mae_decoder(z2, pos2, mae_mask2) # just pick z2 and ignore z1 \n",
    "        loss_dict['mae'] = calc_mae_loss(recon_patches, img2, pos2, mae_mask2)\n",
    "\n",
    "    z1 = z1.reshape(-1, z1.shape[-1])\n",
    "    z2 = z2.reshape(-1, z2.shape[-1])\n",
    "    num_tokens =  z1.shape[0] // len(deltas)  # or just 65\n",
    "    deltas = deltas.repeat_interleave(num_tokens, dim=0)\n",
    "    loss_dict = loss_dict | calc_enc_loss(z1, z2, global_step, deltas=deltas, lambd=cfg.training.lambd, pmasks=(pmask1,pmask2))\n",
    "    if 'mae' in loss_dict.keys(): loss_dict['loss'] += cfg.training.get('mae_lambda', 1.0) * loss_dict['mae'] \n",
    "\n",
    "    return loss_dict, z1, z2, pmask1, pmask2, pos1, pos2, mae_mask2, num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a91da1",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713ab74",
   "metadata": {
    "time_run": "2026-02-15T20:54:29.307273+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\n",
    "def train(cfg: DictConfig):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(\"device = \",device)\n",
    "\n",
    "    train_ds = PRPairDataset(split='train', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    val_ds   = PRPairDataset(split='val',  max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # next bit is to enable curriculum learning, dataloaders re-defined per epoch, gotta use nested function\n",
    "    manager = mp.Manager()\n",
    "    shared_ct_dict = manager.dict(OmegaConf.to_container(cfg))\n",
    "    def worker_init_fn(worker_id):\n",
    "        ds = torch.utils.data.get_worker_info().dataset\n",
    "        ds.max_shift_x = shared_ct_dict['training']['max_shift_x']\n",
    "        ds.max_shift_y = shared_ct_dict['training']['max_shift_y']\n",
    "\n",
    "    model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), cfg.model.patch_size, \n",
    "              cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n",
    "    model = torch.compile(model)\n",
    "    mae_decoder = LightweightMAEDecoder(patch_size=cfg.model.patch_size, dim=cfg.model.dim).to(device)\n",
    "\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.training.lr)\n",
    "    optimizer = torch.optim.AdamW(chain(model.parameters(), mae_decoder.parameters()), lr=cfg.training.lr)\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=cfg.training.lr, steps_per_epoch=1, epochs=cfg.training.epochs)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    if not(cfg.get('no_wandb', False)): wandb.init(project=cfg.wandb.project, config=dict(cfg), settings=wandb.Settings(start_method=\"fork\", _disable_stats=True))\n",
    "\n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    viz_every = 10\n",
    "    for epoch in range(1, cfg.training.epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        if False and epoch > 1: # curriculum learning, easily turned off by setting this to False. DL's re-defined each epoch to init workers\n",
    "            shared_ct_dict['training'] = curr_learn(shared_ct_dict, epoch)\n",
    "            train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)\n",
    "            val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)\n",
    "        for batch in tqdm(train_dl, desc=f\"Epoch {epoch}/{cfg.training.epochs}\"):\n",
    "            global_step += 1\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast('cuda'):\n",
    "                loss_dict, z1, z2, pmask1, pmask2, pos1, pos2, mae_mask2, num_tokens = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "            scaler.scale(loss_dict['loss']).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss_dict['loss'].item()\n",
    "            \n",
    "        # At end of Epoch: validation, viz, etc\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                val_loss_dict, z1, z2, pmask1, pmask2, pos1, pos2, mae_mask2, num_tokens = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "                val_loss += val_loss_dict['loss'].item()\n",
    "\n",
    "        train_loss /= len(train_dl)\n",
    "        val_loss /= len(val_dl)\n",
    "        print(f\"Epoch {epoch}/{cfg.training.epochs}: train_loss={train_loss:.3f} val_loss={val_loss:.3f}\")\n",
    "        \n",
    "        if wandb.run is not None:\n",
    "            wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "               \"train_sim\": loss_dict['sim'], \"train_sigreg\": loss_dict['sigreg'], \"train_anchor\":loss_dict['anchor'], \"train_mae\":loss_dict['mae'],\n",
    "               \"val_sim\": val_loss_dict['sim'], \"val_sigreg\": val_loss_dict['sigreg'], \"val_anchor\": val_loss_dict['anchor'], \"val_mae\": val_loss_dict['mae'],\n",
    "               \"max_shift_x\":shared_ct_dict['training']['max_shift_x'], \"max_shift_y\":shared_ct_dict['training']['max_shift_y'],\n",
    "               \"lr\": optimizer.param_groups[0]['lr'], \"epoch\": epoch}, step=epoch)\n",
    "\n",
    "            if epoch % viz_every == 0:\n",
    "                zs_stacked = torch.cat((z1, z2), dim=0).reshape(-1, z1.shape[-1])\n",
    "                make_emb_viz(zs_stacked, num_tokens, epoch, model=model, pmasks=(pmask1,pmask2), file_idx=batch['file_idx'], deltas=batch['deltas'])\n",
    "                if mae_decoder is not None:\n",
    "                    patches_recon = mae_decoder(z2, pos2, mae_mask2)\n",
    "                    viz_mae_recon(patches_recon, pos, batch['img2'], epoch=epoch)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, cfg, tag=\"enc_\")\n",
    "        scheduler.step()# val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd0128",
   "metadata": {},
   "source": [
    "## CLI Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55b9c3",
   "metadata": {
    "time_run": "2026-02-15T20:40:49.535634+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "if __name__ == \"__main__\" and \"ipykernel\" not in __import__(\"sys\").modules:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977fc5c",
   "metadata": {
    "time_run": "2026-02-15T20:40:49.541746+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
