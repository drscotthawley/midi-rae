{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b916516",
   "metadata": {},
   "source": [
    "# train_enc\n",
    "\n",
    "> Encoder training script for midi_rae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98edfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "os.environ[\"TORCHINDUCTOR_FX_GRAPH_CACHE\"] = \"1\"\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import wandb\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "from midi_rae.core import *\n",
    "from midi_rae.vit import ViTEncoder, LightweightMAEDecoder\n",
    "from midi_rae.swin import SwinEncoder, SwinMAEDecoder\n",
    "from midi_rae.data import PRPairDataset\n",
    "from midi_rae.losses import calc_enc_loss, calc_mae_loss, calc_enc_loss_multiscale\n",
    "from midi_rae.utils import save_checkpoint, load_checkpoint\n",
    "from midi_rae.viz import make_emb_viz, viz_mae_recon\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b6854",
   "metadata": {},
   "source": [
    "## Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d4f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def curr_learn(shared_ct_dict, epoch, interval=100, verbose=False): \n",
    "    \"UNUSED/UNNECESSARY: curriculum learning: increase difficulty with epoch\"\n",
    "    if epoch < interval: return shared_ct_dict['training']\n",
    "    training = shared_ct_dict['training']\n",
    "    training['max_shift_x'] = min(12, 6 + epoch // interval)\n",
    "    training['max_shift_y'] = min(12, 6 + epoch // interval)\n",
    "    if verbose: \n",
    "        print(f\"curr_learn: max_shift_x = {training['max_shift_x']}, max_shift_y = {training['max_shift_y']}\")\n",
    "    return training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5526a",
   "metadata": {},
   "source": [
    "## Compute Loss On Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f6162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.compiler.disable\n",
    "def compute_batch_loss(batch, encoder, cfg, global_step, mae_decoder=None, debug=False):\n",
    "    \"Compute loss and return other exal auxiliary variables (for train or val)\"\n",
    "    device = next(encoder.parameters()).device\n",
    "    img1, img2, deltas = batch['img1'].to(device), batch['img2'].to(device), batch['deltas'].to(device)\n",
    "    enc_out1 = encoder(img1, mask_ratio=cfg.training.mask_ratio)\n",
    "    enc_out2 = encoder(img2, mae_mask=enc_out1.mae_mask)\n",
    "    loss_dict = {}\n",
    "    recon_patches = None\n",
    "    if mae_decoder is not None: # recon at finest scale\n",
    "        eo = enc_out2.patches[-1] # readibility/convenience variable\n",
    "        recon_patches = mae_decoder(enc_out2) \n",
    "        if debug:\n",
    "            for i, lv in enumerate(enc_out2.patches.levels):\n",
    "                print(f\"level {i}: {lv.emb.isnan().any()}, min={lv.emb.min():.4f}, max={lv.emb.max():.4f}\")\n",
    "        loss_dict['mae'] = calc_mae_loss(recon_patches, img2, enc_out2, lambda_visible=cfg.training.get('lambda_visible',0.1))\n",
    "\n",
    "    if isinstance(enc_out1.patches, HierarchicalPatchState):\n",
    "        z1 = [lvl.emb for lvl in enc_out1.patches.levels]\n",
    "        z2 = [lvl.emb for lvl in enc_out2.patches.levels]\n",
    "        non_emptys = [(l1.non_empty, l2.non_empty) for l1, l2 in zip(enc_out1.patches.levels, enc_out2.patches.levels)]\n",
    "    else:\n",
    "        z1 = enc_out1.patches.all_emb.reshape(-1, enc_out1.patches[1].dim)\n",
    "        z2 = enc_out2.patches.all_emb.reshape(-1, enc_out2.patches[1].dim)\n",
    "        non_emptys = (enc_out1.patches.all_non_empty, enc_out2.patches.all_non_empty)\n",
    "\n",
    "    #for i, z in enumerate(z1): print(f\"level {i}: shape={z.shape}, norm={z.norm():.4f}, min={z.min():.4f}, max={z.max():.4f}\")\n",
    "    loss_dict = loss_dict | calc_enc_loss_multiscale(z1, z2, global_step, img_size=cfg.data.image_size, deltas=deltas, lambd=cfg.training.lambd, non_emptys=non_emptys)\n",
    "\n",
    "    if 'mae' in loss_dict.keys(): loss_dict['loss'] += cfg.training.get('mae_lambda', 1.0) * loss_dict['mae']\n",
    "\n",
    "    if torch.isnan(loss_dict['loss']):\n",
    "        print(\"NaN detected!\", {k: v.item() if hasattr(v, 'item') else v for k, v in loss_dict.items()})\n",
    "        breakpoint()\n",
    "    return loss_dict, (z1, z2), (enc_out1, enc_out2), recon_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a91da1",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\n",
    "def train(cfg: DictConfig):\n",
    "    print(\"config:\",cfg)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(\"device = \",device)\n",
    "\n",
    "    train_ds = PRPairDataset(image_dataset_dir=cfg.data.path, split='train', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    val_ds   = PRPairDataset(image_dataset_dir=cfg.data.path, split='val',  max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=8, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    # next bit is to enable curriculum learning, dataloaders re-defined per epoch, gotta use nested function\n",
    "    manager = mp.Manager()\n",
    "    shared_ct_dict = manager.dict(OmegaConf.to_container(cfg))\n",
    "    def worker_init_fn(worker_id):\n",
    "        ds = torch.utils.data.get_worker_info().dataset\n",
    "        ds.max_shift_x = shared_ct_dict['training']['max_shift_x']\n",
    "        ds.max_shift_y = shared_ct_dict['training']['max_shift_y']\n",
    "\n",
    "    # setup models\n",
    "    patch_size = cfg.model.get('patch_size', cfg.model.get('patch_h', 16))\n",
    "    dim = cfg.model.get('dim', cfg.model.get('embed_dim', 256))    \n",
    "    if cfg.model.get('encoder', 'vit') == 'swin':\n",
    "        from midi_rae.swin import SwinEncoder\n",
    "        model = SwinEncoder(img_height=cfg.data.image_size, img_width=cfg.data.image_size,\n",
    "                            patch_h=cfg.model.patch_h, patch_w=cfg.model.patch_w,\n",
    "                            embed_dim=cfg.model.embed_dim, depths=cfg.model.depths,\n",
    "                            num_heads=cfg.model.num_heads, window_size=cfg.model.window_size,\n",
    "                            mlp_ratio=cfg.model.mlp_ratio, drop_path_rate=cfg.model.drop_path_rate).to(device)\n",
    "        dims = tuple(cfg.model.embed_dim * 2**i for i in range(len(cfg.model.depths)-1, -1, -1))\n",
    "        mae_decoder = SwinMAEDecoder(patch_size=patch_size, dims=dims).to(device)\n",
    "    else:\n",
    "        model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), cfg.model.patch_size, \n",
    "              cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n",
    "        mae_decoder = LightweightMAEDecoder(patch_size=patch_size, dim=dim).to(device)\n",
    "\n",
    "    print(\"model       =\",model.__class__.__name__)\n",
    "    print(\"mae_decoder =\",mae_decoder.__class__.__name__)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(chain(model.parameters(), mae_decoder.parameters()), lr=cfg.training.lr)\n",
    "    epoch_start = 1\n",
    "    if (cfg.get('checkpoint', False)): # use \"+checkpoint=<path>\" from CLI\n",
    "        ckpt_path =  cfg.get('checkpoint',None)\n",
    "        model, ckpt = load_checkpoint(model, ckpt_path, return_all=True)\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        epoch_start = ckpt['epoch'] + 1 # next epoch after the one completed in thecheckpoint\n",
    "        try:\n",
    "            mae_decoder = load_checkpoint(mae_decoder, ckpt_path.replace('enc_','maedec_'), return_all=False)\n",
    "        except: pass\n",
    "\n",
    "    if False: # skip compilation\n",
    "        #model = torch.compile(model, dynamic=True)\n",
    "        os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "        os.makedirs(os.environ[\"TORCHINDUCTOR_CACHE_DIR\"], exist_ok=True)\n",
    "        torch._inductor.config.fx_graph_cache = True\n",
    "        torch._inductor.config.compile_threads = 8\n",
    "        torch._dynamo.config.cache_size_limit = 16\n",
    "        #model = torch.compile(model, mode=\"default\", fullgraph=False)\n",
    "        for i, stage in enumerate(model.stages):\n",
    "            model.stages[i] = torch.compile(stage, mode=\"default\")\n",
    "    \n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=cfg.training.lr, steps_per_epoch=1, epochs=cfg.training.epochs, div_factor=5, \n",
    "                        **({'last_epoch': epoch_start-1} if epoch_start > 1 else {}))\n",
    "    #scaler = torch.amp.GradScaler(device)\n",
    "    \n",
    "    if not(cfg.get('no_wandb', False)): \n",
    "        wandb.init(project=cfg.wandb.project, config=dict(cfg), settings=wandb.Settings(start_method=\"fork\", _disable_stats=True))\n",
    "        wandb.define_metric(\"epoch\")\n",
    "        wandb.define_metric(\"*\", step_metric=\"epoch\")\n",
    "\n",
    "    # Training loop\n",
    "    global_step = (epoch_start - 1) * len(train_dl)\n",
    "    #viz_every = 10\n",
    "    viz_every = 1 # make it fail early for debugging\n",
    "    for epoch in range(epoch_start, cfg.training.epochs+1):\n",
    "        if wandb.run is not None: wandb.log({\"epoch\": epoch})\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        if False and epoch > 1: # curriculum learning, easily turned off by setting this to False. DL's re-defined each epoch to init workers\n",
    "            shared_ct_dict['training'] = curr_learn(shared_ct_dict, epoch)\n",
    "            train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)\n",
    "            val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)\n",
    "        for batch in tqdm(train_dl, desc=f\"Epoch {epoch}/{cfg.training.epochs}\"):\n",
    "            global_step += 1\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if True: # with torch.autocast('cuda'):\n",
    "                #loss_dict, z1, z2, non_emptys, pos2, mae_mask2, num_tokens, recon_patches = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "                loss_dict, zs, enc_outs, recon_patches = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "            #scaler.scale(loss_dict['loss']).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()\n",
    "            loss_dict['loss'].backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss_dict['loss'].item()\n",
    "            \n",
    "        # At end of Epoch: validation, viz, etc\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                #val_loss_dict, z1, z2, non_emptys, pos2, mae_mask2, num_tokens, recon_patches = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "                val_loss_dict, zs, enc_outs, recon_patches = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "                val_loss += val_loss_dict['loss'].item()\n",
    "\n",
    "            train_loss /= len(train_dl)\n",
    "            val_loss /= len(val_dl)\n",
    "            print(f\"Epoch {epoch}/{cfg.training.epochs}: train_loss={train_loss:.3f} val_loss={val_loss:.3f}\")\n",
    "            \n",
    "            if wandb.run is not None:\n",
    "                wandb.log({ \n",
    "                    \"train_loss\":train_loss, \"train_sim\":loss_dict['sim'], \"train_sigreg\":loss_dict['sigreg'], \"train_anchor\":loss_dict['anchor'], \"train_mae\":loss_dict['mae'],  \n",
    "                    \"val_loss\":val_loss, \"val_sim\":val_loss_dict['sim'], \"val_sigreg\":val_loss_dict['sigreg'], \"val_anchor\":val_loss_dict['anchor'], \"val_mae\": val_loss_dict['mae'], \n",
    "                    \"max_shift_x\":shared_ct_dict['training']['max_shift_x'], \"max_shift_y\":shared_ct_dict['training']['max_shift_y'],\n",
    "                    \"lr\": optimizer.param_groups[0]['lr'], \"epoch\": epoch})\n",
    "\n",
    "                if epoch % viz_every == 0:\n",
    "                    make_emb_viz(enc_outs, epoch=epoch, model=model, batch=batch)\n",
    "                if mae_decoder is not None and (epoch % (viz_every//5) == 0):\n",
    "                    viz_mae_recon(recon_patches, batch['img2'], enc_out=enc_outs.finest, epoch=epoch)\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, cfg, tag=\"enc_\")\n",
    "        save_checkpoint(mae_decoder, optimizer, epoch, val_loss, cfg, tag=\"maedec_\")\n",
    "\n",
    "        scheduler.step()# val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd0128",
   "metadata": {},
   "source": [
    "## CLI Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "if __name__ == \"__main__\" and \"ipykernel\" not in __import__(\"sys\").modules:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
