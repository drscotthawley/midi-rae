{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b916516",
   "metadata": {},
   "source": [
    "# train_enc\n",
    "\n",
    "> Encoder training script for midi_rae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905c98",
   "metadata": {
    "time_run": "2026-02-15T18:41:53.007367+00:00"
   },
   "outputs": [],
   "source": [
    "#| default_exp train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d15b2",
   "metadata": {
    "time_run": "2026-02-15T18:41:53.011886+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edfbef",
   "metadata": {
    "time_run": "2026-02-15T18:48:53.641276+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "from midi_rae.vit import ViTEncoder, LightweightMAEDecoder\n",
    "from midi_rae.data import PRPairDataset\n",
    "from midi_rae.losses import calc_enc_loss, calc_mae_loss\n",
    "from midi_rae.utils import save_checkpoint\n",
    "from midi_rae.viz import make_emb_viz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b6854",
   "metadata": {},
   "source": [
    "## Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4f354",
   "metadata": {
    "time_run": "2026-02-15T18:41:56.061106+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def curr_learn(shared_ct_dict, epoch, interval=100, verbose=False): \n",
    "    \"UNUSED/UNNECESSARY: curriculum learning: increase difficulty with epoch\"\n",
    "    if epoch < interval: return shared_ct_dict['training']\n",
    "    training = shared_ct_dict['training']\n",
    "    training['max_shift_x'] = min(12, 6 + epoch // interval)\n",
    "    training['max_shift_y'] = min(12, 6 + epoch // interval)\n",
    "    if verbose: \n",
    "        print(f\"curr_learn: max_shift_x = {training['max_shift_x']}, max_shift_y = {training['max_shift_y']}\")\n",
    "    return training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5526a",
   "metadata": {},
   "source": [
    "## Compute Loss On Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6162a",
   "metadata": {
    "time_run": "2026-02-15T18:41:56.068063+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_batch_loss(batch, encoder, cfg, global_step, mae_decoder=None): \n",
    "    \"Compute loss and return other exal auxiliary variables (for train or val)\"\n",
    "    device = next(encoder.parameters()).device\n",
    "    img1, img2, deltas = batch['img1'].to(device), batch['img2'].to(device), batch['deltas'].to(device)\n",
    "    z1, pmask1, pos1, mae_mask1 = encoder(img1, return_cls_only=False) \n",
    "    z2, pmask2, pos2, mae_mask2 = encoder(img2, return_cls_only=False) \n",
    "    loss_dict = {} \n",
    "    if mae_decoder is not None:\n",
    "        recon_patches = mae_decoder(z2, pos2, mae_mask2) # just pick z2 and ignore z1 \n",
    "        loss_dict['mae'] = calc_mae_loss(recon_patches, img2, pos2, mae_mask2)\n",
    "\n",
    "    z1 = z1.reshape(-1, z1.shape[-1])\n",
    "    z2 = z2.reshape(-1, z2.shape[-1])\n",
    "    num_tokens =  z1.shape[0] // len(deltas)  # or just 65\n",
    "    deltas = deltas.repeat_interleave(num_tokens, dim=0)\n",
    "    loss_dict = loss_dict | calc_enc_loss(z1, z2, global_step, deltas=deltas, lambd=cfg.training.lambd, pmasks=(pmask1,pmask2))\n",
    "    if 'mae' in loss_dict.keys(): loss_dict['loss'] += cfg.training.get('mae_lambda', 1.0) * loss_dict['mae'] \n",
    "\n",
    "    return loss_dict, z1, z2, pmask1, pmask2, num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e508bd68",
   "metadata": {
    "solveit_ai": true,
    "time_run": "2026-02-15T18:53:47.593990+00:00"
   },
   "source": [
    "getting an error: \n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/shawley/github/midi-rae/midi_rae/train_enc.py\", line 107, in train\n",
    "    loss_dict, z1, z2, pmask1, pmask2, num_tokens = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "  File \"/home/shawley/github/midi-rae/midi_rae/train_enc.py\", line 56, in compute_batch_loss\n",
    "    loss_dict = loss_dict | calc_enc_loss(z1, z2, global_step, deltas=deltas, lambd=cfg.training.lambd, pmasks=(pmask1,pmask2))\n",
    "  File \"/home/shawley/github/midi-rae/midi_rae/losses.py\", line 60, in calc_enc_loss\n",
    "    loss_dict = LeJEPA(z1[valid], z2[valid], global_step, deltas=deltas[valid], lambd=lambd)\n",
    "IndexError: The shape of the mask [49920] at index 0 does not match the shape of the indexed tensor [6912, 256] at index 0\n",
    "\n",
    "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
    "```\n",
    "you may need to scrape `/app/data/midi-rae/midi_rae/losses.py` and `/app/data/midi-rae/midi_rae/vit.py` But I think the issue is the handling of the shapes right here in `compute_batch_loss`. help?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Looking at the error, the issue is a shape mismatch between the `valid` mask (49920 elements) and `z1`/`z2` (6912 rows).\n",
    "\n",
    "In `compute_batch_loss`, you reshape `z1` and `z2`:\n",
    "```python\n",
    "z1 = z1.reshape(-1, z1.shape[-1])  # becomes [6912, 256]\n",
    "z2 = z2.reshape(-1, z2.shape[-1])\n",
    "```\n",
    "\n",
    "But `pmask1` and `pmask2` are passed directly to `calc_enc_loss` **without** being reshaped to match. That's likely where the `valid` mask is constructed.\n",
    "\n",
    "Can you check: what are the shapes of `pmask1` and `pmask2` before they get passed to `calc_enc_loss`? You could add a quick print:\n",
    "\n",
    "```python\n",
    "print(f\"z1: {z1.shape}, pmask1: {pmask1.shape}, pmask2: {pmask2.shape}\")\n",
    "```\n",
    "\n",
    "That'll confirm whether the pmasks need the same flattening treatment as z1/z2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a84b66",
   "metadata": {
    "solveit_ai": true,
    "time_run": "2026-02-15T18:58:57.612125+00:00"
   },
   "source": [
    "Hmm that seems unlikely. That part of the code used to work fine. What's broken is now that we've added this decoder stuff and the MAE mask stuff. Check the VIT code.  Note that Pmask is still the full set of non-empty patches. We really tried to ensure backwards compatibility.\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "<summary>view(path='/app/data/midi-rae/midi_rae/vit.py')</summary>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_012XfSQwaas4LURPjTSVDeR7\",\n",
    "  \"call\": {\n",
    "    \"function\": \"view\",\n",
    "    \"arguments\": {\n",
    "      \"path\": \"/app/data/midi-rae/midi_rae/vit.py\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'\\\"\\\"\\\"Components & defs for ViT-based Encoder & Decoder\\\"\\\"\\\"\\\\n\\\\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_vit.ipynb.\\\\n\\\\n# %% auto #0\\\\n__all__ = [\\\\'RoPE2D\\\\', \\\\'Attention\\\\', \\\\'TransformerBlock\\\\', \\\\'PatchEmbedding\\\\', \\\\'apply_mae_mask\\\\', \\\\'ViTEncoder\\\\', \\\\'Unpatchify\\\\',\\\\n           \\\\'ViTDecoder\\\\', \\\\'LightweightMAEDecoder\\\\']\\\\n\\\\n# %% ../nbs/02_vit.ipynb #b96051a7\\\\nimport torch\\\\nimport torch.nn as nn\\\\nimport torch.nn.functional as F \\\\n\\\\n# %% ../nbs/02_vit.ipynb #624570b6\\\\nclass RoPE2D(nn.Module):\\\\n    def __init__(self, head_dim):\\\\n        super().__init__()\\\\n        i = torch.arange(0, head_d<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "I see the issue. Look at what `ViTEncoder.forward` returns:\n",
    "\n",
    "```python\n",
    "return (x[:, 0] if return_cls_only else x), pmask, pos, mae_mask\n",
    "```\n",
    "\n",
    "It's returning the **original full** `pmask` and `pos`, not the masked versions (`pmask_visible`, `pos_visible`). But `x` has been masked down to only visible tokens.\n",
    "\n",
    "So when `return_cls_only=False`, you get:\n",
    "- `x` with shape `(B, N_visible, dim)` â€” only the visible (unmasked) tokens\n",
    "- `pmask` with shape `(B, 65)` â€” the **full** mask before MAE masking\n",
    "- `pos` with shape `(65, 2)` â€” the **full** positions\n",
    "\n",
    "The shapes don't match. \n",
    "\n",
    "The fix should be in `ViTEncoder.forward` â€” it should return `pmask_visible` and `pos_visible` instead of `pmask` and `pos`. Does that match your intent for how the encoder output should be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a6e8e",
   "metadata": {
    "solveit_ai": true,
    "time_run": "2026-02-15T19:03:29.906331+00:00"
   },
   "source": [
    "Well, if I do that, then the code elsewhere fails. \n",
    "This is probably because if we do that, then For two different subsequent calls to the encoder, we get two different random MAE masks, don't we?\n",
    "\n",
    "```\n",
    "    loss_dict, z1, z2, pmask1, pmask2, num_tokens = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "  File \"/home/shawley/github/midi-rae/midi_rae/train_enc.py\", line 56, in compute_batch_loss\n",
    "    loss_dict = loss_dict | calc_enc_loss(z1, z2, global_step, deltas=deltas, lambd=cfg.training.lambd, pmasks=(pmask1,pmask2))\n",
    "  File \"/home/shawley/github/midi-rae/midi_rae/losses.py\", line 58, in calc_enc_loss\n",
    "    pmask = pmask1 & pmask2  # both non-empty\n",
    "RuntimeError: The size of tensor a (5) must match the size of tensor b (7) at non-singleton dimension 1\n",
    "```\n",
    "Perhaps we should make it to where you could pass in the MAE mask from the first call into the second call, in which case it won't generate a new random mask, but it'll just use the one from the previous call.?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Yes, exactly right! With random MAE masking, `z1` and `z2` will have different subsets of visible tokens, so you can't directly compare them in the contrastive loss.\n",
    "\n",
    "Your suggestion makes sense: add an optional `mae_mask` parameter to the encoder's forward method. If provided, use it instead of generating a new random one.\n",
    "\n",
    "So the flow in `compute_batch_loss` would become:\n",
    "\n",
    "1. First call: `z1, pmask1, pos1, mae_mask1 = encoder(img1, ...)` â€” generates random mask\n",
    "2. Second call: `z2, pmask2, pos2, mae_mask2 = encoder(img2, mae_mask=mae_mask1, ...)` â€” reuses same mask\n",
    "\n",
    "Does that match what you're thinking? If so, you'd just need to modify `ViTEncoder.forward` to accept an optional `mae_mask` argument and skip the random generation if one is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a91da1",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713ab74",
   "metadata": {
    "time_run": "2026-02-15T18:46:31.210208+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"config\")\n",
    "def train(cfg: DictConfig):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(\"device = \",device)\n",
    "\n",
    "    train_ds = PRPairDataset(split='train', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    val_ds   = PRPairDataset(split='val',  max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # next bit is to enable curriculum learning, dataloaders re-defined per epoch, gotta use nested function\n",
    "    manager = mp.Manager()\n",
    "    shared_ct_dict = manager.dict(OmegaConf.to_container(cfg))\n",
    "    def worker_init_fn(worker_id):\n",
    "        ds = torch.utils.data.get_worker_info().dataset\n",
    "        ds.max_shift_x = shared_ct_dict['training']['max_shift_x']\n",
    "        ds.max_shift_y = shared_ct_dict['training']['max_shift_y']\n",
    "\n",
    "    model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), cfg.model.patch_size, \n",
    "              cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n",
    "    model = torch.compile(model)\n",
    "    mae_decoder = LightweightMAEDecoder(patch_size=cfg.model.patch_size, dim=cfg.model.dim).to(device)\n",
    "\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.training.lr)\n",
    "    optimizer = torch.optim.AdamW(chain(model.parameters(), mae_decoder.parameters()), lr=cfg.training.lr)\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=cfg.training.lr, steps_per_epoch=1, epochs=cfg.training.epochs)\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "    if not(cfg.get('no_wandb', False)): wandb.init(project=cfg.wandb.project, config=dict(cfg), settings=wandb.Settings(start_method=\"fork\", _disable_stats=True))\n",
    "\n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    viz_every = 10\n",
    "    for epoch in range(1, cfg.training.epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        if False and epoch > 1: # curriculum learning, easily turned off by setting this to False. DL's re-defined each epoch to init workers\n",
    "            shared_ct_dict['training'] = curr_learn(shared_ct_dict, epoch)\n",
    "            train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)\n",
    "            val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)\n",
    "        for batch in tqdm(train_dl, desc=f\"Epoch {epoch}/{cfg.training.epochs}\"):\n",
    "            global_step += 1\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast('cuda'):\n",
    "                loss_dict, z1, z2, pmask1, pmask2, num_tokens = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "            scaler.scale(loss_dict['loss']).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss_dict['loss'].item()\n",
    "            \n",
    "        # At end of Epoch: validation, viz, etc\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                val_loss_dict, z1, z2, pmask1, pmask2, num_tokens = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)\n",
    "                val_loss += val_loss_dict['loss'].item()\n",
    "\n",
    "        train_loss /= len(train_dl)\n",
    "        val_loss /= len(val_dl)\n",
    "        print(f\"Epoch {epoch}/{cfg.training.epochs}: train_loss={train_loss:.3f} val_loss={val_loss:.3f}\")\n",
    "        \n",
    "        if wandb.run is not None: wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \n",
    "           \"train_sim\": loss_dict['sim'], \"train_sigreg\": loss_dict['sigreg'], \"train_anchor\":loss_dict['anchor'], \"train_mae\":loss_dict['mae'],\n",
    "           \"val_sim\": val_loss_dict['sim'], \"val_sigreg\": val_loss_dict['sigreg'], \"val_anchor\": val_loss_dict['anchor'], \"val_mae\": val_loss_dict['mae'],\n",
    "           \"max_shift_x\":shared_ct_dict['training']['max_shift_x'], \"max_shift_y\":shared_ct_dict['training']['max_shift_y'], \n",
    "           \"lr\": optimizer.param_groups[0]['lr'], \"epoch\": epoch}, step=epoch)\n",
    "\n",
    "        if epoch % viz_every == 0: \n",
    "            zs_stacked = torch.cat((z1, z2), dim=0).reshape(-1, z1.shape[-1])\n",
    "            make_emb_viz(zs_stacked, num_tokens, epoch, model=model, pmasks=(pmask1,pmask2), file_idx=batch['file_idx'])\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss, cfg, tag=\"enc_\")\n",
    "        scheduler.step()# val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd0128",
   "metadata": {},
   "source": [
    "## CLI Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55b9c3",
   "metadata": {
    "time_run": "2026-02-15T18:41:56.082740+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "if __name__ == \"__main__\" and \"ipykernel\" not in __import__(\"sys\").modules:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977fc5c",
   "metadata": {
    "time_run": "2026-02-15T18:41:56.088996+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
