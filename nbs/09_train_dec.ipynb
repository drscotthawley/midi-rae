{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f09eb3",
   "metadata": {},
   "source": [
    "# Train Decoder\n",
    "\n",
    "> Training script for the ViT decoder with reconstruction losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42d221",
   "metadata": {},
   "source": [
    "## Context for LLM assistance\n",
    "\n",
    "This is part of a Representation Autoencoder (RAE) project for MIDI piano roll images.\n",
    "\n",
    "**What we have:**\n",
    "- `ViTEncoder` in `vit.py`: produces (B, 65, 768) — 64 patch tokens + 1 CLS token from 128×128 images\n",
    "- `ViTDecoder` in `vit.py`: takes (B, 65, 768), strips CLS, unpatchifies back to (B, 1, 128, 128)\n",
    "- `PatchGANDiscriminator` in `losses.py`: for adversarial loss\n",
    "- Pre-encoded data optionally available via `07_preencode.ipynb`\n",
    "\n",
    "**What we're doing:**\n",
    "- Train decoder to reconstruct images from frozen encoder embeddings\n",
    "- Losses: L1 + LPIPS + GAN (adversarial) with adaptive weighting\n",
    "- Encoder is FROZEN — only decoder and discriminator train\n",
    "\n",
    "**What's next:**\n",
    "- After decoder training: `09_dit.ipynb` (diffusion transformer architecture)\n",
    "- Then: `10_train_gen.ipynb` (train DiT for generation in latent space)\n",
    "\n",
    "**Image specs:** 128×128 grayscale (1 channel), patch_size=16, giving 8×8=64 patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2083e34",
   "metadata": {
    "time_run": "2026-02-14T21:17:50.350921+00:00"
   },
   "outputs": [],
   "source": [
    "#| default_exp train_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644c3ec",
   "metadata": {
    "time_run": "2026-02-14T21:17:50.356103+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59b81f",
   "metadata": {
    "time_run": "2026-02-14T21:59:09.967026+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "from tqdm.auto import tqdm\n",
    "import lpips\n",
    "from collections import namedtuple\n",
    "\n",
    "from midi_rae.vit import ViTEncoder, ViTDecoder\n",
    "from midi_rae.losses import PatchGANDiscriminator\n",
    "from midi_rae.data import PRPairDataset  # note, we'll only use img2 and ignore img1\n",
    "from midi_rae.utils import save_checkpoint, load_checkpoint\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9456",
   "metadata": {
    "time_run": "2026-02-14T21:17:56.694706+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class PreEncodedDataset(Dataset):\n",
    "    \"\"\"Load pre-encoded embeddings + images from .pt files\"\"\"\n",
    "    def __init__(self, encoded_dir):\n",
    "        self.files = sorted([f for f in os.listdir(encoded_dir) if f.endswith('.pt')])\n",
    "        self.encoded_dir = encoded_dir\n",
    "        # Load all chunks into memory (adjust if too large)\n",
    "        self.embeddings, self.images = [], []\n",
    "        for f in self.files:\n",
    "            data = torch.load(os.path.join(encoded_dir, f))\n",
    "            self.embeddings.append(data['embeddings'])\n",
    "            self.images.append(data['images'])\n",
    "        self.embeddings = torch.cat(self.embeddings, dim=0)\n",
    "        self.images = torch.cat(self.images, dim=0)\n",
    "    \n",
    "    def __len__(self): return len(self.embeddings)\n",
    "    def __getitem__(self, idx): return self.embeddings[idx], self.images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f0438",
   "metadata": {
    "time_run": "2026-02-14T21:17:56.700152+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_embeddings_batch(batch, encoder=None, preencoded=False, device='cuda'):\n",
    "    \"\"\"Get embeddings + images from batch, either pre-encoded or computed on-the-fly\"\"\"\n",
    "    if preencoded:\n",
    "        z, img = batch\n",
    "        return z.to(device), img.to(device)\n",
    "    else:\n",
    "        img = batch['img2'].to(device)  # adjust key based on your dataset\n",
    "        with torch.no_grad():\n",
    "            z = encoder(img, return_cls_only=False)\n",
    "        return z, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf8a0f",
   "metadata": {
    "time_run": "2026-02-14T21:22:33.806050+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_dataloaders(cfg, preencoded=False):\n",
    "        # --- Data ---\n",
    "    if preencoded:\n",
    "        train_ds = PreEncodedDataset(cfg.preencode.output_dir + '/train')\n",
    "        val_ds = PreEncodedDataset(cfg.preencode.output_dir + '/val')\n",
    "    else:\n",
    "        train_ds = PRPairDataset(split='train', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "        val_ds   = PRPairDataset(split='val',  max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=cfg.training.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    return train_dl, val_dl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221cde1",
   "metadata": {
    "time_run": "2026-02-14T21:57:26.946574+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_models(cfg, device, preencoded): \n",
    "    encoder = None\n",
    "    if not preencoded:\n",
    "        encoder = ViTEncoder(cfg.data.in_channels, cfg.data.image_size, cfg.model.patch_size,\n",
    "                             cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n",
    "        encoder = load_checkpoint(encoder, cfg.get('encoder_ckpt', 'checkpoints/enc_best.pt'))\n",
    "        encoder.eval()  # frozen\n",
    "        for p in encoder.parameters(): p.requires_grad = False\n",
    "    \n",
    "    decoder = ViTDecoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size),\n",
    "                         cfg.model.patch_size, cfg.model.dim, \n",
    "                         cfg.model.get('dec_depth', 4), cfg.model.get('dec_heads', 8)).to(device)\n",
    "    decoder = torch.compile(decoder)\n",
    "    \n",
    "    discriminator = PatchGANDiscriminator(in_ch=cfg.data.in_channels).to(device)\n",
    "    discriminator = torch.compile(discriminator)\n",
    "    return encoder, decoder, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd1ef1",
   "metadata": {
    "time_run": "2026-02-14T21:59:40.409678+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_tstate(cfg, device, decoder, discriminator):\n",
    "    \"Training_state: Losses, Optimizers, Schedulers, AMP Scalers\"\n",
    "    l1_loss = nn.L1Loss()\n",
    "    lpips_loss = lpips.LPIPS(net='vgg').to(device)\n",
    "    opt_dec = torch.optim.AdamW(decoder.parameters(), lr=cfg.training.lr)\n",
    "    opt_disc = torch.optim.AdamW(discriminator.parameters(), lr=cfg.training.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(opt_dec, max_lr=cfg.training.lr, steps_per_epoch=1, epochs=cfg.training.epochs)\n",
    "    schedulerD = torch.optim.lr_scheduler.OneCycleLR(opt_disc, max_lr=cfg.training.lr, steps_per_epoch=1, epochs=cfg.training.epochs)   \n",
    "    scaler_dec, scaler_disc = torch.amp.GradScaler(), torch.amp.GradScaler()\n",
    "    return namedtuple('TrainState', ['opt_disc', 'opt_dec', 'scaler_disc', 'scaler_dec', 'l1_loss', 'lpips_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a8d39",
   "metadata": {
    "time_run": "2026-02-14T21:52:06.434526+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_step(z, img_real, decoder, discriminator, \n",
    "            tstate,  # named tuple containing optimizers, loss fns, scalers \n",
    "            ): \n",
    "    \"training step for decoder (and discriminator)\"\n",
    "    # --- Discriminator step ---\n",
    "    tstate.opt_disc.zero_grad()\n",
    "    with torch.autocast('cuda'):\n",
    "        img_recon = decoder(z)\n",
    "        d_real = discriminator(img_real)\n",
    "        d_fake = discriminator(img_recon.detach())\n",
    "        loss_disc = (torch.relu(1 - d_real).mean() + torch.relu(1 + d_fake).mean()) / 2\n",
    "    tstate.scaler_disc.scale(loss_disc).backward()\n",
    "    tstate.scaler_disc.step(tstate.opt_disc)\n",
    "    tstate.scaler_disc.update()\n",
    "    \n",
    "    # --- Decoder step ---\n",
    "    tstate.opt_dec.zero_grad()\n",
    "    with torch.autocast('cuda'):\n",
    "        # img_recon = decoder(z)  # Don't need to recompute this.\n",
    "        loss_l1 = tstate.l1_loss(img_recon, img_real)\n",
    "        loss_lpips = tstate.lpips_loss(img_recon.repeat(1,3,1,1), img_real.repeat(1,3,1,1)).mean()  # LPIPS wants 3ch\n",
    "        loss_gan = -discriminator(img_recon).mean()  # generator wants discriminator to say \"real\"\n",
    "        loss_dec = loss_l1 + 0.1 * loss_lpips + 0.01 * loss_gan  # TODO: adaptive weighting as in RAE paper (??)\n",
    "    tstate.scaler_dec.scale(loss_dec).backward()\n",
    "    tstate.scaler_dec.step(tstate.opt_dec)\n",
    "    tstate.scaler_dec.update()\n",
    "    \n",
    "    keys = ['disc', 'l1', 'lpips', 'gan', 'dec']\n",
    "    vals = [loss_disc, loss_l1, loss_lpips, loss_gan, loss_dec]\n",
    "    losses = { k:v.item() for k,v in zip(keys, vals) }\n",
    "    return losses, img_recon.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198855af",
   "metadata": {
    "time_run": "2026-02-14T21:59:52.759331+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@hydra.main(version_base=None, config_path='../configs', config_name='config')\n",
    "def train(cfg: DictConfig):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    preencoded = cfg.get('preencoded', False)\n",
    "    \n",
    "    train_dl, val_dl = setup_dataloaders(cfg, preencoded)\n",
    "    encoder, decoder, discriminator = setup_models(cfg, device, preencoded) \n",
    "    tstate = setup_tstate(cfg, device, decoder, discriminator)\n",
    "    wandb.init(project='dec-'+cfg.wandb.project, config=dict(cfg))\n",
    "    \n",
    "    for epoch in range(1, cfg.training.epochs + 1):\n",
    "        decoder.train()\n",
    "        discriminator.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dl, desc=f'Epoch {epoch}/{cfg.training.epochs}'):\n",
    "            z, img_real = get_embeddings_batch(batch, encoder, preencoded, device)\n",
    "            losses, img_recon = train_step(z, img_real, decoder, discriminator, tstate)\n",
    "            train_loss += losses['dec'].item()\n",
    "        \n",
    "        train_loss /= len(train_dl)\n",
    "        print(f'Epoch {epoch}: train_loss={train_loss:.4f}')\n",
    "        wandb.log({'train_loss': train_loss, 'loss_l1': losses['l1'], 'loss_lpips': losses['lpips'], 'loss_gan': losses['gan'],\n",
    "                   'loss_disc': losses['disc'], 'epoch': epoch})\n",
    "        \n",
    "        # TODO: validation, checkpointing, visualization e.g. reconstruction comparison\n",
    "        scheduler.step()\n",
    "        schedulerD.step()\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d6973",
   "metadata": {
    "time_run": "2026-02-14T21:17:56.715360+00:00"
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "if __name__ == \"__main__\" and \"ipykernel\" not in __import__(\"sys\").modules:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc493f",
   "metadata": {
    "time_run": "2026-02-14T21:17:56.723790+00:00"
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
