{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f09eb3",
   "metadata": {},
   "source": [
    "# Train Decoder\n",
    "\n",
    "> Training script for the ViT decoder with reconstruction losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2083e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import wandb\n",
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "from tqdm.auto import tqdm\n",
    "import lpips\n",
    "from collections import namedtuple\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from midi_rae.vit import ViTEncoder, ViTDecoder\n",
    "from midi_rae.losses import PatchGANDiscriminator\n",
    "from midi_rae.data import PRPairDataset  # note, we'll only use img2 and ignore img1\n",
    "from midi_rae.utils import save_checkpoint, load_checkpoint\n",
    "from midi_rae.viz import viz_mae_recon\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PreEncodedDataset(Dataset):\n",
    "    \"\"\"Load pre-encoded embeddings + images from .pt files\"\"\"\n",
    "    def __init__(self, encoded_dir):\n",
    "        self.files = sorted([f for f in os.listdir(encoded_dir) if f.endswith('.pt')])\n",
    "        self.encoded_dir = encoded_dir\n",
    "        # Load all chunks into memory (adjust if too large)\n",
    "        self.embeddings, self.images = [], []\n",
    "        for f in self.files:\n",
    "            data = torch.load(os.path.join(encoded_dir, f))\n",
    "            self.embeddings.append(data['embeddings'])\n",
    "            self.images.append(data['images'])\n",
    "        self.embeddings = torch.cat(self.embeddings, dim=0)\n",
    "        self.images = torch.cat(self.images, dim=0)\n",
    "    \n",
    "    def __len__(self): return len(self.embeddings)\n",
    "    def __getitem__(self, idx): return self.embeddings[idx], self.images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_dataloaders(cfg, preencoded=False):\n",
    "    if preencoded:\n",
    "        train_ds = PreEncodedDataset(cfg.preencode.output_dir + '/train')\n",
    "        val_ds = PreEncodedDataset(cfg.preencode.output_dir + '/val')\n",
    "    else:\n",
    "        train_ds = PRPairDataset(image_dataset_dir=cfg.data.path, split='train', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "        val_ds   = PRPairDataset(image_dataset_dir=cfg.data.path, split='val',  max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) \n",
    "    \n",
    "    batch_size = cfg.training.dec_batch_size\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    return train_dl, val_dl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_models(cfg, device, preencoded): \n",
    "    encoder = None\n",
    "    if not preencoded:\n",
    "        encoder = ViTEncoder(cfg.data.in_channels, cfg.data.image_size, cfg.model.patch_size,\n",
    "                             cfg.model.dim, cfg.model.depth, cfg.model.heads, mask_ratio=0).to(device)\n",
    "        encoder = load_checkpoint(encoder, cfg.get('encoder_ckpt', 'checkpoints/enc_best.pt'))\n",
    "        if cfg.training.get('enc_ft_lr', 0) <=0: \n",
    "            print(\"Freezing Encoder\")\n",
    "            encoder.eval()  # frozen\n",
    "            for p in encoder.parameters(): p.requires_grad = False\n",
    "        else: \n",
    "            print(\"Encoder will train\") \n",
    "            encoder.train()\n",
    "    \n",
    "    decoder = ViTDecoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size),\n",
    "                         cfg.model.patch_size, cfg.model.dim, \n",
    "                         cfg.model.get('dec_depth', 4), cfg.model.get('dec_heads', 8)).to(device)\n",
    "    #decoder = torch.compile(decoder)\n",
    "    \n",
    "    discriminator = PatchGANDiscriminator(in_ch=cfg.data.in_channels).to(device)\n",
    "    #discriminator = torch.compile(discriminator) # can cause issues elsewhere; leave off for now\n",
    "    return encoder, decoder, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_tstate(cfg, device, decoder, discriminator, encoder=None):\n",
    "    \"Training_state: Losses, Optimizers, Schedulers, AMP Scalers\"\n",
    "    l1_loss = nn.L1Loss()\n",
    "    lpips_loss = lpips.LPIPS(net='vgg').to(device)\n",
    "    opt_enc = None #if encoder is None else torch.optim.AdamW(encoder.parameters(), lr=cfg.training.enc_ft_lr)\n",
    "    opt_dec = torch.optim.AdamW(decoder.parameters(), lr=cfg.training.dec_lr)\n",
    "    opt_disc = torch.optim.AdamW(discriminator.parameters(), lr=cfg.training.dec_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(opt_dec, max_lr=cfg.training.dec_lr, steps_per_epoch=1, epochs=cfg.training.dec_epochs)\n",
    "    schedulerD = torch.optim.lr_scheduler.OneCycleLR(opt_disc, max_lr=cfg.training.dec_lr, steps_per_epoch=1, epochs=max(1, cfg.training.dec_epochs-cfg.training.gan_warmup))   \n",
    "    scheduler_enc = None if opt_enc is None else torch.optim.lr_scheduler.OneCycleLR(opt_enc, max_lr=cfg.training.enc_ft_lr, steps_per_epoch=1, epochs=max(1, cfg.training.dec_epochs))   \n",
    "    scaler_dec, scaler_disc = torch.amp.GradScaler(), torch.amp.GradScaler()\n",
    "    TrainState = namedtuple('TrainState', ['opt_enc', 'opt_disc', 'opt_dec', 'scaler_disc', 'scaler_dec', 'l1_loss', 'lpips_loss','scheduler','schedulerD','scheduler_enc'])\n",
    "    return TrainState(opt_enc, opt_disc, opt_dec, scaler_disc, scaler_dec, l1_loss, lpips_loss, scheduler, schedulerD, scheduler_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_embeddings_batch(batch, encoder=None, preencoded=False, device='cuda'):\n",
    "    \"\"\"Get embeddings + images from batch, either pre-encoded or computed on-the-fly\"\"\"\n",
    "    if preencoded:\n",
    "        z, img = batch\n",
    "        return z.to(device), img.to(device)\n",
    "    else:\n",
    "        img1, img2, deltas = batch['img1'].to(device), batch['img2'].to(device), batch['deltas'].to(device)\n",
    "        with torch.no_grad() if not encoder.training else nullcontext():\n",
    "            z1, non_empty1, pos1, mae_mask1  = encoder(img1, return_cls_only=False)\n",
    "            z2, non_empty2, pos2, mae_mask2 = encoder(img2, return_cls_only=False)\n",
    "            img1 = (img1 > 0.2).float()  # binarize\n",
    "            img2 = (img2 > 0.2).float()\n",
    "        return {'z1':z1, 'z2':z2, 'non_empty1':non_empty1, 'non_empty2':non_empty2, 'pos1':pos1, 'pos2':pos2, 'img1':img1, 'img2':img2, 'deltas':deltas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_step(epoch, z, img_real, decoder, discriminator, \n",
    "            tstate,  # named tuple containing optimizers, loss fns, scalers \n",
    "            cfg,    # config\n",
    "            ): \n",
    "    \"training step for decoder (and discriminator)\"\n",
    "    decoder.train(), discriminator.train()\n",
    "    loss_disc, loss_gan = torch.tensor(0.0), torch.tensor(0.0)\n",
    "    if epoch > cfg.training.gan_warmup:\n",
    "        # --- Discriminator step (after warmup) ---\n",
    "        tstate.opt_disc.zero_grad()\n",
    "        with torch.autocast('cuda'):\n",
    "            img_recon = decoder(z)\n",
    "            d_real = discriminator(img_real)\n",
    "            d_fake = discriminator(torch.sigmoid(img_recon.detach()))\n",
    "            loss_disc = (torch.relu(1 - d_real).mean() + torch.relu(1 + d_fake).mean()) / 2\n",
    "        tstate.scaler_disc.scale(loss_disc).backward()\n",
    "        tstate.scaler_disc.step(tstate.opt_disc)\n",
    "        tstate.scaler_disc.update()\n",
    "        #with torch.no_grad(): print(f\"d_real: {d_real.mean().item():.2f}, d_fake: {d_fake.mean().item():.2f}\")\n",
    "    \n",
    "    # --- Decoder step ---\n",
    "    tstate.opt_dec.zero_grad()\n",
    "    with torch.autocast('cuda'):\n",
    "        img_recon = decoder(z)  # recompute for the sake of the comp. graph\n",
    "        loss_bce = F.binary_cross_entropy_with_logits(img_recon, img_real)\n",
    "        #loss_l1 = tstate.l1_loss(img_recon, img_real)\n",
    "        weights = torch.where(img_real > 0.05, 10.0, 1.0) # non-black pixels are worth more than black pixels\n",
    "        img_recon = torch.sigmoid(img_recon)\n",
    "        loss_l1 = (weights * (img_recon - img_real).abs()).mean()\n",
    "        loss_lpips = tstate.lpips_loss(img_recon.repeat(1,3,1,1), img_real.repeat(1,3,1,1)).mean()  # LPIPS wants 3ch\n",
    "        #loss_dec = loss_l1 + 0.02 * loss_lpips # TODO: adaptive weighting as in RAE paper (??)\n",
    "        loss_dec = loss_bce\n",
    "        if epoch > cfg.training.gan_warmup: \n",
    "            loss_gan = -discriminator(img_recon).mean() # - because generator wants discriminator to say \"real\"\n",
    "            loss_dec +=  0.01 * loss_gan\n",
    "    tstate.scaler_dec.scale(loss_dec).backward(retain_graph=True)\n",
    "    tstate.scaler_dec.step(tstate.opt_dec)\n",
    "    tstate.scaler_dec.update()\n",
    "    \n",
    "    keys = ['disc', 'l1', 'lpips', 'gan', 'dec','bce']\n",
    "    vals = [loss_disc, loss_l1, loss_lpips, loss_gan, loss_dec, loss_bce]\n",
    "    losses = { k:v.item() for k,v in zip(keys, vals) }\n",
    "    return losses, img_recon.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@hydra.main(version_base=None, config_path='../configs', config_name='config')\n",
    "def train(cfg: DictConfig):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    preencoded = cfg.get('preencoded', False)\n",
    "    \n",
    "    train_dl, val_dl                = setup_dataloaders(cfg, preencoded)\n",
    "    encoder, decoder, discriminator = setup_models(cfg, device, preencoded) \n",
    "    tstate                          = setup_tstate(cfg, device, decoder, discriminator, encoder=encoder)\n",
    "    if not(cfg.get('no_wandb', False)):  wandb.init(project='dec-'+cfg.wandb.project, config=dict(cfg)) # CLI: +no_wanb=true \n",
    "    global_step = 0 \n",
    "    for epoch in range(1, cfg.training.dec_epochs + 1):\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dl, desc=f'Epoch {epoch}/{cfg.training.dec_epochs}'):\n",
    "            global_step += 1\n",
    "            eb = get_embeddings_batch(batch, encoder, preencoded, device)\n",
    "            z1, z2, non_empty1, non_empty2, deltas, img_real = eb['z1'], eb['z2'], eb['non_empty1'], eb['non_empty2'], eb['deltas'], eb['img2']\n",
    "            if tstate.opt_enc is not None: tstate.opt_enc.zero_grad()\n",
    "            losses, img_recon = train_step(epoch, z2, img_real, decoder, discriminator, tstate, cfg)\n",
    "            train_loss += losses['dec']\n",
    "\n",
    "            # encoder fine-tuning\n",
    "            if tstate.opt_enc is not None: \n",
    "                encoder.train()\n",
    "                z1 = z1.reshape(-1, z1.shape[-1])\n",
    "                z2 = z2.reshape(-1, z2.shape[-1])\n",
    "                num_tokens =  z1.shape[0] // len(deltas)  # or just 65\n",
    "                deltas = deltas.repeat_interleave(num_tokens, dim=0)\n",
    "                enc_loss_dict = calc_enc_loss(z1, z2, global_step, deltas=deltas, lambd=cfg.training.lambd, non_emptys=(non_empty1,non_empty2))\n",
    "                lambda_enc = 100\n",
    "                enc_loss_dict['loss'] *= lambda_enc\n",
    "                enc_loss_dict['loss'].backward()\n",
    "                if False and tstate.opt_enc is not None:\n",
    "                    grad_norm = sum(p.grad.norm().item() for p in encoder.parameters() if p.grad is not None)\n",
    "                    print(f\"Encoder grad norm: {grad_norm:.4f}\")\n",
    "                tstate.opt_enc.step()\n",
    "\n",
    "            if wandb.run is not None: wandb.log({'train_dec': losses['dec'], 'train_l1': losses['l1'], 'train_lpips': losses['lpips'], \n",
    "                    'train_gan': losses['gan'],'train_disc': losses['disc'], 'epoch': epoch})\n",
    "                    #'enc_loss':enc_loss_dict['loss'].item(), \n",
    "        \n",
    "        train_loss /= len(train_dl)\n",
    "        print(f'Epoch {epoch}: train_loss={train_loss:.4f}')\n",
    "        \n",
    "        # validation, checkpointing, visualization e.g. reconstruction comparison\n",
    "        decoder.eval() \n",
    "        with torch.no_grad():\n",
    "            val_loss, loss_gan = 0, torch.tensor(0.0)\n",
    "            for batch in val_dl:\n",
    "                eb = get_embeddings_batch(batch, encoder, preencoded, device)\n",
    "                z2, pos2, img_real = eb['z2'], eb['pos2'], eb['img2']\n",
    "                img_recon = decoder(z2)  # recompute for the sake of the comp. graph\n",
    "                loss_bce = F.binary_cross_entropy_with_logits(img_recon, img_real)\n",
    "                #loss_l1 = tstate.l1_loss(img_recon, img_real)\n",
    "                img_recon = torch.sigmoid(img_recon) \n",
    "                \n",
    "                weights = torch.where(img_real > 0.05, 10.0, 1.0)  # non-black pixels are worth more than black pixels\n",
    "                loss_l1 = (weights * (img_recon - img_real).abs()).mean()\n",
    "                loss_lpips = tstate.lpips_loss(img_recon.repeat(1,3,1,1), img_real.repeat(1,3,1,1)).mean()  # LPIPS wants 3ch\n",
    "                #loss_dec = loss_l1 + 0.02 * loss_lpips # TODO: adaptive weighting as in RAE paper (??)\n",
    "                loss_dec = loss_bce\n",
    "                if epoch > cfg.training.gan_warmup:\n",
    "                    loss_gan = -discriminator(img_recon).mean() # - because generator wants discriminator to say \"real\"\n",
    "                    loss_dec +=  0.01 * loss_gan\n",
    "                val_loss += loss_dec.item()\n",
    "            val_loss /= len(val_dl) \n",
    "\n",
    "\n",
    "            if wandb.run is not None: \n",
    "                viz_mae_recon(img_recon, batch['img2'], epoch=epoch)\n",
    "                wandb.log({'val_dec': loss_dec, 'val_l1': loss_l1, 'val_lpips': loss_lpips, 'val_bce':loss_bce,\n",
    "                    'val_gan': loss_gan, 'epoch': epoch, \n",
    "                    \"lr_dec\": tstate.opt_dec.param_groups[0]['lr'], \"lr_disc\": tstate.opt_disc.param_groups[0]['lr'],})\n",
    "                    #\"lr_enc\": tstate.opt_enc.param_groups[0]['lr'], })\n",
    "                    #'val_real': wandb.Image(grid_real, caption=f\"Epoch {epoch}\"), 'val_recon': wandb.Image(grid_recon, caption=f\"Epoch {epoch}\") })\n",
    "\n",
    "        save_checkpoint(decoder,        tstate.opt_dec, epoch, val_loss, cfg, tag=\"dec_\")\n",
    "        save_checkpoint(discriminator, tstate.opt_disc, epoch, val_loss, cfg, tag=\"disc_\")\n",
    "        tstate.scheduler.step()\n",
    "        if tstate.scheduler_enc is not None: tstate.scheduler_enc.step()\n",
    "        if epoch > cfg.training.gan_warmup: tstate.schedulerD.step()\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| eval: false\n",
    "if __name__ == \"__main__\" and \"ipykernel\" not in __import__(\"sys\").modules:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
