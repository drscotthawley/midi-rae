{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436af6f6",
   "metadata": {
    "time_run": "2026-02-07T04:42:32.786675+00:00"
   },
   "source": [
    "# Inspect\n",
    "\n",
    "> (Notebooke only) Interactive exploration of trained encoder and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be90fe",
   "metadata": {
    "time_run": "2026-02-07T04:41:16.576489+00:00"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "from midi_rae.vit import ViTEncoder\n",
    "from midi_rae.data import PRPairDataset\n",
    "from midi_rae.viz import make_emb_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a87a2e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105de56",
   "metadata": {
    "time_run": "2026-02-07T04:41:22.397634+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "cfg = OmegaConf.load('../configs/config.yaml')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d03473",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d18b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "val_ds = PRPairDataset(split='val', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y)\n",
    "val_dl = DataLoader(val_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True)\n",
    "print(f'Loaded {len(val_ds)} validation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff2335",
   "metadata": {},
   "source": [
    "## Load Encoder from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "ckpt_path = '../checkpoints/enc_best.pt'  # <-- change as needed\n",
    "\n",
    "model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), \n",
    "                   cfg.model.patch_size, cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "print(f'Loaded checkpoint from {ckpt_path}, epoch {ckpt.get(\"epoch\", \"?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2fff6",
   "metadata": {},
   "source": [
    "## Run Batch Through Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "batch = next(iter(val_dl))\n",
    "img1, img2 = batch['img1'].to(device), batch['img2'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z1, pmask1 = model(img1, return_cls_only=False)\n",
    "    z2, pmask2 = model(img2, return_cls_only=False)\n",
    "\n",
    "z1 = z1.reshape(-1, z1.shape[-1])\n",
    "z2 = z2.reshape(-1, z2.shape[-1])\n",
    "num_tokens = z1.shape[0] // len(batch['img1'])\n",
    "\n",
    "print(f'z1: {z1.shape}, z2: {z2.shape}, num_tokens: {num_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76cbc6",
   "metadata": {},
   "source": [
    "## Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56039ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Combined embeddings from both views\n",
    "zs = torch.cat((z1, z2), dim=0)\n",
    "pmask = torch.cat([pmask1, pmask2], dim=0)\n",
    "file_idx = batch['file_idx']\n",
    "\n",
    "# Interactive visualization (without wandb logging)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "from midi_rae.viz import umap_project, pca_project, plot_embeddings_3d\n",
    "\n",
    "\n",
    "coords = pca_project(zs.cpu())\n",
    "fig = plot_embeddings_3d(coords, title='Embeddings (PCA)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a705e9e8",
   "metadata": {},
   "source": [
    "## Inspect Individual Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show a sample image pair\n",
    "idx = 0\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(img1[idx, 0].cpu(), cmap='gray')\n",
    "axes[0].set_title(f'Image 1 (file_idx={file_idx[idx].item()})')\n",
    "axes[1].imshow(img2[idx, 0].cpu(), cmap='gray')\n",
    "axes[1].set_title(f'Image 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
