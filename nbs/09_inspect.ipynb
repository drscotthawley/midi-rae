{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436af6f6",
   "metadata": {
    "time_run": "2026-02-07T04:42:32.786675+00:00"
   },
   "source": [
    "# Inspect\n",
    "\n",
    "> (Notebooke only) Interactive exploration of trained encoder and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be90fe",
   "metadata": {
    "time_run": "2026-02-07T04:41:16.576489+00:00"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from omegaconf import OmegaConf\n",
    "from midi_rae.vit import ViTEncoder\n",
    "from midi_rae.data import PRPairDataset\n",
    "from midi_rae.viz import make_emb_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a87a2e",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105de56",
   "metadata": {
    "time_run": "2026-02-07T04:41:22.397634+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "cfg = OmegaConf.load('../configs/config.yaml')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d03473",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d18b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "val_ds = PRPairDataset(split='val', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y)\n",
    "val_dl = DataLoader(val_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True)\n",
    "print(f'Loaded {len(val_ds)} validation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff2335",
   "metadata": {},
   "source": [
    "## Load Encoder from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f8890",
   "metadata": {
    "time_run": "2026-02-07T05:07:09.980040+00:00"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../checkpoints/enc_best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m",
      "\u001b[32m      3\u001b[39m ckpt_path = \u001b[33m'\u001b[39m\u001b[33m../checkpoints/enc_best.pt\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# <-- change as needed\u001b[39;00m",
      "\u001b[32m      5\u001b[39m model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), ",
      "\u001b[32m      6\u001b[39m                    cfg.model.patch_size, cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)",
      "\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m",
      "\u001b[32m      9\u001b[39m state_dict = {k.replace(\u001b[33m'\u001b[39m\u001b[33m_orig_mod.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ckpt[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m].items()}",
      "\u001b[32m     10\u001b[39m model.load_state_dict(state_dict, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)",
      "",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/serialization.py:1500\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m",
      "\u001b[32m   1497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args:",
      "\u001b[32m   1498\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m",
      "\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:",
      "\u001b[32m   1501\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):",
      "\u001b[32m   1502\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m",
      "\u001b[32m   1503\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m",
      "\u001b[32m   1504\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m",
      "\u001b[32m   1505\u001b[39m         orig_position = opened_file.tell()",
      "",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/serialization.py:768\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m",
      "\u001b[32m    766\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:",
      "\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):",
      "\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m",
      "\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:",
      "\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:",
      "",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/serialization.py:749\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m",
      "\u001b[32m    748\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m | os.PathLike[\u001b[38;5;28mstr\u001b[39m], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:",
      "\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)",
      "",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../checkpoints/enc_best.pt'"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "ckpt_path = '../checkpoints/enc_best.pt'  # <-- change as needed\n",
    "\n",
    "model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), \n",
    "                   cfg.model.patch_size, cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "state_dict = {k.replace('_orig_mod.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "print(f'Loaded checkpoint from {ckpt_path}, epoch {ckpt.get(\"epoch\", \"?\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2fff6",
   "metadata": {
    "time_run": "2026-02-07T05:05:56.376865+00:00"
   },
   "source": [
    "## Run Batch Through Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "batch = next(iter(val_dl))\n",
    "img1, img2 = batch['img1'].to(device), batch['img2'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z1, pmask1 = model(img1, return_cls_only=False)\n",
    "    z2, pmask2 = model(img2, return_cls_only=False)\n",
    "\n",
    "z1 = z1.reshape(-1, z1.shape[-1])\n",
    "z2 = z2.reshape(-1, z2.shape[-1])\n",
    "num_tokens = z1.shape[0] // len(batch['img1'])\n",
    "\n",
    "print(f'z1: {z1.shape}, z2: {z2.shape}, num_tokens: {num_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76cbc6",
   "metadata": {},
   "source": [
    "## Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56039ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Combined embeddings from both views\n",
    "zs = torch.cat((z1, z2), dim=0)\n",
    "pmask = torch.cat([pmask1, pmask2], dim=0)\n",
    "file_idx = batch['file_idx']\n",
    "\n",
    "# Interactive visualization (without wandb logging)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "from midi_rae.viz import umap_project, pca_project, plot_embeddings_3d\n",
    "\n",
    "\n",
    "coords = pca_project(zs.cpu())\n",
    "fig = plot_embeddings_3d(coords, title='Embeddings (PCA)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a705e9e8",
   "metadata": {},
   "source": [
    "## Inspect Individual Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show a sample image pair\n",
    "idx = 0\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(img1[idx, 0].cpu(), cmap='gray')\n",
    "axes[0].set_title(f'Image 1 (file_idx={file_idx[idx].item()})')\n",
    "axes[1].imshow(img2[idx, 0].cpu(), cmap='gray')\n",
    "axes[1].set_title(f'Image 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
