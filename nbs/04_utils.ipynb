{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dc6903",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93157b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96051a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71615b73-c28e-41b0-9905-a99045419702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def save_checkpoint(model, epoch, val_loss, cfg, optimizer=None, save_every=25, n_keep=5, verbose=True, tag=\"\"):\n",
    "    \"\"\"Saves new checkpoint, keeps best & the most recent n_keep.\n",
    "       Can loop over multiple models. (Saves separate files for each model)\"\"\"\n",
    "    if not hasattr(save_checkpoint, 'best_val_loss'):\n",
    "        save_checkpoint.best_val_loss = float('inf')\n",
    "    if epoch % save_every != 0 and val_loss >= save_checkpoint.best_val_loss: return\n",
    "\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    models = model if isinstance(model, (list, tuple)) else [model]\n",
    "    for i, m in enumerate(models):\n",
    "        ckpt = {'epoch': epoch, 'model_state_dict': getattr(m, '_orig_mod', m).state_dict(), 'config': dict(cfg), 'val_loss': val_loss}\n",
    "        if optimizer is not None and i == 0: ckpt = ckpt | {'optimizer_state_dict': optimizer.state_dict()}  # Only first model on list gets optimizer in its ckpt file.\n",
    "\n",
    "        mtag = f\"{getattr(m, '_orig_mod', m).__class__.__name__}_{tag}\"\n",
    "        if epoch % save_every == 0:\n",
    "            if verbose: print(f\"Saving checkpoint to checkpoints/{mtag}ckpt_epoch{epoch}.pt\")\n",
    "            torch.save(ckpt, f'checkpoints/{mtag}ckpt_epoch{epoch}.pt')\n",
    "        if val_loss < save_checkpoint.best_val_loss:\n",
    "            torch.save(ckpt, f'checkpoints/{mtag}_best.pt')\n",
    "\n",
    "        # delete any checkpoints older than the n_keep-th one\n",
    "        ckpts = sorted([f for f in os.listdir('checkpoints') if f.startswith(f'{mtag}ckpt_epoch')],\n",
    "                   key=lambda x: os.path.getmtime(f'checkpoints/{x}'))\n",
    "        for old in ckpts[:-n_keep]: os.remove(f'checkpoints/{old}')\n",
    "\n",
    "    if val_loss <= save_checkpoint.best_val_loss:\n",
    "        save_checkpoint.best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_checkpoint(model, ckpt_path:str, return_all=False, weights_only=False, strict=False):\n",
    "    \"loads a model (and maybe other things) from a checkpoint file\"\n",
    "    device = next(model.parameters()).device\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=weights_only)\n",
    "    print(f\">>> Loaded model checkpoint from {ckpt_path}\")\n",
    "    ckpt['model_state_dict'] = {k.replace('_orig_mod.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "    model.load_state_dict(ckpt['model_state_dict'], strict=strict)\n",
    "    return (model, ckpt) if return_all else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afde7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
