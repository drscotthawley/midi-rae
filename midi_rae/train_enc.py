"""Encoder training script for midi_rae"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_train_enc.ipynb.

# %% auto #0
__all__ = ['curr_learn', 'compute_batch_loss', 'train']

# %% ../nbs/06_train_enc.ipynb #98edfbef
import os
from itertools import chain
import multiprocessing as mp
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import wandb
from hydra import compose, initialize
from omegaconf import DictConfig, OmegaConf
import hydra
from .vit import ViTEncoder, LightweightMAEDecoder
from .data import PRPairDataset
from .losses import calc_enc_loss, calc_mae_loss
from .utils import save_checkpoint
from .viz import make_emb_viz, viz_mae_recon
from tqdm.auto import tqdm

torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('high')

# %% ../nbs/06_train_enc.ipynb #e6d4f354
def curr_learn(shared_ct_dict, epoch, interval=100, verbose=False): 
    "UNUSED/UNNECESSARY: curriculum learning: increase difficulty with epoch"
    if epoch < interval: return shared_ct_dict['training']
    training = shared_ct_dict['training']
    training['max_shift_x'] = min(12, 6 + epoch // interval)
    training['max_shift_y'] = min(12, 6 + epoch // interval)
    if verbose: 
        print(f"curr_learn: max_shift_x = {training['max_shift_x']}, max_shift_y = {training['max_shift_y']}")
    return training

# %% ../nbs/06_train_enc.ipynb #d3f6162a
def compute_batch_loss(batch, encoder, cfg, global_step, mae_decoder=None):
    "Compute loss and return other exal auxiliary variables (for train or val)"
    device = next(encoder.parameters()).device
    img1, img2, deltas = batch['img1'].to(device), batch['img2'].to(device), batch['deltas'].to(device)
    z1, pmask1, pos1, mae_mask1 = encoder(img1, return_cls_only=False)
    z2, pmask2, pos2, mae_mask2 = encoder(img2, return_cls_only=False, mae_mask=mae_mask1) # same mae_mask for both
    pmask1_visible, pmask2_visible = pmask1[:, mae_mask1], pmask2[:, mae_mask1]
    loss_dict = {}
    if mae_decoder is not None:
        recon_patches = mae_decoder(z2, pos2, mae_mask2) # just pick z2 and ignore z1 
        loss_dict['mae'] = calc_mae_loss(recon_patches, img2, pos2, mae_mask2)

    z1 = z1.reshape(-1, z1.shape[-1])
    z2 = z2.reshape(-1, z2.shape[-1])
    pmasks = (pmask1_visible, pmask2_visible)
    num_tokens =  z1.shape[0] // len(deltas)  # or just 65
    deltas = deltas.repeat_interleave(num_tokens, dim=0)
    loss_dict = loss_dict | calc_enc_loss(z1, z2, global_step, deltas=deltas, lambd=cfg.training.lambd, pmasks=pmasks)
    if 'mae' in loss_dict.keys(): loss_dict['loss'] += cfg.training.get('mae_lambda', 1.0) * loss_dict['mae']

    if torch.isnan(loss_dict['loss']):
        print("NaN detected!", {k: v.item() if hasattr(v, 'item') else v for k, v in loss_dict.items()})
        breakpoint()
    return loss_dict, z1, z2, pmasks, pos2, mae_mask2, num_tokens, recon_patches

# %% ../nbs/06_train_enc.ipynb #6713ab74
@hydra.main(version_base=None, config_path="../configs", config_name="config")
def train(cfg: DictConfig):
    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
    print("device = ",device)

    train_ds = PRPairDataset(image_dataset_dir=cfg.data.path, split='train', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) 
    val_ds   = PRPairDataset(image_dataset_dir=cfg.data.path, split='val',  max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y) 
    train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True)
    val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, pin_memory=True)

    # next bit is to enable curriculum learning, dataloaders re-defined per epoch, gotta use nested function
    manager = mp.Manager()
    shared_ct_dict = manager.dict(OmegaConf.to_container(cfg))
    def worker_init_fn(worker_id):
        ds = torch.utils.data.get_worker_info().dataset
        ds.max_shift_x = shared_ct_dict['training']['max_shift_x']
        ds.max_shift_y = shared_ct_dict['training']['max_shift_y']

    model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), cfg.model.patch_size, 
              cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)
    model = torch.compile(model)
    mae_decoder = LightweightMAEDecoder(patch_size=cfg.model.patch_size, dim=cfg.model.dim).to(device)

    #optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.training.lr)
    optimizer = torch.optim.AdamW(chain(model.parameters(), mae_decoder.parameters()), lr=cfg.training.lr)

    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=cfg.training.lr, steps_per_epoch=1, epochs=cfg.training.epochs)
    scaler = torch.amp.GradScaler(device)
    if not(cfg.get('no_wandb', False)): 
        wandb.init(project=cfg.wandb.project, config=dict(cfg), settings=wandb.Settings(start_method="fork", _disable_stats=True))
        wandb.define_metric("epoch")
        wandb.define_metric("*", step_metric="epoch")

    # Training loop
    global_step = 0
    viz_every = 10
    for epoch in range(1, cfg.training.epochs+1):
        wandb.log({"epoch": epoch})
        model.train()
        train_loss = 0
        if False and epoch > 1: # curriculum learning, easily turned off by setting this to False. DL's re-defined each epoch to init workers
            shared_ct_dict['training'] = curr_learn(shared_ct_dict, epoch)
            train_dl = DataLoader(train_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)
            val_dl   = DataLoader(val_ds,   batch_size=cfg.training.batch_size, num_workers=4, drop_last=True, worker_init_fn=worker_init_fn, pin_memory=True)
        for batch in tqdm(train_dl, desc=f"Epoch {epoch}/{cfg.training.epochs}"):
            global_step += 1
            optimizer.zero_grad()
            with torch.autocast('cuda'):
                loss_dict, z1, z2, pmasks, pos2, mae_mask2, num_tokens, recon_patches = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)
            scaler.scale(loss_dict['loss']).backward()
            scaler.step(optimizer)
            scaler.update()
            train_loss += loss_dict['loss'].item()
            
        # At end of Epoch: validation, viz, etc
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_dl:
                val_loss_dict, z1, z2, pmasks, pos2, mae_mask2, num_tokens, recon_patches = compute_batch_loss(batch, model, cfg, global_step, mae_decoder=mae_decoder)
                val_loss += val_loss_dict['loss'].item()

            train_loss /= len(train_dl)
            val_loss /= len(val_dl)
            print(f"Epoch {epoch}/{cfg.training.epochs}: train_loss={train_loss:.3f} val_loss={val_loss:.3f}")
            
            if wandb.run is not None:
                wandb.log({ 
                    "train_loss":train_loss, "train_sim":loss_dict['sim'], "train_sigreg":loss_dict['sigreg'], "train_anchor":loss_dict['anchor'], "train_mae":loss_dict['mae'],  
                    "val_loss":val_loss, "val_sim":val_loss_dict['sim'], "val_sigreg":val_loss_dict['sigreg'], "val_anchor":val_loss_dict['anchor'], "val_mae": val_loss_dict['mae'], 
                    "max_shift_x":shared_ct_dict['training']['max_shift_x'], "max_shift_y":shared_ct_dict['training']['max_shift_y'],
                    "lr": optimizer.param_groups[0]['lr'], "epoch": epoch})

                if epoch % viz_every == 0:
                    zs_stacked = torch.cat((z1, z2), dim=0).reshape(-1, z1.shape[-1])
                    make_emb_viz(zs_stacked, num_tokens, epoch, model=model, pmasks=pmasks, file_idx=batch['file_idx'], deltas=batch['deltas'])
                    if mae_decoder is not None:
                        viz_mae_recon(recon_patches, pos2, batch['img2'])

        save_checkpoint(model, optimizer, epoch, val_loss, cfg, tag="enc_")
        scheduler.step()# val_loss)

# %% ../nbs/06_train_enc.ipynb #dc55b9c3
#| eval: false
if __name__ == "__main__" and "ipykernel" not in __import__("sys").modules:
    train()
