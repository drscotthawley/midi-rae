"""LeJEPA, GAN discriminator, ...aand more"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_losses.ipynb.

# %% auto #0
__all__ = ['SIGReg', 'attraction_loss', 'LeJEPA', 'anchor_loss', 'calc_enc_loss', 'PatchGANDiscriminator']

# %% ../nbs/03_losses.ipynb #b96051a7
import torch
import torch.nn as nn

# %% ../nbs/03_losses.ipynb #a164c279
def SIGReg(x, global_step, num_slices=256):
    """SIGReg with Epps-Pulley statistic. x is (N, K) tensor."""
    device = x.device
    g = torch.Generator(device=device).manual_seed(global_step)
    proj_shape = (x.size(1), num_slices)
    A = torch.randn(proj_shape, generator=g, device=device)
    A = A / (A.norm(dim=0, keepdim=True) + 1e-10)  # normalize columns
    
    # Epps-Pulley statistic
    t = torch.linspace(-5, 5, 17, device=device) # values used in LeJEPA paper, worked for SSLtoy
    exp_f = torch.exp(-0.5 * t**2)  # theoretical CF for N(0,1)
    x_t = (x @ A).unsqueeze(2) * t  # (N, M, T)
    ecf = (torch.exp(1j * x_t).mean(dim=0)).abs()  # empirical CF
    diff = (ecf - exp_f).abs().square().mul(exp_f)  # weighted L2 distance
    #N = x.size(0)  # With respect to Yann: Don't scale by N because then if you change the batch size you have to retune lambd by hand ugh
    T = torch.trapz(diff, t, dim=1).sum() #* N
    return T

# %% ../nbs/03_losses.ipynb #cf61d695
def attraction_loss(z1, z2,  # embeddings of two "views" of the same thing (in batches)
                    deltas=None,   # optional/TBD: info on semantic 'distance' between z1 & z2
                    tau = 100.0):    # inverse strength of fall-off for delta distances, big=slower
    "How we pull similar 'views' together"
    if deltas is None: return (z1 - z2).square().mean()
    delta_diag = (deltas**2).sum(dim=1)
    delta_fac = torch.exp(-delta_diag / tau) # less attraction for more 'distant' views
    #delta_fac = 1/(1 + delta_diag/tau)  # longer tail than exp
    return ((z1 - z2).square() * delta_fac.unsqueeze(-1) ).mean()

# %% ../nbs/03_losses.ipynb #624570b6
def LeJEPA(z1, z2, global_step, lambd=0.5, deltas=None): 
    "Main LeJEPA loss function"
    sim = attraction_loss(z1, z2, deltas=deltas)
    sigreg = SIGReg( torch.cat((z1, z2), dim=0), global_step ) * 2 # normalize to similar scale as sim
    return {'loss': (1-lambd)*sim + lambd*sigreg, 'sim':sim.item(), 'sigreg':sigreg.item()}

# %% ../nbs/03_losses.ipynb #3fa01fc2
def anchor_loss(z1, z2):
    "Anchor embeddings of empty patches to the origin"
    return z1.square().mean() + z2.square().mean()

# %% ../nbs/03_losses.ipynb #5a89c2b1
def calc_enc_loss(z1, z2, global_step, deltas=None, lambd=0.5, pmasks=(None,None), lambda_anchor=0.1):
    "Main loss function for Encoder"
    pmask1, pmask2 = pmasks
    pmask = pmask1 & pmask2  # both non-empty
    valid = pmask.view(-1).bool()
    loss_dict = LeJEPA(z1[valid], z2[valid], global_step, deltas=deltas[valid], lambd=lambd)
    aloss = anchor_loss(z1[~pmask1.view(-1).bool()], z2[~pmask2.view(-1).bool()])
    loss_dict['anchor'] = aloss
    loss_dict['loss'] = loss_dict['loss'] + lambda_anchor * aloss
    return loss_dict

# %% ../nbs/03_losses.ipynb #ade84ead
class PatchGANDiscriminator(nn.Module):
    def __init__(self, in_ch=1, base_ch=64, n_layers=3, use_spectral_norm=True):
        super().__init__()
        norm = nn.utils.spectral_norm if use_spectral_norm else (lambda x: x)
        layers = [norm(nn.Conv2d(in_ch, base_ch, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, True)]
        ch = base_ch
        for i in range(1, n_layers):
            ch_next = min(ch * 2, 512)  # double channels each layer, but cap at 512 to limit params
            layers += [norm(nn.Conv2d(ch, ch_next, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, True)]
            ch = ch_next
        layers.append(norm(nn.Conv2d(ch, 1, kernel_size=4, stride=1, padding=1)))
        self.net = nn.Sequential(*layers)

    def forward(self, x): return self.net(x)
