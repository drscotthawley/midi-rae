"""LeJEPA, GAN discriminator, ...aand more"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_losses.ipynb.

# %% auto #0
__all__ = ['safe_mean', 'SIGReg', 'attraction_loss', 'LeJEPA', 'calc_mae_loss', 'anchor_loss', 'calc_enc_loss',
           'PatchGANDiscriminator']

# %% ../nbs/03_losses.ipynb #b96051a7
import torch
import torch.nn as nn
import torch.nn.functional as F 

# %% ../nbs/03_losses.ipynb #59c62704
def safe_mean(t, dim=None): 
    """safe replacement for torch.mean( ).  can't be used as a suffix though"""
    return t.mean(dim=dim) if t.numel() > 0 else 0.0


# %% ../nbs/03_losses.ipynb #a164c279
def SIGReg(x, global_step, num_slices=256):
    """SIGReg with Epps-Pulley statistic. x is (N, K) tensor."""
    device = x.device
    g = torch.Generator(device=device).manual_seed(global_step)
    proj_shape = (x.size(1), num_slices)
    A = torch.randn(proj_shape, generator=g, device=device)
    A = A / (A.norm(dim=0, keepdim=True) + 1e-10)  # normalize columns
    
    # Epps-Pulley statistic
    t = torch.linspace(-5, 5, 17, device=device) # values used in LeJEPA paper, worked for SSLtoy
    exp_f = torch.exp(-0.5 * t**2)  # theoretical CF for N(0,1)
    x_t = (x @ A).unsqueeze(2) * t  # (N, M, T)
    ecf = (torch.exp(1j * x_t).mean(dim=0)).abs()  # empirical CF
    diff = (ecf - exp_f).abs().square().mul(exp_f)  # weighted L2 distance
    #N = x.size(0)  # With respect to Yann: Don't scale by N because then if you change the batch size you have to retune lambd by hand ugh
    T = torch.trapz(diff, t, dim=1).sum() #* N  # sum here is over num slices, not data points
    return T

# %% ../nbs/03_losses.ipynb #cf61d695
def attraction_loss(z1, z2,  # embeddings of two "views" of the same thing (in batches)
                    deltas=None,   # optional/TBD: info on semantic 'distance' between z1 & z2
                    tau = 100.0):    # inverse strength of fall-off for delta distances, big=slower
    "How we pull similar 'views' together"
    if deltas is None: return safe_mean( (z1 - z2).square() )
    delta_diag = (deltas**2).sum(dim=1)
    delta_fac = torch.exp(-delta_diag / tau) # less attraction for more 'distant' views
    #delta_fac = 1/(1 + delta_diag/tau)  # longer tail than exp
    return safe_mean( (z1 - z2).square() * delta_fac.unsqueeze(-1) )

# %% ../nbs/03_losses.ipynb #624570b6
def LeJEPA(z1, z2, global_step, lambd=0.5, deltas=None): 
    "Main LeJEPA loss function"
    sim = attraction_loss(z1, z2, deltas=deltas)
    sigreg = SIGReg( torch.cat((z1, z2), dim=0), global_step ) * 1 # normalize to similar scale as sim
    return {'loss': (1-lambd)*sim + lambd*sigreg, 'sim':sim.item(), 'sigreg':sigreg.item()}

# %% ../nbs/03_losses.ipynb #34ecc897
def calc_mae_loss(recon_patches, img, pos_full, mae_mask, patch_size=16, all_patches=False, use_bce=True):
    """for now, MSE evaluated at masked patches.  TODO: upgrade to some other loss? 
    """
    img_patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)  # (B, C, H//ps, W//ps, ps, ps)
    img_patches = img_patches.flatten(2, 3).flatten(-2, -1).squeeze(1)    # (B, C, N, ps*ps) -> reshape to (B, N, ps*ps)
    if all_patches:  # ignore mae mask 
        if use_bce: 
            return F.binary_cross_entropy_with_logits(recon_patches[:, 1:, :], img_patches) 
        return safe_mean((recon_patches[:, 1:, :] - img_patches).square())
    if use_bce: 
        return F.binary_cross_entropy_with_logits(recon_patches[:, 1:, :][:, ~mae_mask[1:], :] , img_patches[:, ~mae_mask[1:], :])
    return safe_mean((recon_patches[:, 1:, :][:, ~mae_mask[1:], :] - img_patches[:, ~mae_mask[1:], :]).square())

# %% ../nbs/03_losses.ipynb #3fa01fc2
def anchor_loss(z1, z2):
    "Anchor embeddings of empty patches to the origin"
    return safe_mean( z1.square() ) + safe_mean( z2.square() )

# %% ../nbs/03_losses.ipynb #5a89c2b1
def calc_enc_loss(z1, z2, global_step, deltas=None, lambd=0.5, pmasks=(None,None), lambda_anchor=0.1):
    "Main loss function for Encoder"
    pmask1, pmask2 = pmasks
    pmask = pmask1 & pmask2  # both non-empty
    valid = pmask.view(-1).bool()
    loss_dict = LeJEPA(z1[valid], z2[valid], global_step, deltas=deltas[valid], lambd=lambd)
    aloss = anchor_loss(z1[~pmask1.view(-1).bool()], z2[~pmask2.view(-1).bool()])
    loss_dict['anchor'] = aloss
    loss_dict['loss'] = loss_dict['loss'] + lambda_anchor * aloss
    return loss_dict

# %% ../nbs/03_losses.ipynb #ade84ead
class PatchGANDiscriminator(nn.Module):
    def __init__(self, in_ch=1, base_ch=64, n_layers=3, use_spectral_norm=True):
        super().__init__()
        norm = nn.utils.spectral_norm if use_spectral_norm else (lambda x: x)
        layers = [norm(nn.Conv2d(in_ch, base_ch, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, True)]
        ch = base_ch
        for i in range(1, n_layers):
            ch_next = min(ch * 2, 512)  # double channels each layer, but cap at 512 to limit params
            layers += [norm(nn.Conv2d(ch, ch_next, kernel_size=4, stride=2, padding=1)), nn.LeakyReLU(0.2, True)]
            ch = ch_next
        layers.append(norm(nn.Conv2d(ch, 1, kernel_size=4, stride=1, padding=1)))
        self.net = nn.Sequential(*layers)

    def forward(self, x): return self.net(x)
