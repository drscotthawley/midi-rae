"""Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/swin_dev_chat.ipynb.

# %% auto #0
__all__ = ['SwinEncoder']

# %% ../nbs/swin_dev_chat.ipynb #d51a52f7
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, Set, Type, Union
from functools import partial

from timm.models.swin_transformer_v2 import SwinTransformerV2Stage
from timm.layers import trunc_normal_, to_2tuple, calculate_drop_path_rates
from .core import PatchState, HierarchicalPatchState, EncoderOutput

# %% ../nbs/swin_dev_chat.ipynb #1347f17a
class SwinEncoder(nn.Module):
    "Swin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder.  (Wrapper for timm routines)"
    def __init__(self,
                 img_height:int,           # Input image height in pixels (e.g. 128)
                 img_width:int,            # Input image width in pixels (e.g. 128)
                 patch_h:int=2,            # Patch height for initial embedding
                 patch_w:int=2,            # Patch width for initial embedding
                 in_chans:int=1,           # Number of input channels (1 for piano roll)
                 embed_dim:int=4,          # Base embedding dimension (doubles each stage)
                 depths:tuple=(1,1,2,2,6,2,1),   # Number of transformer blocks per stage
                 num_heads:tuple=(1,1,1,2,4,8,16),# Attention heads per stage
                 window_size:int=8,        # Window size for windowed attention
                 mlp_ratio:float=4.,       # MLP hidden dim = embed_dim * mlp_ratio
                 qkv_bias:bool=True,       # Add bias to QKV projections
                 drop_rate:float=0.,       # Dropout after patch embedding
                 proj_drop_rate:float=0.,  # Dropout after attention projection
                 attn_drop_rate:float=0.,  # Dropout on attention weights
                 drop_path_rate:float=0.1, # Stochastic depth rate
                 norm_layer:type=nn.LayerNorm, # Normalization layer class
                 mae_ratio:float=0.,       # Fraction of non-empty patches to mask (0=no masking)
                 empty_mask_ratio:float=0.05): # Mask rate for empty patches relative to mae_ratio
        super().__init__()
        self.num_stages, self.embed_dim = len(depths), embed_dim
        self.num_features = int(embed_dim * 2 ** (self.num_stages - 1))
        self.patch_h, self.patch_w, self.grid_size = patch_h, patch_w,  (img_height // patch_h, img_width // patch_w)
        self.mae_ratio, self.empty_mask_ratio = mae_ratio, empty_mask_ratio

        # Patch embedding
        self.patch_embed = nn.Conv2d(in_chans, embed_dim, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))
        self.patch_norm = norm_layer(embed_dim)
        self.pos_drop = nn.Dropout(p=drop_rate)

        # Learnable replacement tokens
        self.empty_token, self.mask_token = nn.Parameter(torch.zeros(embed_dim)), nn.Parameter(torch.zeros(embed_dim))

        # Build stages using timm's SwinTransformerV2Stage
        embed_dims = [int(embed_dim * 2 ** i) for i in range(self.num_stages)]
        dpr = calculate_drop_path_rates(drop_path_rate, list(depths), stagewise=True)
        self.stages = nn.ModuleList()
        in_dim, scale = embed_dims[0], 1
        for i in range(self.num_stages):
            out_dim = embed_dims[i]
            self.stages.append(SwinTransformerV2Stage(
                dim=in_dim, out_dim=out_dim, depth=depths[i], num_heads=num_heads[i],
                input_resolution=(self.grid_size[0] // scale, self.grid_size[1] // scale),
                window_size=window_size, downsample=(i > 0), mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias, proj_drop=proj_drop_rate, attn_drop=attn_drop_rate,
                drop_path=dpr[i], norm_layer=norm_layer))
            in_dim = out_dim
            if i > 0: scale *= 2

        self.norm = norm_layer(self.num_features)
        self.apply(self._init_weights)
        for stage in self.stages: stage._init_respostnorm()

    def _init_weights(self, m):
        if isinstance(m, (nn.Linear, nn.Conv2d)):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None: nn.init.constant_(m.bias, 0)

    @torch.jit.ignore
    def no_weight_decay(self) -> Set[str]:
        nod = {'empty_token', 'mask_token'}
        for n, _ in self.named_parameters():
            if any(kw in n for kw in ('cpb_mlp', 'logit_scale')): nod.add(n)
        return nod

    def _compute_non_empty(self, img):
        "Detect which patches have content (non-black)."
        patches = img.unfold(2, self.patch_h, self.patch_h).unfold(3, self.patch_w, self.patch_w)
        return (patches.amax(dim=(-1, -2)) > 0.2).squeeze(1).flatten(1)  # (B, N)

    def _make_mae_mask(self, non_empty, device):
        "Two-rate MAE mask: non-empty at mae_ratio, empty at mae_ratio*empty_mask_ratio. Returns (B,N) bool, True=visible."
        B, N = non_empty.shape
        rand = torch.rand(B, N, device=device)
        threshold = torch.where(non_empty.bool(),
            torch.full_like(rand, 1.0 - self.mae_ratio),
            torch.full_like(rand, 1.0 - self.mae_ratio * self.empty_mask_ratio))
        return rand < threshold

    def _make_grid_pos(self, h, w, device):
        "Build (h*w, 2) grid positions."
        return torch.stack(torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij'), dim=-1).reshape(-1, 2)

    def forward(self, x, mask_ratio:float=0., mae_mask:Optional[torch.Tensor]=None) -> EncoderOutput:
        "x: (B,C,H,W) piano roll. mask_ratio overrides self.mae_ratio. mae_mask: (N,) bool, True=visible."
        B, device = x.shape[0], x.device
        grid_h, grid_w = self.grid_size
        N_full = grid_h * grid_w

        non_empty = self._compute_non_empty(x)                              # (B, N_full)
        x = self.patch_embed(x)                                             # (B, C, H', W')
        x = self.patch_norm(x.permute(0, 2, 3, 1).contiguous())            # → (B, H', W', C) NHWC
        B, H, W, C = x.shape

        # Replace empty patches with learned empty_token
        ne4d = non_empty.view(B, H, W, 1)
        x = torch.where(ne4d, x, self.empty_token.view(1, 1, 1, -1).expand_as(x))

        # MAE masking: replace masked positions with learned mask_token
        effective_ratio = mask_ratio if mask_ratio > 0 else self.mae_ratio
        if mae_mask is None and effective_ratio > 0:
            mae_mask = self._make_mae_mask(non_empty, device)
        if mae_mask is not None:
            m4d = mae_mask.view(B, H, W, 1)
            x = torch.where(m4d, x, self.mask_token.view(1, 1, 1, -1).expand_as(x))
        else:
            mae_mask = torch.ones(B, N_full, device=device, dtype=torch.bool)
        x = self.pos_drop(x)

        # Run stages, collect intermediates
        intermediates = []
        for stage in self.stages:
            x = stage(x)
            intermediates.append(x)
        intermediates[-1] = self.norm(intermediates[-1])

        # Build non-empty masks for each scale via max-pool cascade
        ne = non_empty.view(B, 1, grid_h, grid_w).float()
        ne_scales = []
        for feat in intermediates:
            Hf, Wf = feat.shape[1], feat.shape[2]
            while ne.shape[2] > Hf:
                ne = F.max_pool2d(ne, 2)
            ne_scales.append(ne.view(B, -1))

        # Build HierarchicalPatchState (coarsest first)
        levels = []
        for feat, ne_s in zip(reversed(intermediates), reversed(ne_scales)):
            Bf, Hf, Wf, Cf = feat.shape
            n = Hf * Wf
            levels.append(PatchState(
                emb=feat.reshape(Bf, n, Cf), pos=self._make_grid_pos(Hf, Wf, device),
                non_empty=ne_s,
                mae_mask=torch.ones(n, device=device, dtype=torch.bool)))

        return EncoderOutput(patches=HierarchicalPatchState(levels=levels),
            full_pos=self._make_grid_pos(grid_h, grid_w, device),
            full_non_empty=non_empty, mae_mask=mae_mask)
