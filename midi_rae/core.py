"""Core data structures for midi-rae: PatchState and HierarchicalPatchState"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto #0
__all__ = ['PatchState', 'HierarchicalPatchState', 'EncoderOutput']

# %% ../nbs/00_core.ipynb #b96051a7
import torch
from dataclasses import dataclass

# %% ../nbs/00_core.ipynb #c1d2e3f4
@dataclass
class PatchState:
    "Bundle of patch embeddings at a single spatial scale with their metadata."
    emb: torch.Tensor      # (B, N, dim) patch embeddings
    pos: torch.Tensor      # (N, 2) grid coordinates (row, col) for each patch
    pmask: torch.Tensor    # (B, N) content mask — 1 where patch has content (e.g. notes), 0 for empty
    mae_mask: torch.Tensor # (N,) MAE visibility mask — 1=visible, 0=masked out for reconstruction

    @property
    def visible(self):
        """New PatchState filtered to only MAE-visible patches"""
        m = self.mae_mask.bool()
        return PatchState(
            emb=self.emb[:, m],
            pos=self.pos[m],
            pmask=self.pmask[:, m],
            mae_mask=self.mae_mask[m],
        )

    @property
    def masked(self):
        """New PatchState filtered to only MAE-masked patches"""
        m = ~self.mae_mask.bool()
        return PatchState(
            emb=self.emb[:, m],
            pos=self.pos[m],
            pmask=self.pmask[:, m],
            mae_mask=self.mae_mask[m],
        )

    @property
    def non_empty_flat(self):
        """Flat bool mask for non-empty patches — useful for loss computation"""
        return self.pmask.reshape(-1).bool()

    @property
    def dim(self): return self.emb.shape[-1]

    @property
    def num_patches(self): return self.emb.shape[1]

    @property
    def batch_size(self): return self.emb.shape[0]

# %% ../nbs/00_core.ipynb #d1e2f3a4
@dataclass
class HierarchicalPatchState:
    "Multi-scale patch states, ordered coarsest → finest (currently: [0]=CLS, [1]=spatial patches)."
    levels: list # List of `PatchState`, one per scale

    @property
    def coarsest(self) -> PatchState: return self.levels[0]

    @property
    def finest(self) -> PatchState: return self.levels[-1]

    @property
    def num_levels(self) -> int: return len(self.levels)

# %% ../nbs/00_core.ipynb #e1f2a3b4
@dataclass
class EncoderOutput:
    "Full encoder output."
    patches: HierarchicalPatchState # Encoded representations (visible patches only)
    full_pos: torch.Tensor          # (N_full, 2) all grid positions before MAE masking (needed by decoder)
    full_pmask: torch.Tensor        # (B, N_full) all content masks before MAE masking
    mae_mask: torch.Tensor          # (N_full,) the MAE mask applied (1=visible, 0=masked)
