"""Components & defs for ViT-based Encoder & Decoder"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_vit.ipynb.

# %% auto #0
__all__ = ['RoPE2D', 'Attention', 'TransformerBlock', 'PatchEmbedding', 'make_mae_mask', 'apply_mae_mask', 'ViTEncoder',
           'Unpatchify', 'ViTDecoder', 'LightweightMAEDecoder']

# %% ../nbs/02_vit.ipynb #b96051a7
import torch
import torch.nn as nn
import torch.nn.functional as F 
from .core import PatchState, HierarchicalPatchState, EncoderOutput

# %% ../nbs/02_vit.ipynb #624570b6
class RoPE2D(nn.Module):
    def __init__(self, head_dim):
        super().__init__()
        i = torch.arange(0, head_dim // 4)
        self.register_buffer('ifreq', 1.0 / (10000 ** (2 * i / head_dim)))
        self.num_patches = -1

    def calc_pi(self, num_patches, grid_w): 
        self.num_patches = num_patches
        pi = torch.arange(self.num_patches, device=self.ifreq.device)
        self.register_buffer('pih', pi // grid_w)
        self.register_buffer('piw', pi % grid_w)

    def _rotate(self, x, sin, cos):
        x_even, x_odd = x[..., 0::2], x[..., 1::2]
        x_out = torch.empty_like(x)
        x_out[..., 0::2] = x_even * cos - x_odd * sin
        x_out[..., 1::2] = x_odd * cos + x_even * sin
        return x_out

    def forward(self, x, pos=None, nphw=(16,16)):  # x: (batch, heads, num_patches, head_dim)
        num_patches, grid_w, head_dim = x.shape[2], nphw[-1], x.shape[-1]
        if pos is None: 
            if num_patches != self.num_patches: self.calc_pi(num_patches, grid_w)
            freqs_h = torch.outer(self.pih.float(), self.ifreq)[None, None]
            freqs_w = torch.outer(self.piw.float(), self.ifreq)[None, None]
        else:
            pih, piw = pos[:, 0], pos[:, 1] 
            freqs_h = torch.outer(pih.float(), self.ifreq)[None, None]
            freqs_w = torch.outer(piw.float(), self.ifreq)[None, None]        

        sin_h, cos_h = torch.sin(freqs_h), torch.cos(freqs_h)
        sin_w, cos_w = torch.sin(freqs_w), torch.cos(freqs_w)
        
        x_h, x_w = x[..., :head_dim//2], x[..., head_dim//2:]
        x_h_out = self._rotate(x_h, sin_h, cos_h)
        x_w_out = self._rotate(x_w, sin_w, cos_w)
        
        return torch.cat([x_h_out, x_w_out], dim=-1)

# %% ../nbs/02_vit.ipynb #b5498e36
class Attention(nn.Module):
    def __init__(self, dim, heads, dim_qkv=None):
        super().__init__()
        if dim_qkv is None: dim_qkv = dim
        self.heads, self.dim_qkv  = heads, dim_qkv
        self.head_dim = dim_qkv // heads
        self.rope = RoPE2D(self.head_dim)
        self.qkv = nn.Linear(dim, dim_qkv * 3)
        self.proj = nn.Linear(dim_qkv, dim)
        
    def forward(self, x, pos=None):  # x: (batch, num_patches, dim)
        B, N = x.shape[:2]
        # Project and split into q, k, v
        qkv = self.qkv(x)  # (B, N, dim_qkv * 3)
        qkv = qkv.reshape(B, N, 3, self.heads, self.head_dim)  # split into 3, heads, head_dim
        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)
        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, heads, N, head_dim)
        q, k = self.rope(q, pos=pos), self.rope(k, pos=pos)

        out = F.scaled_dot_product_attention(q, k, v)  # (B, heads, N, head_dim), "flash attention"
        out = out.transpose(1, 2).reshape(B, N, self.dim_qkv)  # Merge heads
        return self.proj(out)  # project back

# %% ../nbs/02_vit.ipynb #8b6e260d
class TransformerBlock(nn.Module):
    def __init__(self, dim, heads, dim_qkv=None, hdim=None):
        super().__init__()
        self.attn = Attention(dim, heads, dim_qkv)
        if hdim is None: hdim = 4*dim
        self.lin1, self.lin2 = nn.Linear(dim, hdim), nn.Linear(hdim, dim)
        self.norm1, self.norm2 = nn.LayerNorm(dim), nn.LayerNorm(dim)
        self.act = nn.GELU() 
        
    def forward(self, x, pos=None):  # x: (batch, num_patches, dim)
        x = x + self.attn(self.norm1(x), pos=pos)   # "pre-norm"
        x = x + self.lin2(self.act(self.lin1(self.norm2(x))))  
        return x  # (batch, num_patches, dim)

# %% ../nbs/02_vit.ipynb #a164c279
class PatchEmbedding(nn.Module):
    def __init__(self, 
                in_channels=1,  # 1 for solo piano, for midi PR's, = # of instruments
                patch_size=16,  # assuming square patches, e.g. 16 implies 16x16
                dim=768):       # embedding dimension
        super().__init__()
        self.conv = nn.Conv2d(in_channels, dim, kernel_size=patch_size, stride=patch_size)
        
    def forward(self, x):  # x: (batch, channels, height, width)
        assert all(s % self.conv.kernel_size[0] == 0 for s in x.shape[-2:]), \
            f"Image size {x.shape[-2:]} must be divisible by patch_size {self.conv.kernel_size[0]}"
        conv_patches = self.conv(x).flatten(2).permute(0,2,1)
        # Check if each patch region in the image is empty
        k = self.conv.kernel_size[0]
        patches = x.unfold(2, k, k).unfold(3, k, k)  # extract patches
        non_empty = (patches.amax(dim=(-1,-2)) > 0.2).squeeze(1).flatten(1) # (B, num_patches), 0=empty, 1=not

        # save patch position "coordinates" (for masking later)
        H, W = x.shape[-2] // k, x.shape[-1] // k  # grid dimensions
        rows = torch.arange(H, device=x.device).repeat_interleave(W)
        cols = torch.arange(W, device=x.device).repeat(H)
        pos = torch.stack([rows, cols], dim=-1)  # (num_patches, 2)
        return conv_patches, non_empty, pos  


# %% ../nbs/02_vit.ipynb #2fab57b5
def make_mae_mask(non_empty, ratio=0, has_cls_token=True):
    "Apply token masking for MAE training. 1=keep, 0=masked"
    if ratio < 1e-8: return torch.ones(non_empty.shape[1], device=non_empty.device, dtype=torch.bool)
    mae_mask = (torch.rand(non_empty.shape[1], device=non_empty.device) > ratio)  #  (B, N), 1=visible, 0=masked
    #mae_mask = non_empty[0] & mae_mask # temp check to recover old behavior
    if has_cls_token: mae_mask[0] = True  
    return mae_mask 

# %% ../nbs/02_vit.ipynb #85f5423d
def apply_mae_mask(x, pos, non_empty, mae_mask):
    "Apply token masking for MAE training. 1=keep, 0=masked"
    return x[:, mae_mask, :], pos[mae_mask, :], non_empty[:,mae_mask]

# %% ../nbs/02_vit.ipynb #32916a79
class ViTEncoder(nn.Module):
    """Vision Transformer Encoder for piano rolls, keeps track of empty patches (non_empty) and supports masking"""
    def __init__(self, 
                in_channels,    # 1 for grayscale
                image_size,     # tuple (H,W), e.g. (256, 256)
                patch_size,     # assuming square patches, e.g 16
                dim,            # embedding dim, e.g. 768
                depth,          # number of transformerblock layers -- 4? 
                heads,          # number of attention heads - 8? 
                ):
        super().__init__()
        self.patch_embed = PatchEmbedding(in_channels=in_channels,patch_size=patch_size, dim=dim)
        self.blocks = nn.ModuleList([ TransformerBlock(dim, heads) for _ in range(depth) ])
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
    def forward(self, x, mask_ratio=0.0, mae_mask=None):  # mr=0 means masking is turned off for now
        x, non_empty, pos = self.patch_embed(x)      # x is now patches, non_empty=1 for non-empty patches, 0 for empty

        # tack on cls token 
        cls = self.cls_token.expand(x.shape[0], -1, -1) 
        x = torch.cat([cls, x], dim=1)
        cls_pos = torch.tensor([[-1, -1]], device=pos.device)
        pos = torch.cat([cls_pos, pos], dim=0)  # (num_patches+1, 2)
        non_empty = torch.cat([non_empty.new_ones(non_empty.shape[0], 1), non_empty], dim=1)  # (B, 65)
        if mae_mask is None: mae_mask = make_mae_mask(non_empty, ratio=mask_ratio)
        x, pos_visible, non_empty_visible = apply_mae_mask(x, pos, non_empty, mae_mask)

        for block in self.blocks:  
            x = block(x, pos=pos_visible) 
            x = torch.where(non_empty_visible.unsqueeze(-1), x, x * 1e-3)  # empty patches go to small but nonzero #s

        #return (x[:, 0] if return_cls_only else x), non_empty, pos, mae_mask  # return full pos & non_empty, can calc *_visible outside w/ mae_mask
        cls_ps = PatchState(emb=x[:, 0:1], pos=pos[0:1], non_empty=non_empty[:, 0:1], mae_mask=mae_mask[0:1])
        patch_ps = PatchState(emb=x[:, 1:], pos=pos_visible[1:], non_empty=non_empty_visible[:, 1:], mae_mask=mae_mask[1:])
        return EncoderOutput(patches=HierarchicalPatchState(levels=[cls_ps, patch_ps]), full_pos=pos, full_non_empty=non_empty, mae_mask=mae_mask)

# %% ../nbs/02_vit.ipynb #9c00b8ba
class Unpatchify(nn.Module):
    "Take patches and assemble an image"
    def __init__(self, 
                out_channels=1,  # 1 for solo piano, for midi PR's, = # of instruments
                image_size = (128, 128),  # h,w for output image  
                patch_size=16,  # assuming square patches, e.g. 16 implies 16x16
                dim=768):       # embedding dimension
        super().__init__()
        self.image_size, self.patch_size, self.out_channels = image_size, patch_size, out_channels
        self.npatches_x, self.npatches_y = image_size[0]//patch_size, image_size[1]//patch_size 
        self.lin = nn.Linear(dim, out_channels * patch_size * patch_size )  # (B, 64, 768) -> (B, 64, 256) 
        
    def forward(self, z):  # z: patch embeddings (batch, num_patches, dim)
        out = self.lin(z)  # B, N, D 
        out = out.reshape(-1, self.npatches_x, self.npatches_y, self.patch_size, self.patch_size, self.out_channels)
        out = out.permute(0, 5, 1, 3, 2, 4)  # (B, 1, 8, 16, 8, 16)
        out = out.reshape(-1, self.out_channels, self.image_size[0], self.image_size[1])        
        return out # (B, out_channels, H, W) 

# %% ../nbs/02_vit.ipynb #7a7b630a
class ViTDecoder(nn.Module):
    """Vision Transformer Decoder for piano rolls"""
    def __init__(self, 
                out_channels,  # 
                image_size,   # tuple (H,W), e.g. (256, 256)
                patch_size,   # assuming square patches, e.g 16
                dim,          # embedding dim, e.g. 768
                depth=4,        # number of transformerblock layers -- 4? 
                heads=8):       # number of attention heads - 8? 
        super().__init__()
        self.blocks = nn.ModuleList([ TransformerBlock(dim, heads) for _ in range(depth) ])
        self.unpatch = Unpatchify(out_channels, image_size, patch_size, dim)
        
    def forward(self, z, strip_cls_token=True):
        for block in self.blocks:  z = block(z)
        if strip_cls_token: z = z[:,1:] 
        img = self.unpatch(z) 
        return img

# %% ../nbs/02_vit.ipynb #233cd8a4
class LightweightMAEDecoder(nn.Module):
    """Simple decoder for MAE pretraining - reconstructs masked patches
     loss should compare `output[:, ~mae_mask]` against original masked patch pixels.
    """
    def __init__(self, patch_size=16, dim=256, depth=6, heads=4):
        super().__init__()
        self.patch_size = patch_size
        self.mask_token = nn.Parameter(torch.randn(1, 1, dim))
        self.blocks = nn.ModuleList([TransformerBlock(dim, heads) for _ in range(depth)])
        self.proj = nn.Linear(dim, patch_size * patch_size)  # output pixels per patch
        
    def forward(self, enc_out):
        z = enc_out.patches.levels[-1].emb  # (B, N_vis, dim) finest level
        pos_full = enc_out.full_pos          # (N, 2)
        mae_mask = enc_out.mae_mask          # (B, N) or (N,)
        B, N_full = z.shape[0], pos_full.shape[0]
        z_full = self.mask_token.expand(B, N_full, -1).clone()
        if z.ndim < 3: z = z.reshape(B, -1, z.shape[-1])          #  # (B*N_vis, dim) -> (B, N_vis, dim)
        z_full[:, mae_mask, :] = z  # insert visible tokens
        for block in self.blocks: z_full = block(z_full, pos=pos_full)
        return self.proj(z_full)  # (B, N_full, patch_size^2)
