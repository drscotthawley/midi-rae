"""Pre-encode images using frozen encoder for faster decoder training"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_preencode.ipynb.

# %% auto #0
__all__ = ['preencode']

# %% ../nbs/08_preencode.ipynb #2b6f3731
import os
import torch
from torch.utils.data import DataLoader
from omegaconf import DictConfig, OmegaConf
import hydra
from tqdm.auto import tqdm
from pathlib import Path

from .vit import ViTEncoder
from .data import PRPairDataset  # we'll use use img2 and ignore img1

# %% ../nbs/08_preencode.ipynb #8e40339b
@hydra.main(version_base=None, config_path="../configs", config_name="config")
def preencode(cfg: DictConfig):
    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
    print(f"device = {device}")
    
    # Load encoder from checkpoint
    ckpt_path = cfg.get('encoder_ckpt', 'checkpoints/enc_best.pt')
    print(f"Loading encoder from {ckpt_path}")
    
    model = ViTEncoder(
        cfg.data.in_channels, 
        (cfg.data.image_size, cfg.data.image_size), 
        cfg.model.patch_size,
        cfg.model.dim, 
        cfg.model.depth, 
        cfg.model.heads
    ).to(device)
    
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
    state_dict = {k.replace('_orig_mod.', ''): v for k, v in ckpt['model_state_dict'].items()}
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    
    # Output directory
    output_dir = Path(cfg.get('preencode', {}).get('output_dir', 'preencoded/'))
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Saving embeddings to {output_dir}")
    
    for split in ['train', 'val']:
        print(f"\nProcessing {split} split...")
        ds = PRPairDataset(split=split, max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y)
        dl = DataLoader(ds, batch_size=cfg.training.batch_size, num_workers=4, shuffle=False)
        
        num_chunks = cfg.preencode.num_passes # chunk = 1 pass thru ds
        for chunk in range(1,num_chunks+1):
            chunk_embeddings = []
            chunk_images = []  # optionally save original images too for reconstruction comparison
            with torch.no_grad():
                for batch in tqdm(dl, desc=f"Encoding {split}, Chunk {chunk}/{num_chunks}"):
                    img = batch['img2'].to(device)  # img2 come from wider distribution than img1, ignore img1
                    z = model(img, return_cls_only=False)  # (B, 65, 768)
                    chunk_embeddings.append(z.cpu())
                    chunk_images.append(img.cpu())
            
            # Concatenate and save
            embeddings = torch.cat(chunk_embeddings, dim=0)
            images = torch.cat(chunk_images, dim=0)
            
            save_path = output_dir / f"{split}_embeddings_{chunk}.pt"
            torch.save({
                'embeddings': embeddings,
                'images': images,  # for reconstruction loss computation
            }, save_path)
            print(f"Saved {len(embeddings)} embeddings to {save_path}")
            print(f"  embeddings shape: {embeddings.shape}")
            print(f"  images shape: {images.shape}")


# %% ../nbs/08_preencode.ipynb #c159f875
#| eval: false
if __name__ == "__main__" and "ipykernel" not in __import__("sys").modules:
    preencode()
