# losses


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Safe Mean

Turns out zero element tensors will yield NaN when you try to run
`.mean()`, so…

------------------------------------------------------------------------

### safe_mean

``` python

def safe_mean(
    t, dim:NoneType=None
):

```

*safe replacement for torch.mean( ). can’t be used as a suffix though*

## LeJEPA Loss

For an interactive overview of LeJEPA, see
https://www.scotthawley.com/ssltoy/

------------------------------------------------------------------------

### SIGReg

``` python

def SIGReg(
    x, global_step, num_slices:int=256
):

```

*SIGReg with Epps-Pulley statistic. x is (N, K) tensor.*

``` python
# Test SIGReg with random embeddings
batch_size, embed_dim = 32, 64
x = torch.randn(batch_size, embed_dim)
loss = SIGReg(x, global_step=0, num_slices=256)
print(f"SIGReg loss: {loss.item():.4f}")
```

    SIGReg loss: 2.9035

------------------------------------------------------------------------

### attraction_loss

``` python

def attraction_loss(
    z1, z2, # embeddings of two "views" of the same thing (in batches)
    deltas:NoneType=None, # optional/TBD: info on semantic 'distance' between z1 & z2
    tau:float=100.0, # inverse strength of fall-off for delta distances, big=slower
):

```

*How we pull similar ‘views’ together*

------------------------------------------------------------------------

### LeJEPA

``` python

def LeJEPA(
    z1, z2, global_step, lambd:float=0.5, deltas:NoneType=None
):

```

*Main LeJEPA loss function*

``` python
# Test LeJEPA loss
batch_size, embed_dim = 32, 64
z1 = torch.randn(batch_size, embed_dim)
z2 = torch.randn(batch_size, embed_dim)

loss = LeJEPA(z1, z2, global_step=0, lambd=0.5)
print(f"LeJEPA loss: {loss['loss'].item():.4f}")
print(f"  Attraction: {attraction_loss(z1, z2).item():.4f}")
print(f"  SIGReg: {SIGReg(torch.cat((z1, z2), dim=0), global_step=0).item():.4f}")
```

    LeJEPA loss: 1.8238
      Attraction: 2.0526
      SIGReg: 1.5950

## Masked (Auto)Encoder Loss

------------------------------------------------------------------------

### calc_mae_loss

``` python

def calc_mae_loss(
    recon_patches, img, enc_out, lambda_visible:float=0.1
):

```

*BCE loss on reconstructed patches, with optional downweighting of
visible patches*

## Full Encoder Loss

------------------------------------------------------------------------

### anchor_loss

``` python

def anchor_loss(
    z1, z2
):

```

*Anchor embeddings of empty patches to the origin*

------------------------------------------------------------------------

### calc_enc_loss

``` python

def calc_enc_loss(
    z1, z2, global_step, deltas:NoneType=None, lambd:float=0.5, non_emptys:tuple=(None, None),
    lambda_anchor:float=0.05
):

```

*Main loss function for Encoder*

------------------------------------------------------------------------

### calc_enc_loss_multiscale

``` python

def calc_enc_loss_multiscale(
    z1, z2, # lists of embeddings at each swin hierarchy level, 0=coarsest, -1=finest
    global_step, img_size, deltas:NoneType=None, lambd:float=0.5, non_emptys:NoneType=None, lambda_anchor:float=0.05
):

```

*Compute encoder loss at each hierarchy level, weighted by patch overlap
fraction.* Levels where delta/shift exceeds patch size (resulting in
zero overlap) are skipped entirely. Non-empty patch masks focus the
sim/anchor losses on musically active regions.

## Adversarial Loss

------------------------------------------------------------------------

### PatchGANDiscriminator

``` python

def PatchGANDiscriminator(
    in_ch:int=1, base_ch:int=64, n_layers:int=3, use_spectral_norm:bool=True
):

```

*Base class for all neural network modules.*

Your models should also subclass this class.

Modules can also contain other Modules, allowing them to be nested in a
tree structure. You can assign the submodules as regular attributes::

    import torch.nn as nn
    import torch.nn.functional as F

    class Model(nn.Module):
        def __init__(self) -> None:
            super().__init__()
            self.conv1 = nn.Conv2d(1, 20, 5)
            self.conv2 = nn.Conv2d(20, 20, 5)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            return F.relu(self.conv2(x))

Submodules assigned in this way will be registered, and will also have
their parameters converted when you call :meth:`to`, etc.

.. note:: As per the example above, an `__init__()` call to the parent
class must be made before assignment on the child.

:ivar training: Boolean represents whether this module is in training or
evaluation mode. :vartype training: bool
