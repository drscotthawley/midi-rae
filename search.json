[
  {
    "objectID": "swin.html",
    "href": "swin.html",
    "title": "swin",
    "section": "",
    "text": "We’re not doing mere image segmentation. The representations at different scales may be very different from one another and mapped non-linearly. Consider: the top-level representation could be something about musical genre. The next level down could be something about what part of the song we’re in. The next level down could be which part of the verse or chorus we’re in. The next level could be a given musical phrase. The next level could be the individual notes in the phrase.\n\n\n\nViT: The default ViT we’ve been using has 65 patches with 256 dimensions each: 65 * 256 = 16,640 encoding parameters.\nFor the a Swin with 6 stages, embed_dim=8, patch_h=patch_w=4, so the finest grid is 32×32:\n\n\n\nLevel\nPatch Size (px)\nGrid\nDim\nScalars\n\n\n\n\n0 (coarsest)\n128x128\n1×1\n256\n256\n\n\n1\n64x64\n2×2\n128\n512\n\n\n2\n32x32\n4×4\n64\n1,024\n\n\n3\n16x16\n8×8\n32\n2,048\n\n\n4\n8x8\n16×16\n16\n4,096\n\n\n5 (finest)\n4x4\n32x32\n8\n8,192\n\n\nTotal\n\n\n16,128 encoding parameters\n\n\n\n\n…So the Swin has basically the same amount of information as the ViT (actually slightly less!), it’s just organized differently.\n\n\n\nSwinEncoder is a drop-in replacement for ViTEncoder that uses the Swin Transformer V2 architecture. It takes a piano roll image (B, 1, 128, 128) and returns an EncoderOutput with hierarchical multi-scale patch states.\n\n\n\n\nHierarchical representation: 7 levels from finest (64×64 grid, dim=4) down to a single CLS-like token (1×1, dim=256), compared to ViT’s flat single-scale output\nEfficient attention: Windowed attention with shifted windows — O(N) instead of O(N²)\nV2 improvements: Cosine attention with learned log-scale temperature, continuous position bias via CPB MLP, res-post-norm for training stability\n\n\n\n\n\n\n\nStage\nGrid\nPatch covers\nDim\nDepths\nHeads\n\n\n\n\n0\n64×64\n2×2\n4\n1\n1\n\n\n1\n32×32\n4×4\n8\n1\n1\n\n\n2\n16×16\n8×8\n16\n2\n1\n\n\n3\n8×8\n16×16\n32\n2\n2\n\n\n4\n4×4\n32×32\n64\n6\n4\n\n\n5\n2×2\n64×64\n128\n2\n8\n\n\n6\n1×1\n128×128\n256\n1\n16\n\n\n\nConfig is in configs/config_swin.yaml.\n\n\n\nWe use timm’s SwinTransformerV2Stage directly — no copied or modified Swin internals. Our SwinEncoder wrapper handles only:\n\nPatch embedding — Conv2d(1, 4, kernel_size=2, stride=2) + LayerNorm\nEmpty patch detection — patches where all pixels are black get a learnable empty_token\nMAE masking (SimMIM-style) — masked patches get a learnable mask_token, grid stays intact so windowed attention works unmodified. Two-rate sampling: non-empty patches masked at mask_ratio, empty patches at mask_ratio × empty_mask_ratio (default 5%)\nHierarchical output — collects each stage’s output into HierarchicalPatchState (coarsest-first), packaged as EncoderOutput\n\n\n\n\n\nNo CLS token (stage 6’s single 1×1 token serves as a global summary)\nNo RoPE (Swin V2 uses its own continuous position bias)\nMAE masking keeps all tokens (SimMIM-style) — no compute savings but preserves spatial grid\nempty_mask_ratio controls how often trivial-to-reconstruct empty patches are masked\n\n\n\n\n\nHierarchicalPatchState could store window_size per level\nEncoderOutput could store scale metadata (downsample factors per level)\n\n\n\n\n\nInter-stage patch masking: dropout-style masking between encoder stages, tapered ratio per stage (mae_ratio / 2**stage_idx), using a learnable mask token. Forces robust representations at every scale.\nSelf-distillation (DINO/iBOT-style): EMA teacher provides latent targets at all scales, eliminating need for pixel-level reconstruction at coarser levels.\nMulti-scale reconstruction losses: reconstruct downsampled piano rolls at each hierarchy level (requires bidirectional decoder, e.g. U-Net with skip connections).\n\n\n\n\n\n\ndef SwinEncoder(\n    img_height:int, # Input image height in pixels (e.g. 128)\n    img_width:int, # Input image width in pixels (e.g. 128)\n    patch_h:int=4, # Patch height for initial embedding\n    patch_w:int=4, # Patch width for initial embedding\n    in_chans:int=1, # Number of input channels (1 for piano roll)\n    embed_dim:int=8, # Base embedding dimension (doubles each stage)\n    depths:tuple=(1, 2, 2, 6, 2, 1), # Number of transformer blocks per stage\n    num_heads:tuple=(1, 1, 2, 4, 8, 16), # Attention heads per stage\n    window_size:int=8, # Window size for windowed attention\n    mlp_ratio:float=4.0, # MLP hidden dim = embed_dim * mlp_ratio\n    qkv_bias:bool=True, # Add bias to QKV projections\n    drop_rate:float=0.0, # Dropout after patch embedding\n    proj_drop_rate:float=0.0, # Dropout after attention projection\n    attn_drop_rate:float=0.0, # Dropout on attention weights\n    drop_path_rate:float=0.1, # Stochastic depth rate\n    norm_layer:type=LayerNorm, # Normalization layer class\n    mae_ratio:float=0.0, # Fraction of non-empty patches to mask (0=no masking)\n    empty_mask_ratio:float=0.05, # Mask rate for empty patches relative to mae_ratio\n):\n\nSwin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder. (Wrapper for timm routines)\n\n# Test: verify SwinEncoder output shapes\nB, C, H, W = 2, 1, 128, 128\nenc = SwinEncoder(img_height=H, img_width=W)\nx = torch.randn(B, C, H, W)\nout = enc(x)\n\nprint(f'mae_mask:        {out.mae_mask.shape}')\nprint(f'full_pos:        {out.full_pos.shape}')\nprint(f'full_non_empty:  {out.full_non_empty.shape}')\nprint(f'num levels:      {len(out.patches.levels)}')\nfor i, ps in enumerate(out.patches.levels):\n    g = int(ps.pos.shape[0]**0.5)\n    p = H // g\n    print(f'  level {i}: emb={ps.emb.shape}, pos={ps.pos.shape}  — grid {g}×{g} ({p}×{p} patch{\"es\" if ps.emb.shape[1]&gt;1 else \"\"})')\n\n# Expected hierarchy (coarsest first), 128×128 image, 2×2 patches:\n#   level 0 (coarsest): emb=(1, 1,    256) — grid 1×1  (CLS-like)\n#   level 1:            emb=(1, 4,    128) — grid 2×2\n#   level 2:            emb=(1, 16,    64) — grid 4×4\n#   level 3:            emb=(1, 64,    32) — grid 8×8\n#   level 4:            emb=(1, 256,   16) — grid 16×16\n#   level 5:            emb=(1, 1024,   8) — grid 32×32\n#   level 6 (finest):   emb=(1, 4096,   4) — grid 64×64\n\nmae_mask:        torch.Size([2, 4096])\nfull_pos:        torch.Size([4096, 2])\nfull_non_empty:  torch.Size([2, 4096])\nnum levels:      7\n  level 0: emb=torch.Size([2, 1, 256]), pos=torch.Size([1, 2])  — grid 1×1 (128×128 patch)\n  level 1: emb=torch.Size([2, 4, 128]), pos=torch.Size([4, 2])  — grid 2×2 (64×64 patches)\n  level 2: emb=torch.Size([2, 16, 64]), pos=torch.Size([16, 2])  — grid 4×4 (32×32 patches)\n  level 3: emb=torch.Size([2, 64, 32]), pos=torch.Size([64, 2])  — grid 8×8 (16×16 patches)\n  level 4: emb=torch.Size([2, 256, 16]), pos=torch.Size([256, 2])  — grid 16×16 (8×8 patches)\n  level 5: emb=torch.Size([2, 1024, 8]), pos=torch.Size([1024, 2])  — grid 32×32 (4×4 patches)\n  level 6: emb=torch.Size([2, 4096, 4]), pos=torch.Size([4096, 2])  — grid 64×64 (2×2 patches)\n\n\nTesting code to check for non-empty patches: Green equals non-empty, red equals empty\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom midi_rae.data import PRPairDataset\n\n# Load one image from the dataset\nds = PRPairDataset(split='val')\nimg_tensor = ds[0]['img1'][:1]\nx = img_tensor.unsqueeze(0)\n\n# Run empty patch detection\nenc = SwinEncoder(img_height=128, img_width=128)\nnon_empty = enc._compute_non_empty(x)\nne = non_empty[0].reshape(1, 1, 64, 64).float()\n\n# Build hierarchy via max-pool cascade\nlevels = [ne[0, 0].cpu()]  # 64×64\nwhile levels[-1].shape[0] &gt; 1:\n    ne = F.max_pool2d(ne, 2)\n    levels.append(ne[0, 0].cpu())\n\n# Plot: original image + all levels\nfig, axes = plt.subplots(1, len(levels) + 1, figsize=(16, 2.7))\naxes[0].imshow(img_tensor[0].cpu(), cmap='gray', origin='lower', aspect='auto')\naxes[0].set_title('Original\\n128×128 px')\nfor i, grid in enumerate(levels):\n    g = grid.shape[0]\n    axes[i+1].imshow(grid.numpy(), cmap='RdYlGn', origin='lower', aspect='auto', vmin=0, vmax=1)\n    p = 128 // g\n    axes[i+1].set_title(f'Level {i}\\n{g}×{g} ({p}×{p}px)')\n    axes[i+1].axis('off')\naxes[0].axis('off')\nplt.tight_layout()\nplt.show()\n\nLoading 273 val files from ~/datasets/POP909_images/...\nFinished loading.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef PixelShuffleHead(\n    out_channels:int=1, fpn_dim:int=64, hidden_ch:int=64, patch_size:int=4, grid_h:int=32, grid_w:int=32\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\n\ndef SwinMAEDecoder(\n    patch_size:int=4, dims:tuple=(256, 128, 64, 32, 16, 8), fpn_dim:int=64, depth:int=2, heads:int=4\n):\n\nFPN-style MAE decoder for SwinEncoder hierarchical output. Top-down pathway fuses all levels, reconstructs at finest scale.\n\n\n\n\n\ndef SwinDecoder(\n    img_height:int=128, # Output image height\n    img_width:int=128, # Output image width\n    patch_h:int=4, # Patch height (must match encoder)\n    patch_w:int=4, # Patch width (must match encoder)\n    out_channels:int=1, # Output channels (1 for piano roll)\n    embed_dim:int=8, # Base embedding dim (same as encoder)\n    depths:tuple=(1, 2, 2, 6, 2, 1), # Encoder depths (finest→coarsest); reversed internally\n    num_heads:tuple=(1, 1, 2, 4, 8, 16), # Encoder heads (finest→coarsest); reversed internally\n    window_size:int=8, # Window size for windowed attention\n    mlp_ratio:float=4.0, # MLP hidden dim = dim * mlp_ratio\n    qkv_bias:bool=True, # Bias in QKV projections\n    drop_path_rate:float=0.1, # Stochastic depth rate\n    proj_drop_rate:float=0.0, # Dropout after attention projection\n    attn_drop_rate:float=0.0, # Dropout on attention weights\n    norm_layer:type=LayerNorm\n):\n\nSwin V2 Decoder for midi-rae — symmetric multi-stage decoder.\nMirrors the encoder architecture: processes coarsest→finest with Swin V2 windowed attention at every spatial scale, fusing encoder skip connections via lateral projections at each level.\nPass the same config values (embed_dim, depths, num_heads) as the encoder; they are reversed internally for the coarsest→finest decode direction.\nTakes EncoderOutput directly (same interface as SwinMAEDecoder).\nTODO: Try ConvTranspose2d or PixelShuffle as alternatives to linear unpatchify. TODO: Make the unpatchify head swappable via a factory or argument.\n\n\n\n\n\ndef PatchExpand(\n    in_dim, out_dim, norm_layer:type=LayerNorm\n):\n\nInverse of patch merging: doubles spatial resolution via learned linear expansion. (B, H, W, C_in) → (B, 2H, 2W, C_out)\nTest: verify SwinDecoder output shapes:\n\nB, C, H, W = 2, 1, 128, 128\ndepths, num_heads = (1,2,2,6,2,1), (1,1,2,4,8,16)\nenc = SwinEncoder(img_height=H, img_width=W, patch_h=4, patch_w=4,\n                  embed_dim=8, depths=depths, num_heads=num_heads)\ndec = SwinDecoder(img_height=H, img_width=W, patch_h=4, patch_w=4,\n                  embed_dim=8, depths=depths, num_heads=num_heads)\n\nx = torch.randn(B, C, H, W)\nenc_out = enc(x)\nrecon = dec(enc_out)\n\nprint(f'Input:  {x.shape}')\nprint(f'Output: {recon.shape}')\nassert recon.shape == x.shape, f'Shape mismatch: {recon.shape} != {x.shape}'\nprint('✓ Shapes match!')\n\nenc_params = sum(p.numel() for p in enc.parameters())\ndec_params = sum(p.numel() for p in dec.parameters())\nprint(f'Encoder params: {enc_params:,}')\nprint(f'Decoder params: {dec_params:,}')\n\nInput:  torch.Size([2, 1, 128, 128])\nOutput: torch.Size([2, 1, 128, 128])\n✓ Shapes match!\nEncoder params: 1,748,135\nDecoder params: 1,035,735",
    "crumbs": [
      "swin"
    ]
  },
  {
    "objectID": "swin.html#design-overview",
    "href": "swin.html#design-overview",
    "title": "swin",
    "section": "",
    "text": "We’re not doing mere image segmentation. The representations at different scales may be very different from one another and mapped non-linearly. Consider: the top-level representation could be something about musical genre. The next level down could be something about what part of the song we’re in. The next level down could be which part of the verse or chorus we’re in. The next level could be a given musical phrase. The next level could be the individual notes in the phrase.\n\n\n\nViT: The default ViT we’ve been using has 65 patches with 256 dimensions each: 65 * 256 = 16,640 encoding parameters.\nFor the a Swin with 6 stages, embed_dim=8, patch_h=patch_w=4, so the finest grid is 32×32:\n\n\n\nLevel\nPatch Size (px)\nGrid\nDim\nScalars\n\n\n\n\n0 (coarsest)\n128x128\n1×1\n256\n256\n\n\n1\n64x64\n2×2\n128\n512\n\n\n2\n32x32\n4×4\n64\n1,024\n\n\n3\n16x16\n8×8\n32\n2,048\n\n\n4\n8x8\n16×16\n16\n4,096\n\n\n5 (finest)\n4x4\n32x32\n8\n8,192\n\n\nTotal\n\n\n16,128 encoding parameters\n\n\n\n\n…So the Swin has basically the same amount of information as the ViT (actually slightly less!), it’s just organized differently.\n\n\n\nSwinEncoder is a drop-in replacement for ViTEncoder that uses the Swin Transformer V2 architecture. It takes a piano roll image (B, 1, 128, 128) and returns an EncoderOutput with hierarchical multi-scale patch states.\n\n\n\n\nHierarchical representation: 7 levels from finest (64×64 grid, dim=4) down to a single CLS-like token (1×1, dim=256), compared to ViT’s flat single-scale output\nEfficient attention: Windowed attention with shifted windows — O(N) instead of O(N²)\nV2 improvements: Cosine attention with learned log-scale temperature, continuous position bias via CPB MLP, res-post-norm for training stability\n\n\n\n\n\n\n\nStage\nGrid\nPatch covers\nDim\nDepths\nHeads\n\n\n\n\n0\n64×64\n2×2\n4\n1\n1\n\n\n1\n32×32\n4×4\n8\n1\n1\n\n\n2\n16×16\n8×8\n16\n2\n1\n\n\n3\n8×8\n16×16\n32\n2\n2\n\n\n4\n4×4\n32×32\n64\n6\n4\n\n\n5\n2×2\n64×64\n128\n2\n8\n\n\n6\n1×1\n128×128\n256\n1\n16\n\n\n\nConfig is in configs/config_swin.yaml.\n\n\n\nWe use timm’s SwinTransformerV2Stage directly — no copied or modified Swin internals. Our SwinEncoder wrapper handles only:\n\nPatch embedding — Conv2d(1, 4, kernel_size=2, stride=2) + LayerNorm\nEmpty patch detection — patches where all pixels are black get a learnable empty_token\nMAE masking (SimMIM-style) — masked patches get a learnable mask_token, grid stays intact so windowed attention works unmodified. Two-rate sampling: non-empty patches masked at mask_ratio, empty patches at mask_ratio × empty_mask_ratio (default 5%)\nHierarchical output — collects each stage’s output into HierarchicalPatchState (coarsest-first), packaged as EncoderOutput\n\n\n\n\n\nNo CLS token (stage 6’s single 1×1 token serves as a global summary)\nNo RoPE (Swin V2 uses its own continuous position bias)\nMAE masking keeps all tokens (SimMIM-style) — no compute savings but preserves spatial grid\nempty_mask_ratio controls how often trivial-to-reconstruct empty patches are masked\n\n\n\n\n\nHierarchicalPatchState could store window_size per level\nEncoderOutput could store scale metadata (downsample factors per level)\n\n\n\n\n\nInter-stage patch masking: dropout-style masking between encoder stages, tapered ratio per stage (mae_ratio / 2**stage_idx), using a learnable mask token. Forces robust representations at every scale.\nSelf-distillation (DINO/iBOT-style): EMA teacher provides latent targets at all scales, eliminating need for pixel-level reconstruction at coarser levels.\nMulti-scale reconstruction losses: reconstruct downsampled piano rolls at each hierarchy level (requires bidirectional decoder, e.g. U-Net with skip connections).\n\n\n\n\n\n\ndef SwinEncoder(\n    img_height:int, # Input image height in pixels (e.g. 128)\n    img_width:int, # Input image width in pixels (e.g. 128)\n    patch_h:int=4, # Patch height for initial embedding\n    patch_w:int=4, # Patch width for initial embedding\n    in_chans:int=1, # Number of input channels (1 for piano roll)\n    embed_dim:int=8, # Base embedding dimension (doubles each stage)\n    depths:tuple=(1, 2, 2, 6, 2, 1), # Number of transformer blocks per stage\n    num_heads:tuple=(1, 1, 2, 4, 8, 16), # Attention heads per stage\n    window_size:int=8, # Window size for windowed attention\n    mlp_ratio:float=4.0, # MLP hidden dim = embed_dim * mlp_ratio\n    qkv_bias:bool=True, # Add bias to QKV projections\n    drop_rate:float=0.0, # Dropout after patch embedding\n    proj_drop_rate:float=0.0, # Dropout after attention projection\n    attn_drop_rate:float=0.0, # Dropout on attention weights\n    drop_path_rate:float=0.1, # Stochastic depth rate\n    norm_layer:type=LayerNorm, # Normalization layer class\n    mae_ratio:float=0.0, # Fraction of non-empty patches to mask (0=no masking)\n    empty_mask_ratio:float=0.05, # Mask rate for empty patches relative to mae_ratio\n):\n\nSwin Transformer V2 Encoder for midi-rae — drop-in replacement for ViTEncoder. (Wrapper for timm routines)\n\n# Test: verify SwinEncoder output shapes\nB, C, H, W = 2, 1, 128, 128\nenc = SwinEncoder(img_height=H, img_width=W)\nx = torch.randn(B, C, H, W)\nout = enc(x)\n\nprint(f'mae_mask:        {out.mae_mask.shape}')\nprint(f'full_pos:        {out.full_pos.shape}')\nprint(f'full_non_empty:  {out.full_non_empty.shape}')\nprint(f'num levels:      {len(out.patches.levels)}')\nfor i, ps in enumerate(out.patches.levels):\n    g = int(ps.pos.shape[0]**0.5)\n    p = H // g\n    print(f'  level {i}: emb={ps.emb.shape}, pos={ps.pos.shape}  — grid {g}×{g} ({p}×{p} patch{\"es\" if ps.emb.shape[1]&gt;1 else \"\"})')\n\n# Expected hierarchy (coarsest first), 128×128 image, 2×2 patches:\n#   level 0 (coarsest): emb=(1, 1,    256) — grid 1×1  (CLS-like)\n#   level 1:            emb=(1, 4,    128) — grid 2×2\n#   level 2:            emb=(1, 16,    64) — grid 4×4\n#   level 3:            emb=(1, 64,    32) — grid 8×8\n#   level 4:            emb=(1, 256,   16) — grid 16×16\n#   level 5:            emb=(1, 1024,   8) — grid 32×32\n#   level 6 (finest):   emb=(1, 4096,   4) — grid 64×64\n\nmae_mask:        torch.Size([2, 4096])\nfull_pos:        torch.Size([4096, 2])\nfull_non_empty:  torch.Size([2, 4096])\nnum levels:      7\n  level 0: emb=torch.Size([2, 1, 256]), pos=torch.Size([1, 2])  — grid 1×1 (128×128 patch)\n  level 1: emb=torch.Size([2, 4, 128]), pos=torch.Size([4, 2])  — grid 2×2 (64×64 patches)\n  level 2: emb=torch.Size([2, 16, 64]), pos=torch.Size([16, 2])  — grid 4×4 (32×32 patches)\n  level 3: emb=torch.Size([2, 64, 32]), pos=torch.Size([64, 2])  — grid 8×8 (16×16 patches)\n  level 4: emb=torch.Size([2, 256, 16]), pos=torch.Size([256, 2])  — grid 16×16 (8×8 patches)\n  level 5: emb=torch.Size([2, 1024, 8]), pos=torch.Size([1024, 2])  — grid 32×32 (4×4 patches)\n  level 6: emb=torch.Size([2, 4096, 4]), pos=torch.Size([4096, 2])  — grid 64×64 (2×2 patches)\n\n\nTesting code to check for non-empty patches: Green equals non-empty, red equals empty\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom midi_rae.data import PRPairDataset\n\n# Load one image from the dataset\nds = PRPairDataset(split='val')\nimg_tensor = ds[0]['img1'][:1]\nx = img_tensor.unsqueeze(0)\n\n# Run empty patch detection\nenc = SwinEncoder(img_height=128, img_width=128)\nnon_empty = enc._compute_non_empty(x)\nne = non_empty[0].reshape(1, 1, 64, 64).float()\n\n# Build hierarchy via max-pool cascade\nlevels = [ne[0, 0].cpu()]  # 64×64\nwhile levels[-1].shape[0] &gt; 1:\n    ne = F.max_pool2d(ne, 2)\n    levels.append(ne[0, 0].cpu())\n\n# Plot: original image + all levels\nfig, axes = plt.subplots(1, len(levels) + 1, figsize=(16, 2.7))\naxes[0].imshow(img_tensor[0].cpu(), cmap='gray', origin='lower', aspect='auto')\naxes[0].set_title('Original\\n128×128 px')\nfor i, grid in enumerate(levels):\n    g = grid.shape[0]\n    axes[i+1].imshow(grid.numpy(), cmap='RdYlGn', origin='lower', aspect='auto', vmin=0, vmax=1)\n    p = 128 // g\n    axes[i+1].set_title(f'Level {i}\\n{g}×{g} ({p}×{p}px)')\n    axes[i+1].axis('off')\naxes[0].axis('off')\nplt.tight_layout()\nplt.show()\n\nLoading 273 val files from ~/datasets/POP909_images/...\nFinished loading.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef PixelShuffleHead(\n    out_channels:int=1, fpn_dim:int=64, hidden_ch:int=64, patch_size:int=4, grid_h:int=32, grid_w:int=32\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\n\ndef SwinMAEDecoder(\n    patch_size:int=4, dims:tuple=(256, 128, 64, 32, 16, 8), fpn_dim:int=64, depth:int=2, heads:int=4\n):\n\nFPN-style MAE decoder for SwinEncoder hierarchical output. Top-down pathway fuses all levels, reconstructs at finest scale.\n\n\n\n\n\ndef SwinDecoder(\n    img_height:int=128, # Output image height\n    img_width:int=128, # Output image width\n    patch_h:int=4, # Patch height (must match encoder)\n    patch_w:int=4, # Patch width (must match encoder)\n    out_channels:int=1, # Output channels (1 for piano roll)\n    embed_dim:int=8, # Base embedding dim (same as encoder)\n    depths:tuple=(1, 2, 2, 6, 2, 1), # Encoder depths (finest→coarsest); reversed internally\n    num_heads:tuple=(1, 1, 2, 4, 8, 16), # Encoder heads (finest→coarsest); reversed internally\n    window_size:int=8, # Window size for windowed attention\n    mlp_ratio:float=4.0, # MLP hidden dim = dim * mlp_ratio\n    qkv_bias:bool=True, # Bias in QKV projections\n    drop_path_rate:float=0.1, # Stochastic depth rate\n    proj_drop_rate:float=0.0, # Dropout after attention projection\n    attn_drop_rate:float=0.0, # Dropout on attention weights\n    norm_layer:type=LayerNorm\n):\n\nSwin V2 Decoder for midi-rae — symmetric multi-stage decoder.\nMirrors the encoder architecture: processes coarsest→finest with Swin V2 windowed attention at every spatial scale, fusing encoder skip connections via lateral projections at each level.\nPass the same config values (embed_dim, depths, num_heads) as the encoder; they are reversed internally for the coarsest→finest decode direction.\nTakes EncoderOutput directly (same interface as SwinMAEDecoder).\nTODO: Try ConvTranspose2d or PixelShuffle as alternatives to linear unpatchify. TODO: Make the unpatchify head swappable via a factory or argument.\n\n\n\n\n\ndef PatchExpand(\n    in_dim, out_dim, norm_layer:type=LayerNorm\n):\n\nInverse of patch merging: doubles spatial resolution via learned linear expansion. (B, H, W, C_in) → (B, 2H, 2W, C_out)\nTest: verify SwinDecoder output shapes:\n\nB, C, H, W = 2, 1, 128, 128\ndepths, num_heads = (1,2,2,6,2,1), (1,1,2,4,8,16)\nenc = SwinEncoder(img_height=H, img_width=W, patch_h=4, patch_w=4,\n                  embed_dim=8, depths=depths, num_heads=num_heads)\ndec = SwinDecoder(img_height=H, img_width=W, patch_h=4, patch_w=4,\n                  embed_dim=8, depths=depths, num_heads=num_heads)\n\nx = torch.randn(B, C, H, W)\nenc_out = enc(x)\nrecon = dec(enc_out)\n\nprint(f'Input:  {x.shape}')\nprint(f'Output: {recon.shape}')\nassert recon.shape == x.shape, f'Shape mismatch: {recon.shape} != {x.shape}'\nprint('✓ Shapes match!')\n\nenc_params = sum(p.numel() for p in enc.parameters())\ndec_params = sum(p.numel() for p in dec.parameters())\nprint(f'Encoder params: {enc_params:,}')\nprint(f'Decoder params: {dec_params:,}')\n\nInput:  torch.Size([2, 1, 128, 128])\nOutput: torch.Size([2, 1, 128, 128])\n✓ Shapes match!\nEncoder params: 1,748,135\nDecoder params: 1,035,735",
    "crumbs": [
      "swin"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "PRPairDataset\n\ndef PRPairDataset(\n    image_dataset_dir:str='~/datasets/POP909_images/', crop_size:int=128, max_shift_x:int=10, max_shift_y:int=12,\n    split:str='train', val_fraction:float=0.1, seed:int=42, verbose:bool=True\n):\n\npiano roll pair dataset\nCode to test that:\n\ndata = PRPairDataset()\nprint(\"len(data) =\",len(data))\nprint(\"data.actual_len =\",data.actual_len)\ndata_dict = next(iter(data)) \nprint(\"data_dict =n\",data_dict)\n\n# Let's take the sum to make sure there's some non-zero pixel values\nfor imstr in ['img1','img2']:\n    print(f\"data_dict['{imstr}'].sum() = \",data_dict[imstr].sum())\n\nLoading 818 train files from ~/datasets/POP909_images/...\nFinished loading.\nlen(data) = 81800\ndata.actual_len = 818\ndata_dict =n {'img1': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'img2': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]), 'deltas': tensor([3., 9.]), 'file_idx': 201}\ndata_dict['img1'].sum() =  tensor(337.7961)\ndata_dict['img2'].sum() =  tensor(340.7451)\n\n\n\nimport matplotlib.pyplot as plt\n\nsamples = [data[i] for i in range(5)]\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nprint(\"top row: img1, bottom row: img2\")\nfor i in range(5):\n    sample = samples[i]\n    for j, imstr in enumerate(['img1','img2']): \n        axes[j, i].imshow(data_dict[imstr].squeeze(), cmap='gray') # top row: img1, bottom row: img2\n        axes[j, i].axis('off')\n    axes[1, i].set_title(f\"Δx={sample['deltas'][0]}, Δy={sample['deltas'][1]}\", fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\ntop row: img1, bottom row: img2",
    "crumbs": [
      "data"
    ]
  },
  {
    "objectID": "preencode.html",
    "href": "preencode.html",
    "title": "Pre-encode",
    "section": "",
    "text": "This script pre-encodes images using a trained encoder checkpoint, saving the embeddings for faster decoder training.\nUsage:\npython midi_rae/preencode.py encoder_ckpt=checkpoints/best.pt preencode.output_dir=preencoded/\nTODO: - May need a simpler Dataset that returns single images (not pairs) + their filenames - Decide on output format: one .pt per image, or chunked/batched files? - Add config entries for encoder_ckpt path and preencode.output_dir\n\n\n\n\ndef preencode(\n    cfg:DictConfig\n):",
    "crumbs": [
      "Pre-encode"
    ]
  },
  {
    "objectID": "preencode.html#notes",
    "href": "preencode.html#notes",
    "title": "Pre-encode",
    "section": "",
    "text": "This script pre-encodes images using a trained encoder checkpoint, saving the embeddings for faster decoder training.\nUsage:\npython midi_rae/preencode.py encoder_ckpt=checkpoints/best.pt preencode.output_dir=preencoded/\nTODO: - May need a simpler Dataset that returns single images (not pairs) + their filenames - Decide on output format: one .pt per image, or chunked/batched files? - Add config entries for encoder_ckpt path and preencode.output_dir\n\n\n\n\ndef preencode(\n    cfg:DictConfig\n):",
    "crumbs": [
      "Pre-encode"
    ]
  },
  {
    "objectID": "vit.html",
    "href": "vit.html",
    "title": "ViT",
    "section": "",
    "text": "used in both encoder and decoder.\n\n\n\n\ndef RoPE2D(\n    head_dim\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing\nhead_dim = 768//4\nx = torch.rand((2, 8, 256, head_dim))\nrope = RoPE2D(head_dim)\nrot_x = rope(x) \nprint(\"rot_x.shape = \",rot_x.shape)\n\nrot_x.shape =  torch.Size([2, 8, 256, 192])\n\n\n\n\n\n\n\ndef Attention(\n    dim, heads, dim_qkv:NoneType=None\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing \nx = torch.rand(2, 256, 768)\nattn = Attention(768, 8) \na = attn(x) \nprint(\"Done: a.shape = \",a.shape)\n\nattn2 = Attention(768, 8, 64) \na2 = attn2(x) \nprint(\"Done: a2.shape = \",a2.shape)\n\nDone: a.shape =  torch.Size([2, 256, 768])\nDone: a2.shape =  torch.Size([2, 256, 768])\n\n\n\n\n\n\n\ndef TransformerBlock(\n    dim, heads, dim_qkv:NoneType=None, hdim:NoneType=None\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing\nx = torch.randn(2, 256, 768) \ntrans = TransformerBlock(768, 8) \nout = trans(x) \nprint(\"out.shape = \",out.shape)\n\nout.shape =  torch.Size([2, 256, 768])",
    "crumbs": [
      "ViT"
    ]
  },
  {
    "objectID": "vit.html#vit-components",
    "href": "vit.html#vit-components",
    "title": "ViT",
    "section": "",
    "text": "used in both encoder and decoder.\n\n\n\n\ndef RoPE2D(\n    head_dim\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing\nhead_dim = 768//4\nx = torch.rand((2, 8, 256, head_dim))\nrope = RoPE2D(head_dim)\nrot_x = rope(x) \nprint(\"rot_x.shape = \",rot_x.shape)\n\nrot_x.shape =  torch.Size([2, 8, 256, 192])\n\n\n\n\n\n\n\ndef Attention(\n    dim, heads, dim_qkv:NoneType=None\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing \nx = torch.rand(2, 256, 768)\nattn = Attention(768, 8) \na = attn(x) \nprint(\"Done: a.shape = \",a.shape)\n\nattn2 = Attention(768, 8, 64) \na2 = attn2(x) \nprint(\"Done: a2.shape = \",a2.shape)\n\nDone: a.shape =  torch.Size([2, 256, 768])\nDone: a2.shape =  torch.Size([2, 256, 768])\n\n\n\n\n\n\n\ndef TransformerBlock(\n    dim, heads, dim_qkv:NoneType=None, hdim:NoneType=None\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing\nx = torch.randn(2, 256, 768) \ntrans = TransformerBlock(768, 8) \nout = trans(x) \nprint(\"out.shape = \",out.shape)\n\nout.shape =  torch.Size([2, 256, 768])",
    "crumbs": [
      "ViT"
    ]
  },
  {
    "objectID": "vit.html#encoder",
    "href": "vit.html#encoder",
    "title": "ViT",
    "section": "Encoder",
    "text": "Encoder\nDoes patch embedding and then some transformer blocks.\n\n\nPatchEmbedding\n\ndef PatchEmbedding(\n    in_channels:int=1, # 1 for solo piano, for midi PR's, = # of instruments\n    patch_size:int=16, # assuming square patches, e.g. 16 implies 16x16\n    dim:int=768, # embedding dimension\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n# testing\npe = PatchEmbedding()\nx = torch.randn(2, 1, 256, 256)\nz, non_empty, pos = pe(x) \nprint(\"z.shape, non_empty.shape, pos.shape = \",z.shape, non_empty.shape, pos.shape) \nprint(\"pos = \\n\",pos.tolist())\n\nz.shape, non_empty.shape, pos.shape =  torch.Size([2, 256, 768]) torch.Size([2, 256]) torch.Size([256, 2])\npos = \n [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11], [0, 12], [0, 13], [0, 14], [0, 15], [1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [1, 13], [1, 14], [1, 15], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11], [2, 12], [2, 13], [2, 14], [2, 15], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [3, 10], [3, 11], [3, 12], [3, 13], [3, 14], [3, 15], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [4, 10], [4, 11], [4, 12], [4, 13], [4, 14], [4, 15], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9], [5, 10], [5, 11], [5, 12], [5, 13], [5, 14], [5, 15], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 6], [6, 7], [6, 8], [6, 9], [6, 10], [6, 11], [6, 12], [6, 13], [6, 14], [6, 15], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [7, 6], [7, 7], [7, 8], [7, 9], [7, 10], [7, 11], [7, 12], [7, 13], [7, 14], [7, 15], [8, 0], [8, 1], [8, 2], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7], [8, 8], [8, 9], [8, 10], [8, 11], [8, 12], [8, 13], [8, 14], [8, 15], [9, 0], [9, 1], [9, 2], [9, 3], [9, 4], [9, 5], [9, 6], [9, 7], [9, 8], [9, 9], [9, 10], [9, 11], [9, 12], [9, 13], [9, 14], [9, 15], [10, 0], [10, 1], [10, 2], [10, 3], [10, 4], [10, 5], [10, 6], [10, 7], [10, 8], [10, 9], [10, 10], [10, 11], [10, 12], [10, 13], [10, 14], [10, 15], [11, 0], [11, 1], [11, 2], [11, 3], [11, 4], [11, 5], [11, 6], [11, 7], [11, 8], [11, 9], [11, 10], [11, 11], [11, 12], [11, 13], [11, 14], [11, 15], [12, 0], [12, 1], [12, 2], [12, 3], [12, 4], [12, 5], [12, 6], [12, 7], [12, 8], [12, 9], [12, 10], [12, 11], [12, 12], [12, 13], [12, 14], [12, 15], [13, 0], [13, 1], [13, 2], [13, 3], [13, 4], [13, 5], [13, 6], [13, 7], [13, 8], [13, 9], [13, 10], [13, 11], [13, 12], [13, 13], [13, 14], [13, 15], [14, 0], [14, 1], [14, 2], [14, 3], [14, 4], [14, 5], [14, 6], [14, 7], [14, 8], [14, 9], [14, 10], [14, 11], [14, 12], [14, 13], [14, 14], [14, 15], [15, 0], [15, 1], [15, 2], [15, 3], [15, 4], [15, 5], [15, 6], [15, 7], [15, 8], [15, 9], [15, 10], [15, 11], [15, 12], [15, 13], [15, 14], [15, 15]]\n\n\n\n\n\nmake_mae_mask\n\ndef make_mae_mask(\n    non_empty, ratio:int=0, has_cls_token:bool=True\n):\n\nApply token masking for MAE training. 1=keep, 0=masked\n\n\n\napply_mae_mask\n\ndef apply_mae_mask(\n    x, pos, non_empty, mae_mask\n):\n\nApply token masking for MAE training. 1=keep, 0=masked\n\n\n\nViTEncoder\n\ndef ViTEncoder(\n    in_channels, # 1 for grayscale\n    image_size, # tuple (H,W), e.g. (256, 256)\n    patch_size, # assuming square patches, e.g 16\n    dim, # embedding dim, e.g. 768\n    depth, # number of transformerblock layers -- 4?\n    heads, # number of attention heads - 8?\n):\n\nVision Transformer Encoder for piano rolls, keeps track of empty patches (non_empty) and supports masking\n\nB, C, H, W = 4, 1, 128, 128\npatch_size, dim, depth, heads = 16, 256, 2, 4 \nx = torch.randn(B,C,H,W) \nencoder = ViTEncoder( C, (H,W), patch_size, dim, depth, heads) \nenc_out = encoder(x) \nprint(\"CLS shape:\", enc_out.patches[0].emb.shape)\nprint(\"patch shape:\", enc_out.patches[1].emb[0].shape)\n\nCLS shape: torch.Size([4, 1, 256])\npatch shape: torch.Size([64, 256])",
    "crumbs": [
      "ViT"
    ]
  },
  {
    "objectID": "vit.html#decoder",
    "href": "vit.html#decoder",
    "title": "ViT",
    "section": "Decoder",
    "text": "Decoder\nLike the Encoder, only instead of doing “PatchEmbedding” on the front end “UnPatchify”\n\n\nUnpatchify\n\ndef Unpatchify(\n    out_channels:int=1, # 1 for solo piano, for midi PR's, = # of instruments\n    image_size:tuple=(128, 128), # h,w for output image\n    patch_size:int=16, # assuming square patches, e.g. 16 implies 16x16\n    dim:int=768, # embedding dimension\n):\n\nTake patches and assemble an image\n\nz = torch.randn([3, 64, dim]) \nunpatch = Unpatchify(dim=dim) # keep the defaults\nimg = unpatch(z) \nprint(\"img.shape = \",img.shape)\n\nimg.shape =  torch.Size([3, 1, 128, 128])\n\n\n\n\n\nViTDecoder\n\ndef ViTDecoder(\n    out_channels, image_size, # tuple (H,W), e.g. (256, 256)\n    patch_size, # assuming square patches, e.g 16\n    dim, # embedding dim, e.g. 768\n    depth:int=4, # number of transformerblock layers -- 4?\n    heads:int=8, # number of attention heads - 8?\n):\n\nVision Transformer Decoder for piano rolls\n\nz = torch.randn(3, 65, dim)  # batch of 3, with CLS token\ndecoder = ViTDecoder(out_channels=1, image_size=(H,W), patch_size=16, dim=dim)\nimg = decoder(z)\nprint(img.shape)\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[19], line 4\n      2 z = torch.randn(3, 65, dim)  # batch of 3, with CLS token\n      3 decoder = ViTDecoder(out_channels=1, image_size=(H,W), patch_size=16, dim=dim)\n----&gt; 4 img = decoder(z)\n      5 print(img.shape)\n\nFile ~/envs/midi-rae/lib/python3.10/site-packages/torch/nn/modules/module.py:1776, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1774     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1775 else:\n-&gt; 1776     return self._call_impl(*args, **kwargs)\n\nFile ~/envs/midi-rae/lib/python3.10/site-packages/torch/nn/modules/module.py:1787, in Module._call_impl(self, *args, **kwargs)\n   1782 # If we don't have any hooks, we want to skip the rest of the logic in\n   1783 # this function, and just call forward.\n   1784 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1785         or _global_backward_pre_hooks or _global_backward_hooks\n   1786         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1787     return forward_call(*args, **kwargs)\n   1789 result = None\n   1790 called_always_called_hooks = set()\n\nCell In[18], line 16, in ViTDecoder.forward(self, enc_out, strip_cls_token)\n     15 def forward(self, enc_out, strip_cls_token=True):\n---&gt; 16     z = enc_out.patches.all_emb          # (B, 1+N, dim) — CLS + patches\n     17     for block in self.blocks:  z = block(z)\n     18     if strip_cls_token: z = z[:,1:] \n\nAttributeError: 'Tensor' object has no attribute 'patches'\n\n\n\n\n\n\nLightweightMAEDecoder\n\ndef LightweightMAEDecoder(\n    patch_size:int=16, dim:int=256, depth:int=6, heads:int=4\n):\n\nSimple decoder for MAE pretraining - reconstructs masked patches loss should compare output[:, ~mae_mask] against original masked patch pixels.",
    "crumbs": [
      "ViT"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "print(logo)\n\n\n          ▬▬    ▬▬▬    ▬▬                                       ▬▬                                  \"Scrawl Me Maybe\"   \n                 ▬▬                                                                                                     \n ▬▬  ▬▬  ▬▬▬     ▬▬   ▬▬▬        ▬▬▬▬▬▬ ▬▬▬▬   ▬▬▬▬            ▬▬▬   ▬▬▬▬ ▬▬▬▬▬▬  ▬▬▬▬           ▬▬▬▬▬   ▬▬▬▬  ▬▬▬▬▬    \n ▬▬▬▬▬▬▬  ▬▬  ▬▬▬▬▬    ▬▬   ▬▬▬▬  ▬▬  ▬▬   ▬▬ ▬▬  ▬▬   ▬▬▬▬     ▬▬  ▬▬  ▬▬ ▬▬  ▬▬    ▬▬   ▬▬▬▬  ▬▬      ▬▬  ▬▬ ▬▬  ▬▬ \n ▬▬ ▬ ▬▬  ▬▬ ▬▬  ▬▬    ▬▬         ▬▬    ▬▬▬▬▬ ▬▬▬▬▬▬            ▬▬  ▬▬▬▬▬▬ ▬▬  ▬▬ ▬▬▬▬▬          ▬▬▬▬▬  ▬▬  ▬▬ ▬▬  ▬▬   \n ▬▬   ▬▬  ▬▬ ▬▬  ▬▬    ▬▬         ▬▬   ▬▬  ▬▬ ▬▬            ▬▬  ▬▬  ▬▬     ▬▬▬▬▬ ▬▬  ▬▬              ▬▬ ▬▬  ▬▬ ▬▬  ▬▬ \n ▬▬   ▬▬ ▬▬▬▬ ▬▬▬▬▬▬  ▬▬▬▬       ▬▬▬▬   ▬▬▬▬▬  ▬▬▬▬▬        ▬▬  ▬▬   ▬▬▬▬▬ ▬▬     ▬▬▬▬▬         ▬▬▬▬▬▬   ▬▬▬▬  ▬▬  ▬▬ \n                                                             ▬▬▬▬         ▬▬▬▬",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#overview",
    "href": "core.html#overview",
    "title": "core",
    "section": "Overview",
    "text": "Overview\nThese dataclasses bundle patch embeddings with their spatial and mask metadata, replacing scattered positional return values and manual mask indexing throughout the codebase.\n\nPatchState\nHolds a set of patch embeddings at a single spatial scale, along with their grid positions and masks. Provides convenience properties for common operations like filtering visible patches.\n\n\nHierarchicalPatchState\nA list of PatchState objects ordered coarsest → finest (index 0 = global/CLS level). Currently used with two levels (CLS + patches), designed to extend to Swin-style multi-scale later.\n\n\nEncoderOutput\nFull encoder output bundling the hierarchical patch states with the full (pre-MAE-masking) positions and masks needed by the decoder for reconstruction.\n\n\n\nPatchState\n\ndef PatchState(\n    emb:Tensor, pos:Tensor, non_empty:Tensor, mae_mask:Tensor\n)-&gt;None:\n\nBundle of patch embeddings at a single spatial scale with their metadata.\nAttributes: emb: (B, N, dim) patch embeddings pos: (N, 2) grid coordinates (row, col) for each patch non_empty: (B, N) content mask — 1 where patch has content (e.g. notes), 0 for empty mae_mask: (N,) MAE visibility mask — 1=visible, 0=masked out for reconstruction\n\n\n\nHierarchicalPatchState\n\ndef HierarchicalPatchState(\n    levels:list\n)-&gt;None:\n\nMulti-scale patch states, ordered coarsest → finest (currently: [0]=CLS, [1]=spatial patches).\nAttributes: levels: List of PatchState, one per scale\nNote: enc_out.patches[i] and enc_out.patches.levels[i] are equivalent\n\n\n\nto\n\ndef to(\n    device\n):\n\n\n\n\nEncoderOutput\n\ndef EncoderOutput(\n    patches:HierarchicalPatchState, full_pos:Tensor, full_non_empty:Tensor, mae_mask:Tensor\n)-&gt;None:\n\nFull encoder output.\nAttributes: patches: Encoded representations (visible patches only) full_pos: (N_full, 2) all grid positions before MAE masking (needed by decoder) full_non_empty: (B, N_full) all content masks before MAE masking mae_mask: (N_full,) the MAE mask applied (1=visible, 0=masked)\n\n\nSample usage\nEncoder returns EncoderOutput containing a HierarchicalPatchState:\nenc_out = encoder(img, mask_ratio=0.5)\n\n# Access the patch hierarchy\ncls_state = enc_out.patches.coarsest    # PatchState with CLS token\npatch_state = enc_out.patches.finest    # PatchState with patch embeddings\nWorking with PatchState — filtering, shapes, masks:\nps = enc_out.patches.finest\n\nps.emb          # (B, N_visible, dim) — patch embeddings\nps.pos          # (N_visible, 2) — grid coordinates (row, col)\nps.non_empty    # (B, N_visible) — content mask (1=has notes)\nps.mae_mask     # (N_visible,) — all True for already-filtered patches\nps.dim          # embedding dimension\nps.num_patches  # number of patches\n\nvis = ps.visible  # new PatchState with only MAE-visible patches\nIn compute_batch_loss (encoder training):\n# BEFORE: 8 positional return values\n# loss_dict, z1, z2, non_emptys, pos2, mae_mask2, num_tokens, recon_patches = ...\n\n# AFTER:\nloss_dict, enc_out1, enc_out2, recon_patches = compute_batch_loss(...)\nnon_emptys = (enc_out1.patches.finest.non_empty, enc_out2.patches.finest.non_empty)\nIn LightweightMAEDecoder:\n# BEFORE:\n# def forward(self, z, pos_full, mae_mask): ...\n\n# AFTER:\n# def forward(self, enc_out: EncoderOutput): ...\n#   — gets visible embeddings, full positions, and mae_mask all from enc_out\nFuture Swin hierarchy (coarsest → finest):\n# levels[0] = global (like CLS), levels[1] = 4x4, levels[2] = 8x8, levels[3] = 16x16\nh = enc_out.patches\nh.coarsest          # global summary\nh.finest            # finest-resolution patches  \nh.levels[1]         # intermediate scale\nh.levels[1].visible # visible patches at that scale",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "train_enc.html",
    "href": "train_enc.html",
    "title": "train_enc",
    "section": "",
    "text": "# #| export\n# def curr_learn(shared_ct_dict, epoch, interval=100, verbose=False): \n#     \"UNUSED/UNNECESSARY: curriculum learning: increase difficulty with epoch\"\n#     if epoch &lt; interval: return shared_ct_dict['training']\n#     training = shared_ct_dict['training']\n#     training['max_shift_x'] = min(12, 6 + epoch // interval)\n#     training['max_shift_y'] = min(12, 6 + epoch // interval)\n#     if verbose: \n#         print(f\"curr_learn: max_shift_x = {training['max_shift_x']}, max_shift_y = {training['max_shift_y']}\")\n#     return training",
    "crumbs": [
      "train_enc"
    ]
  },
  {
    "objectID": "train_enc.html#curriculum-learning",
    "href": "train_enc.html#curriculum-learning",
    "title": "train_enc",
    "section": "",
    "text": "# #| export\n# def curr_learn(shared_ct_dict, epoch, interval=100, verbose=False): \n#     \"UNUSED/UNNECESSARY: curriculum learning: increase difficulty with epoch\"\n#     if epoch &lt; interval: return shared_ct_dict['training']\n#     training = shared_ct_dict['training']\n#     training['max_shift_x'] = min(12, 6 + epoch // interval)\n#     training['max_shift_y'] = min(12, 6 + epoch // interval)\n#     if verbose: \n#         print(f\"curr_learn: max_shift_x = {training['max_shift_x']}, max_shift_y = {training['max_shift_y']}\")\n#     return training",
    "crumbs": [
      "train_enc"
    ]
  },
  {
    "objectID": "train_enc.html#compute-loss-on-batch",
    "href": "train_enc.html#compute-loss-on-batch",
    "title": "train_enc",
    "section": "Compute Loss On Batch",
    "text": "Compute Loss On Batch\n\n\ncompute_batch_loss\n\ndef compute_batch_loss(\n    batch, encoder, cfg, global_step, mae_decoder:NoneType=None, debug:bool=False\n):\n\nCompute loss and return other exal auxiliary variables (for train or val)",
    "crumbs": [
      "train_enc"
    ]
  },
  {
    "objectID": "train_enc.html#main-training-loop",
    "href": "train_enc.html#main-training-loop",
    "title": "train_enc",
    "section": "Main Training Loop",
    "text": "Main Training Loop\n\n\ntrain\n\ndef train(\n    cfg:DictConfig\n):",
    "crumbs": [
      "train_enc"
    ]
  },
  {
    "objectID": "train_enc.html#cli-entry-point",
    "href": "train_enc.html#cli-entry-point",
    "title": "train_enc",
    "section": "CLI Entry Point",
    "text": "CLI Entry Point",
    "crumbs": [
      "train_enc"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "midi-rae",
    "section": "",
    "text": "▬▬    ▬▬▬    ▬▬                                       ▬▬                                  \"Scrawl Me Maybe\"   \n                 ▬▬                                                                                                     \n ▬▬  ▬▬  ▬▬▬     ▬▬   ▬▬▬        ▬▬▬▬▬▬ ▬▬▬▬   ▬▬▬▬            ▬▬▬   ▬▬▬▬ ▬▬▬▬▬▬  ▬▬▬▬           ▬▬▬▬▬   ▬▬▬▬  ▬▬▬▬▬    \n ▬▬▬▬▬▬▬  ▬▬  ▬▬▬▬▬    ▬▬   ▬▬▬▬  ▬▬  ▬▬   ▬▬ ▬▬  ▬▬   ▬▬▬▬     ▬▬  ▬▬  ▬▬ ▬▬  ▬▬    ▬▬   ▬▬▬▬  ▬▬      ▬▬  ▬▬ ▬▬  ▬▬ \n ▬▬ ▬ ▬▬  ▬▬ ▬▬  ▬▬    ▬▬         ▬▬    ▬▬▬▬▬ ▬▬▬▬▬▬            ▬▬  ▬▬▬▬▬▬ ▬▬  ▬▬ ▬▬▬▬▬          ▬▬▬▬▬  ▬▬  ▬▬ ▬▬  ▬▬   \n ▬▬   ▬▬  ▬▬ ▬▬  ▬▬    ▬▬         ▬▬   ▬▬  ▬▬ ▬▬            ▬▬  ▬▬  ▬▬     ▬▬▬▬▬ ▬▬  ▬▬              ▬▬ ▬▬  ▬▬ ▬▬  ▬▬ \n ▬▬   ▬▬ ▬▬▬▬ ▬▬▬▬▬▬  ▬▬▬▬       ▬▬▬▬   ▬▬▬▬▬  ▬▬▬▬▬        ▬▬  ▬▬   ▬▬▬▬▬ ▬▬     ▬▬▬▬▬         ▬▬▬▬▬▬   ▬▬▬▬  ▬▬  ▬▬ \n                                                             ▬▬▬▬         ▬▬▬▬\nTODO: Not documented yet because it’s not ready for anyone besides the author to use.",
    "crumbs": [
      "midi-rae"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "midi-rae",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall midi_rae in Development mode\n# make sure midi_rae package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to midi_rae\n$ nbdev_prepare",
    "crumbs": [
      "midi-rae"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "midi-rae",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/drscotthawley/midi-rae.git\nor from pypi:\n$ pip install midi-rae\n\n\nGPU Acceleration (Optional)\nFor faster UMAP and PCA projections using NVIDIA GPUs, install with the gpu extra:\n$ pip install midi-rae[gpu]\nThis requires RAPIDS cuML and cuPy. See the RAPIDS installation guide for setup instructions — pip wheels are available for supported CUDA versions:\n$ pip install cuml-cu12 cupy-cuda12x  # for CUDA 12\nWithout GPU dependencies, the package falls back to CPU-based sklearn and umap-learn automatically.\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "midi-rae"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "midi-rae",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "midi-rae"
    ]
  },
  {
    "objectID": "train_dec.html",
    "href": "train_dec.html",
    "title": "Train Decoder",
    "section": "",
    "text": "PreEncodedDataset\n\ndef PreEncodedDataset(\n    encoded_dir\n):\n\nLoad pre-encoded embeddings + images from .pt files\n\n\n\nsetup_dataloaders\n\ndef setup_dataloaders(\n    cfg, preencoded:bool=False\n):\n\n\n\n\nsetup_models\n\ndef setup_models(\n    cfg, device, preencoded\n):\n\n\n\n\nsetup_tstate\n\ndef setup_tstate(\n    cfg, device, decoder, discriminator, encoder:NoneType=None\n):\n\nTraining_state: Losses, Optimizers, Schedulers, AMP Scalers\n\n\n\nget_embeddings_batch\n\ndef get_embeddings_batch(\n    batch, encoder:NoneType=None, preencoded:bool=False, device:str='cuda', allow_grad:bool=False\n):\n\n\n\n\ntrain_step\n\ndef train_step(\n    epoch, enc_out, img_real, decoder, discriminator, tstate, # named tuple containing optimizers, loss fns, scalers\n    cfg, # config\n):\n\ntraining step for decoder (and discriminator)\n\n\n\ntrain\n\ndef train(\n    cfg:DictConfig\n):",
    "crumbs": [
      "Train Decoder"
    ]
  },
  {
    "objectID": "inspect.html",
    "href": "inspect.html",
    "title": "Inspect",
    "section": "",
    "text": "import os\nassert False == os.path.isdir('/app/data'), \"Do not try to run this on solveit. The memory requirements will crash the VM.\"\nimport torch\nfrom torch.utils.data import DataLoader\nfrom omegaconf import OmegaConf\nfrom midi_rae.vit import ViTEncoder, ViTDecoder\nfrom midi_rae.swin import SwinEncoder, SwinDecoder\nfrom midi_rae.data import PRPairDataset\nfrom midi_rae.viz import make_emb_viz, viz_mae_recon\nfrom midi_rae.utils import load_checkpoint\nimport matplotlib.pyplot as plt\n\n# Interactive visualization (without wandb logging)\nimport plotly.io as pio\npio.renderers.default = 'notebook'\nfrom midi_rae.viz import umap_project, pca_project, plot_embeddings_3d, make_emb_viz, viz_mae_recon",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#config",
    "href": "inspect.html#config",
    "title": "Inspect",
    "section": "Config",
    "text": "Config\n\n#cfg = OmegaConf.load('../configs/config.yaml')\ncfg = OmegaConf.load('../configs/config_swin.yaml')\n#device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\ndevice = 'cpu'  # leave GPU free for training while we do analysis here.\nprint(f'device = {device}')\n\ndevice = cpu",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#load-dataset",
    "href": "inspect.html#load-dataset",
    "title": "Inspect",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nval_ds = PRPairDataset(image_dataset_dir=cfg.data.path, split='val', max_shift_x=cfg.training.max_shift_x, max_shift_y=cfg.training.max_shift_y)\nval_dl = DataLoader(val_ds, batch_size=cfg.training.batch_size, num_workers=4, drop_last=True)\nprint(f'Loaded {len(val_ds)} validation samples, batch_size = {cfg.training.batch_size}')\n\nLoading 91 val files from /home/shawley/datasets/POP909_images_basic...\nFinished loading.\nLoaded 9100 validation samples, batch_size = 380",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#inspect-data",
    "href": "inspect.html#inspect-data",
    "title": "Inspect",
    "section": "Inspect Data",
    "text": "Inspect Data\n\nbatch = next(iter(val_dl))\nimg1, img2, deltas, file_idx = batch['img1'].to(device), batch['img2'].to(device), batch['deltas'].to(device), batch['file_idx'].to(device)\nprint(\"img1.shape, deltas.shape, file_idx.shape =\",tuple(img1.shape), tuple(deltas.shape), tuple(file_idx.shape))\n\nimg1.shape, deltas.shape, file_idx.shape = (380, 1, 128, 128) (380, 2) (380,)\n\n\n\n# Show a sample image pair\nidx = 0\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\naxes[0].imshow(img1[idx, 0].cpu(), cmap='gray')\naxes[0].set_title(f'Image 1 (file_idx={file_idx[idx].item()})')\naxes[1].imshow(img2[idx, 0].cpu(), cmap='gray')\naxes[1].set_title(f'Image 2 (deltas = {deltas[idx].cpu().int().numpy()})')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#load-encoder-from-checkpoint",
    "href": "inspect.html#load-encoder-from-checkpoint",
    "title": "Inspect",
    "section": "Load Encoder from Checkpoint",
    "text": "Load Encoder from Checkpoint\n\n# if cfg.model.get('encoder', 'vit') == 'swin':\n\n# model = ViTEncoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size), \n#                    cfg.model.patch_size, cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\n\n# ckpt_path = f'../checkpoints/{}__best.pt'  # &lt;-- change as needed\n# ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n# state_dict = {k.replace('_orig_mod.', ''): v for k, v in ckpt['model_state_dict'].items()}\n# model.load_state_dict(state_dict, strict=False)\n\nif cfg.model.get('encoder', 'vit') == 'swin':\n    encoder = SwinEncoder(img_height=cfg.data.image_size, img_width=cfg.data.image_size,\n                    patch_h=cfg.model.patch_h, patch_w=cfg.model.patch_w,\n                    embed_dim=cfg.model.embed_dim, depths=cfg.model.depths,\n                    num_heads=cfg.model.num_heads, window_size=cfg.model.window_size,\n                    mlp_ratio=cfg.model.mlp_ratio, drop_path_rate=cfg.model.drop_path_rate).to(device)\nelse:\n    encoder = ViTEncoder(cfg.data.in_channels, cfg.data.image_size, cfg.model.patch_size,\n                         cfg.model.dim, cfg.model.depth, cfg.model.heads).to(device)\nencoder = load_checkpoint(encoder, cfg.get('encoder_ckpt', f'../checkpoints/{encoder.__class__.__name__}__best.pt'))\nencoder.eval()\nprint(f\"Loaded {encoder.__class__.__name__}\")\n\n&gt;&gt;&gt; Loaded model checkpoint from ../checkpoints/SwinEncoder__best.pt\nLoaded SwinEncoder",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#run-batch-through-encoder",
    "href": "inspect.html#run-batch-through-encoder",
    "title": "Inspect",
    "section": "Run Batch Through Encoder",
    "text": "Run Batch Through Encoder\n\nwith torch.no_grad():\n    enc_out1 = encoder(img1)\n    enc_out2 = encoder(img2)\n\n#     z1 = enc_out1.patches.all_emb.reshape(-1, enc_out1.patches[1].dim)\n#     z2 = enc_out2.patches.all_emb.reshape(-1, enc_out2.patches[1].dim)\n#     num_tokens = enc_out1.patches.all_emb.shape[1]\n\n# print(f'z1: {z1.shape}, z2: {z2.shape}, num_tokens: {num_tokens}')",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#visualize-embeddings",
    "href": "inspect.html#visualize-embeddings",
    "title": "Inspect",
    "section": "Visualize Embeddings",
    "text": "Visualize Embeddings\nNOTE: This will visualize all embeddings in the entire batch, not just the single pair of images shown above.\n\nfigs = make_emb_viz((enc_out1, enc_out2), encoder=encoder, batch=batch, do_umap=False)\nfigs.keys() # show what figures are available\n\ndict_keys(['cls_pca_fig', 'cls_umap_fig', 'patch_pca_fig', 'patch_umap_fig', 'empty_pca_fig'])\n\n\nNext code cell reads:\nfigs['cls_pca_fig'].show()\nMake sure the next code cell is hidden or else the plotly.js will swamp the LLM context.\n\nfigs['cls_pca_fig'].show()\n\n                                                    \n\n\nNote how the CLS tokens are nicely grouped in pairs. Let’s see if the same is true for the randomly-sampled pairs of non-empty patch embeddings 🤞:\nNext code cell reads:\nfigs['patch_pca_fig'].show()\nMake sure the next code cell is hidden or else the plotly.js will swamp the LLM context.\n\nfigs['patch_pca_fig'].show()",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#svd-analysis",
    "href": "inspect.html#svd-analysis",
    "title": "Inspect",
    "section": "SVD Analysis",
    "text": "SVD Analysis\n\ndef svd_analysis(enc_out, level=1,  title='', top_k=20):\n    \"Run SVD on encoder output, plot singular value spectrum and cumulative variance\"\n    z = enc_out.patches[level].emb.detach().cpu().float().reshape(-1, enc_out.patches[level].dim)  # flatten batch\n    z = z - z.mean(dim=0)  # center\n    U, S, Vt = torch.linalg.svd(z, full_matrices=False) # Vt for \"V transpose\" (technically it's \"V hermitian\" but we've got real data)\n    var_exp = (S**2) / (S**2).sum()\n    cum_var = var_exp.cumsum(0)\n\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n    ax1.semilogy(S.numpy()); ax1.axvline(x=top_k, color='r', ls='--', alpha=0.5)\n    ax1.set(xlabel='Component', ylabel='Singular value', title=f'{title} Singular Values')\n    ax2.bar(range(top_k), var_exp[:top_k].numpy())\n    ax2.set(xlabel='Component', ylabel='Variance explained', title=f'{title} Top {top_k} Variance')\n    ax3.plot(cum_var.numpy()); ax3.axhline(y=0.9, color='r', ls='--', alpha=0.5, label='90%')\n    ax3.set(xlabel='Component', ylabel='Cumulative variance', title=f'{title} Cumulative Variance')\n    ax3.legend()\n    plt.tight_layout(); plt.show()\n\n    n90 = (cum_var &lt; 0.9).sum().item() + 1\n    print(f\"{title}: {n90} components for 90% variance, top-1 explains {var_exp[0]:.1%}\")\n    return S, U, Vt, var_exp\n\n\nS, U, Vt, var_exp = svd_analysis(enc_out2, title='Patches')\n\n\n\n\n\n\n\n\nPatches: 7 components for 90% variance, top-1 explains 59.3%\n\n\n\ncls_S, cls_U, cls_Vt, cls_var_exp = svd_analysis(enc_out2, level=0, title='CLS')\n\n\n\n\n\n\n\n\nCLS: 2 components for 90% variance, top-1 explains 85.9%\n\n\nTwo key takeaways:\n\nPatches need 178/256 dims for 90%. The representation is highly distributed with no dominant direction. This means the encoder is using nearly all its capacity, which is healthy (no dimensional collapse). But it also suggests rhythm and pitch aren’t cleanly factored — if they were, you’d expect a sharper elbow in the spectrum (the first 1 or 2 components notwithstanding).\nCLS only needs 23/256 dims. The global summary is much more compressed. That’s interesting for generation: it suggests the “gist” of a musical passage lives in a ~23-dimensional subspace. The gradual slope in the top-20 bars (no single dominant component) means it’s not collapsing to a trivial representation either.",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#decoder-performance",
    "href": "inspect.html#decoder-performance",
    "title": "Inspect",
    "section": "Decoder Performance",
    "text": "Decoder Performance\n\nif cfg.model.get('encoder', 'vit') == 'swin': # decoder should match encoder\n    decoder = SwinDecoder(img_height=cfg.data.image_size, img_width=cfg.data.image_size,\n                        patch_h=cfg.model.patch_h, patch_w=cfg.model.patch_w,\n                        embed_dim=cfg.model.embed_dim,\n                        depths=cfg.model.get('dec_depths', cfg.model.depths), \n                        num_heads=cfg.model.get('dec_num_heads', cfg.model.num_heads)).to(device)\nelse: \n    decoder = ViTDecoder(cfg.data.in_channels, (cfg.data.image_size, cfg.data.image_size),\n                     cfg.model.patch_size, cfg.model.dim, \n                     cfg.model.get('dec_depth', 4), cfg.model.get('dec_heads', 8)).to(device)\n\nname = decoder.__class__.__name__\nprint(\"Name = \",name)\ndecoder = load_checkpoint(decoder, cfg.get('encoder_ckpt', f'../checkpoints/{decoder.__class__.__name__}__best.pt'))\n\nName =  SwinDecoder\n&gt;&gt;&gt; Loaded model checkpoint from ../checkpoints/SwinDecoder__best.pt\n\n\n\nrecon_logits = decoder(enc_out2)\nimg_recon = torch.sigmoid(recon_logits) \nimg_real = img2\nprint(\"img_recon.shape, img_real.shape =\",img_recon.shape, img_real.shape)\n\nimg_recon.shape, img_real.shape = torch.Size([380, 1, 128, 128]) torch.Size([380, 1, 128, 128])\n\n\n\ngrid_recon, grid_real, grid_map, evals = viz_mae_recon(img_recon, img_real, enc_out=None, epoch=0, debug=False, return_maps=True)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\nax1.imshow(grid_real.permute(1,2,0), cmap='gray'); ax1.set_title('Real')\nax2.imshow(grid_recon.permute(1,2,0), cmap='gray'); ax2.set_title('Recon')\nax3.imshow(grid_map.permute(1,2,0)); ax3.set_title('Map')\nplt.show()\nprint(', '.join(f\"{k}: {v.item():.4f}\" for k, v in evals.items() if not k.endswith('map')))\n\n\n\n\n\n\n\n\nprecision: 0.9990, recall: 0.9998, specificity: 1.0000, f1: 0.9994",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "inspect.html#wow-f1-0.9995",
    "href": "inspect.html#wow-f1-0.9995",
    "title": "Inspect",
    "section": "Wow! F1 = 0.9995!",
    "text": "Wow! F1 = 0.9995!\nThat’s nearly perfect reconstruction: F1 accuracy of 99.95% Seems we have our representation autoencoder!\nLet’s Show the map image really big. It’s designed to show red pixels wherever there are False Positives and yellow pixels wherever there are False Negatives (and white = True Pos, black = True Neg)…\nI don’t see any red or yellow, do you?\nIn the next cell we’re gonna plot an image showing the maps as a very large image, we’re gonna hide it from the LLM because it doesn’t need to see it and we wanna spare the context.\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg = Image.fromarray((grid_map*255).permute(1,2,0).byte().numpy())\ndisplay(img)",
    "crumbs": [
      "Inspect"
    ]
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "viz",
    "section": "",
    "text": "def cpu_umap_project(\n    embeddings, n_components:int=3, n_neighbors:int=15, min_dist:float=0.1, random_state:int=42\n):\n\nProject embeddings to n_components dimensions via UMAP (on CPU)\n\n\n\n\n\ndef cuml_umap_project(\n    embeddings, n_components:int=3, n_neighbors:int=15, min_dist:float=0.1, random_state:int=42\n):\n\nProject embeddings to n_components dimensions via cuML UMAP (GPU)\n\n\n\n\n\ndef umap_project(\n    embeddings, kwargs:VAR_KEYWORD\n):\n\nCalls one of two preceding UMAP routines based on device availability.",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#umap",
    "href": "viz.html#umap",
    "title": "viz",
    "section": "",
    "text": "def cpu_umap_project(\n    embeddings, n_components:int=3, n_neighbors:int=15, min_dist:float=0.1, random_state:int=42\n):\n\nProject embeddings to n_components dimensions via UMAP (on CPU)\n\n\n\n\n\ndef cuml_umap_project(\n    embeddings, n_components:int=3, n_neighbors:int=15, min_dist:float=0.1, random_state:int=42\n):\n\nProject embeddings to n_components dimensions via cuML UMAP (GPU)\n\n\n\n\n\ndef umap_project(\n    embeddings, kwargs:VAR_KEYWORD\n):\n\nCalls one of two preceding UMAP routines based on device availability.",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#pca",
    "href": "viz.html#pca",
    "title": "viz",
    "section": "PCA",
    "text": "PCA\n\n\ncuml_pca_project\n\ndef cuml_pca_project(\n    embeddings, n_components:int=3\n):\n\nProject embeddings to n_components dimensions via cuML PCA (GPU)\n\n\n\ncpu_pca_project\n\ndef cpu_pca_project(\n    embeddings, n_components:int=3\n):\n\nProject embeddings to n_components dimensions via sklearn PCA (CPU)\n\n\n\npca_project\n\ndef pca_project(\n    embeddings, kwargs:VAR_KEYWORD\n):\n\nCalls GPU or CPU PCA based on availability",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#d-plotly-scatterplots",
    "href": "viz.html#d-plotly-scatterplots",
    "title": "viz",
    "section": "3D Plotly Scatterplots",
    "text": "3D Plotly Scatterplots\n\n\nplot_embeddings_3d\n\ndef plot_embeddings_3d(\n    coords, color_by:str='pairs', file_idx:NoneType=None, deltas:NoneType=None, title:str='Embeddings',\n    debug:bool=False\n):\n\n3D scatter plot of embeddings. color_by: ‘none’, ‘file’, or ‘pair’",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#main-routine",
    "href": "viz.html#main-routine",
    "title": "viz",
    "section": "Main Routine",
    "text": "Main Routine\nCalls the preceding routines\nTesting that:\n\nn_pairs, dim = 5, 1  # data points\nz1 = 200*torch.arange(n_pairs).unsqueeze(-1).unsqueeze(-1)\nz2 = z1 + 1 \nzs = torch.cat([z1, z2], dim=0).view(-1, dim)\nprint(\"zs.shape = \",zs.shape)\nindices = torch.arange(2*n_pairs)\ndeltas = torch.randint(0,12,(2*n_pairs, 2))\nprint(\"zs = \\n\",zs)\nprint(\"indices =\",indices)\ndata_perm, indices2, deltas2 = _subsample(zs, indices, deltas, max_points=2*(n_pairs-2), debug=True)\nprint(\"data_perm.shape = \",data_perm.shape,\", data_perm = \\n\",data_perm)\n\nzs.shape =  torch.Size([10, 1])\nzs = \n tensor([[  0],\n        [200],\n        [400],\n        [600],\n        [800],\n        [  1],\n        [201],\n        [401],\n        [601],\n        [801]])\nindices = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ndata_perm.shape =  torch.Size([6, 1]) , data_perm = \n tensor([[200],\n        [800],\n        [600],\n        [201],\n        [801],\n        [601]])\n\n\nYes. That does what I expect. Moving on…\n\n\nmake_emb_viz\n\ndef make_emb_viz(\n    enc_outs, epoch:int=-1, encoder:NoneType=None, batch:NoneType=None, title:str='Embeddings', max_points:int=5000,\n    do_umap:bool=False, debug:bool=False\n):\n\nthis is the main viz routine, showing different groups of embeddings\nTesting visualization:\n\nimport plotly.io as pio\npio.renderers.default = 'notebook'\nfrom midi_rae.core import PatchState, HierarchicalPatchState, EncoderOutput\n\nbs, dim = 32, 256\nnum_patch = 64\n\n# Build fake embeddings\nz1_cls = torch.randn(bs, 1, dim)\nz1_patch = torch.randn(bs, num_patch, dim)\nz2_cls = z1_cls + 0.1 * torch.randn(bs, 1, dim)\nz2_patch = z1_patch + 0.1 * torch.randn(bs, num_patch, dim)\n\n# Positions and masks\ncls_pos = torch.tensor([[-1, -1]])\npatch_pos = torch.stack([torch.tensor([r, c]) for r in range(8) for c in range(8)])\nmae_mask_cls = torch.ones(1, dtype=torch.bool)\nmae_mask_patch = torch.ones(num_patch, dtype=torch.bool)\n\nne1 = torch.ones(bs, num_patch, dtype=torch.bool)\nne2 = torch.ones(bs, num_patch, dtype=torch.bool)\nne2[16:, :] = 0  # make half empty\n\nenc_out1 = EncoderOutput(\n    patches=HierarchicalPatchState(levels=[\n        PatchState(emb=z1_cls, pos=cls_pos, non_empty=torch.ones(bs, 1, dtype=torch.bool), mae_mask=mae_mask_cls),\n        PatchState(emb=z1_patch, pos=patch_pos, non_empty=ne1, mae_mask=mae_mask_patch),\n    ]),\n    full_pos=torch.cat([cls_pos, patch_pos]), full_non_empty=torch.cat([torch.ones(bs,1,dtype=torch.bool), ne1], dim=1),\n    mae_mask=torch.cat([mae_mask_cls, mae_mask_patch]),\n)\nenc_out2 = EncoderOutput(\n    patches=HierarchicalPatchState(levels=[\n        PatchState(emb=z2_cls, pos=cls_pos, non_empty=torch.ones(bs, 1, dtype=torch.bool), mae_mask=mae_mask_cls),\n        PatchState(emb=z2_patch, pos=patch_pos, non_empty=ne2, mae_mask=mae_mask_patch),\n    ]),\n    full_pos=torch.cat([cls_pos, patch_pos]), full_non_empty=torch.cat([torch.ones(bs,1,dtype=torch.bool), ne2], dim=1),\n    mae_mask=torch.cat([mae_mask_cls, mae_mask_patch]),\n)\n\nbatch = {'file_idx': torch.arange(bs), 'deltas': torch.randint(0, 12, (bs, 2))}\n\nfigs = make_emb_viz((enc_out1, enc_out2), title='testing', batch=batch, do_umap=False, debug=True)\n\nNext code cell reads\nfigs['patch_pca_fig'].show()\nMake sure the next code cell is hidden from LLM or the Plotly JS code will swamp the context.\n\n# Make sure this cell is hidden from LLM or it will swamp the context.\nfigs['patch_pca_fig'].show()",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "viz.html#reconstructions",
    "href": "viz.html#reconstructions",
    "title": "viz",
    "section": "Reconstructions",
    "text": "Reconstructions\n\n\nexpand_patch_mask\n\ndef expand_patch_mask(\n    mae_mask, grid_h, grid_w, patch_size\n):\n\nExpand patch-level mask (N,) to pixel-level mask (H, W)\n\n\n\ndo_recon_eval\n\ndef do_recon_eval(\n    recon, real, mae_mask:NoneType=None, patch_size:int=16, eps:float=1e-08, return_maps:bool=False\n):\n\nEvaluate recon accuracy, optionally only on masked patches\n\n\n\npatches_to_img\n\ndef patches_to_img(\n    recon_patches, img_real, patch_size:int=16, mae_mask:NoneType=None\n):\n\nConvert image patches to full image. Copy over real patches where appropriate.\n\n\n\nviz_mae_recon\n\ndef viz_mae_recon(\n    recon, img_real, enc_out:NoneType=None, epoch:int=-1, patch_size:int=16, debug:bool=False,\n    return_maps:bool=False\n):\n\nShow how our LightweightMAEDecoder is doing (during encoder training)\nTesting code:\n\nfrom midi_rae.core import *\nimport matplotlib.pyplot as plt\n\nB, patch_size = 4, 16\nimg_real = (torch.rand(B, 1, 128, 128) &gt; 0.7).float()  # fake sparse piano roll\nrecon = torch.randn(B, 65, patch_size**2)  # 64 patches + CLS, raw logits\n\nmae_mask = torch.ones(65, dtype=torch.bool)\nmae_mask[1::2] = False  # mask every other patch (skip CLS at 0)\n\nenc_out = EncoderOutput(\n    patches=HierarchicalPatchState(levels=[\n        PatchState(emb=torch.randn(B,1,256), pos=torch.tensor([[-1,-1]]), non_empty=torch.ones(B,1,dtype=torch.bool), mae_mask=mae_mask[0:1]),\n        PatchState(emb=torch.randn(B,64,256), pos=torch.zeros(64,2), non_empty=torch.ones(B,64,dtype=torch.bool), mae_mask=mae_mask[1:]),\n    ]),\n    full_pos=torch.zeros(65,2), full_non_empty=torch.ones(B,65,dtype=torch.bool), mae_mask=mae_mask,\n)\n\ngrid_recon, grid_real, grid_map, evals = viz_mae_recon(recon, img_real, enc_out=enc_out, epoch=0, debug=True, return_maps=True)\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 6))\nax1.imshow(grid_real.permute(1,2,0), cmap='gray'); ax1.set_title('Real')\nax2.imshow(grid_recon.permute(1,2,0), cmap='gray'); ax2.set_title('Recon')\nax3.imshow(grid_map.permute(1,2,0)); ax3.set_title('Map')\nplt.show()\nprint(', '.join(f\"{k}: {v.item():.4f}\" for k, v in evals.items() if not k.endswith('map')))\n\nmae_mask: shape=torch.Size([64]), pct_visible=0.500\n\n\n\n\n\n\n\n\n\nprecision: 0.2956, recall: 0.4998, specificity: 0.4960, f1: 0.3715",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "toy_factorization.html",
    "href": "toy_factorization.html",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "",
    "text": "We use our explicit data augmentation of translations and pitch and time and feed those as labels into a loss function to try and encode those operations geometrically. Add this loss function to an encoder and see if it achieves a soft factorization of pitch and time:\ndef factorization_loss(z_anchor, z_crop1, z_crop2, targets):\n    d1 = z_crop1 - z_anchor\n    d2 = z_crop2 - z_anchor\n    cos = F.cosine_similarity(d1, d2, dim=-1)\n    return ((cos - targets) ** 2).mean()\nwhere targets are +1 (same-type, same-sign), −1 (same-type, opposite-sign), or 0 (cross-type):\n\n\n\nimage.png\n\n\n(We do not specify these directions. We simply enforce parallel, anti-parallel, or orthogonal, and let the system evolve on its own.)\nSpoiler: it works. Since this toy model succeeds, we will have confidence to move on to applying it to the full problem with real MIDI data.",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#tldr",
    "href": "toy_factorization.html#tldr",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "",
    "text": "We use our explicit data augmentation of translations and pitch and time and feed those as labels into a loss function to try and encode those operations geometrically. Add this loss function to an encoder and see if it achieves a soft factorization of pitch and time:\ndef factorization_loss(z_anchor, z_crop1, z_crop2, targets):\n    d1 = z_crop1 - z_anchor\n    d2 = z_crop2 - z_anchor\n    cos = F.cosine_similarity(d1, d2, dim=-1)\n    return ((cos - targets) ** 2).mean()\nwhere targets are +1 (same-type, same-sign), −1 (same-type, opposite-sign), or 0 (cross-type):\n\n\n\nimage.png\n\n\n(We do not specify these directions. We simply enforce parallel, anti-parallel, or orthogonal, and let the system evolve on its own.)\nSpoiler: it works. Since this toy model succeeds, we will have confidence to move on to applying it to the full problem with real MIDI data.",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#motivation",
    "href": "toy_factorization.html#motivation",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Motivation",
    "text": "Motivation\n\nWhy factorize pitch and time?\nMusical motifs exhibit repeated melodic patterns (pitch) and repeated rhythmic patterns (time) that together explain much of the variety within a song. This suggests a natural decomposition of musical representations into a pitch vocabulary and a rhythm vocabulary, where a given motif can be approximated as an interaction (outer product) of pitch and time components. Factored representations are also connected to compositional algebraic properties — the GloVe word vectors (Pennington et al., 2014) demonstrated that vector arithmetic relationships (e.g., king − man + woman ≈ queen) arose from GloVe’s explicitly factorized construction of word co-occurrence matrices.\n\n\nWhy soft factorization?\nPrior work in symbolic music analysis has explored both fully factored and fully combined representations. Notably, MelodyGLM (Wu et al., 2023) compared pitch n-grams, rhythm n-grams, and combined n-grams for melody generation. Their ablation study showed that using all three together worked best, but that the combined (non-factored) n-grams carried meaningful information lost by independent factorization. Similarly, Music SketchNet (Chen et al., 2020) and PiRhDy (Liang et al., 2020) construct factored representations by design, which limits the model’s ability to capture pitch–rhythm interactions.\nThis motivates a middle path: rather than forcing factorization by construction (hard factorization) or hoping to discover it post-hoc (e.g., GANSpace), we use a soft factorization via the loss function. The cosine-similarity loss encourages orthogonality between pitch and time directions in latent space, while still allowing the model freedom to represent non-factored structure. This approach is inspired by the idea—articulated in Albinet (2026)—that competing soft constraints in an objective can give rise to discrete eigenstructure at equilibrium, analogous to how kernel PCA discovers structure through balanced forces rather than hard architectural choices.\nRecent theoretical work lends further support to this direction. Shai et al. (2026) show that transformers trained via next-token prediction naturally learn factored representations in orthogonal subspaces when the underlying factors are conditionally independent — and exhibit an inductive bias toward factoring even when strict independence is violated. This suggests that factored structure is not merely a convenient assumption but something models actively prefer, and that nudging them toward it via a soft loss aligns with their natural learning dynamics.\n\n\nWhy blobs?\nThis toy experiment uses random binary blob images as a stand-in for MIDI piano-roll data. Vertical shifts simulate pitch transposition and horizontal shifts simulate time shifting. These shifts are exact, continuous, and independently controllable—making this domain particularly well-suited for testing whether soft geometric constraints can induce factored latent structure. The real target application is learning factored representations of musical motifs, where the training signal would come from a JEPA-style self-supervised objective augmented with this orthogonality loss.",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#the-larger-context",
    "href": "toy_factorization.html#the-larger-context",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "The Larger Context",
    "text": "The Larger Context\nI’m building a Representation Autoencoder (RAE, Zheng et al., 2025) for MIDI music based on piano roll representations. I had the idea that some kind of factorization in pitch and time would be useful, but actually prescribing it by construction would probably be detrimental (as the MelodyGLM paper noted). And I love trying to do geometrical operations in latent space, e.g., Operational Latent Spaces (OpLaS, Hawley & Tackett, 2024).\nSo, after reading Franck’s series on SVD (Abinet, 2026), which includes contact with contrastive losses and soft objectives. This seemed like a natural thing thing to try!\n\nWhy a toy model?\nI generally like to create toy models when I do anything new because they’re interpretable and you can iterate quickly. If I couldn’t even get the thing to work on this simple system with a small latent space and fake piano rolls, then what hope would I have for doing it in the larger context with the full code and the real data?\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt, numpy as np\nfrom pathlib import Path\nfrom tqdm.auto import trange, tqdm\nimport matplotlib.pyplot as plt\ntry: import wandb; HAS_WANDB = True\nexcept ImportError: HAS_WANDB = False\nprint(\"HAS_WANDB =\",HAS_WANDB)\ndevice = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\nprint(f\"Using {device}\")\n\nHAS_WANDB = True\nUsing cuda",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#data-random-blob-patches-augmentations",
    "href": "toy_factorization.html#data-random-blob-patches-augmentations",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Data: Random blob patches + augmentations",
    "text": "Data: Random blob patches + augmentations\n\ndef make_blob(sz=16, n_blobs=3):\n    \"\"\"Generate a random binary blob image (1, sz, sz).\"\"\"\n    img = torch.zeros(1, sz, sz)\n    for _ in range(n_blobs):\n        r, c = torch.randint(0, sz-3, (2,))\n        h, w = torch.randint(1, 4, (2,))\n        img[0, r:r+h, c:c+w] = 1.0\n    if torch.rand(1) &gt; 0.5: img = torch.flip(img, [1])\n    if torch.rand(1) &gt; 0.5: img = torch.flip(img, [2])\n    return img\n\n\ndef shift_no_wrap(x, shifts, dims):\n    \"\"\"Drop-in replacement for torch.roll with zero-fill instead of wrap.\"\"\"\n    if shifts == 0: return x\n    out = torch.roll(x, shifts=shifts, dims=dims)\n    n = abs(shifts)\n    if shifts &gt; 0:\n        out.narrow(dims, 0, n).zero_()\n    else:\n        out.narrow(dims, out.size(dims)-n, n).zero_()\n    return out\n\n\n# Quick visual test\nimg = make_blob()\nfig, axes = plt.subplots(2, 5, figsize=(10, 5))\nfor row in range(2):\n    axes[row, 0].imshow(img[0], cmap='gray', vmin=0, vmax=1)\n    axes[row, 0].set_title('Original')\nfor i, s in enumerate([-4, -2, 2, 4]):\n    axes[0, i+1].imshow(shift_no_wrap(img, s, 1)[0], cmap='gray', vmin=0, vmax=1)\n    axes[0, i+1].set_title(f'dy={s}')\n    axes[1, i+1].imshow(shift_no_wrap(img, s, 2)[0], cmap='gray', vmin=0, vmax=1)\n    axes[1, i+1].set_title(f'dx={s}')\nfor ax in axes.flat: ax.set_xticks([]); ax.set_yticks([])\naxes[0, 0].set_ylabel('Vertical', fontsize=12)\naxes[1, 0].set_ylabel('Horizontal', fontsize=12)\nplt.suptitle('shift_no_wrap: zero-padded shifts', fontsize=14)\nplt.tight_layout(); plt.show()\n\n\n\n\n\n\n\n\n\nclass BlobShiftDataset(torch.utils.data.Dataset):\n    def __init__(self, epoch_size=8192, sz=16, max_shift=4):\n        self.epoch_size = epoch_size\n        self.sz = sz\n        self.max_shift = max_shift\n\n    def __len__(self): return self.epoch_size\n\n    def __getitem__(self, idx):\n        sz, ms = self.sz, self.max_shift\n        while True:\n            img = make_blob(sz)\n            scheme = torch.randint(0, 3, (1,)).item()\n            s1 = torch.randint(1, ms+1, (1,)).item()\n            s2 = torch.randint(1, ms+1, (1,)).item()\n            sign1 = 1 if torch.rand(1) &gt; 0.5 else -1\n            sign2 = 1 if torch.rand(1) &gt; 0.5 else -1\n            if scheme == 0:    dy1, dx1, dy2, dx2 = sign1*s1, 0, sign2*s2, 0\n            elif scheme == 1:  dy1, dx1, dy2, dx2 = 0, sign1*s1, 0, sign2*s2\n            else:              dy1, dx1, dy2, dx2 = sign1*s1, 0, 0, sign2*s2\n            c1 = shift_no_wrap(shift_no_wrap(img, shifts=dy1, dims=1), shifts=dx1, dims=2)\n            c2 = shift_no_wrap(shift_no_wrap(img, shifts=dy2, dims=1), shifts=dx2, dims=2)\n            if not torch.equal(c1, c2): break\n        target = float(np.sign(sign1 * sign2)) if scheme &lt; 2 else 0.0\n        return img, c1, c2, torch.tensor(target), torch.tensor(scheme), (dy1, dx1), (dy2, dx2)\n\nViz test of that:\n\ndef show_triplets(anchors, crop1s, crop2s, targets, schemes, deltas1, deltas2, n_rows=None):\n    \"\"\"Visualize triplets with colored borders and delta labels.\n    Colors: blue=pitch/pitch, green=time/time, red=pitch+time.\n    \"\"\"\n    from matplotlib.patches import Patch\n    scheme_colors = {0: '#4488ff', 1: '#33bb55', 2: '#dd3333'}\n    if n_rows is None: n_rows = len(anchors)\n    n_rows = min(n_rows, len(anchors))\n\n    fig, axes = plt.subplots(n_rows, 3, figsize=(8, n_rows * 1.5))\n    if n_rows == 1: axes = axes[None]  # ensure 2D\n\n    for i in trange(n_rows):\n        color = scheme_colors[schemes[i].item()]\n        for j, (img, title) in enumerate(zip(\n                [anchors[i], crop1s[i], crop2s[i]], [\"Anchor\", \"Crop 1\", \"Crop 2\"])):\n            axes[i, j].imshow(img[0], cmap=\"gray\", vmin=0, vmax=1)\n            axes[i, j].set_title(title if i == 0 else \"\", fontsize=24)\n            axes[i, j].set_xticks([]); axes[i, j].set_yticks([])\n            for spine in axes[i, j].spines.values():\n                spine.set_color(color); spine.set_linewidth(2.5)\n        dy1, dx1 = deltas1[0][i].item(), deltas1[1][i].item()\n        dy2, dx2 = deltas2[0][i].item(), deltas2[1][i].item()\n        axes[i, 1].set_ylabel(f\"Δ({dy1:+d},{dx1:+d})\", fontsize=15, color=color, fontweight='bold')\n        axes[i, 2].set_ylabel(f\"Δ({dy2:+d},{dx2:+d})\", fontsize=15, color=color, fontweight='bold')\n\n    legend_items = [\n        Patch(facecolor='white', edgecolor='#4488ff', linewidth=3, label='Pitch only'),\n        Patch(facecolor='white', edgecolor='#33bb55', linewidth=3, label='Time only'),\n        Patch(facecolor='white', edgecolor='#dd3333', linewidth=3, label='Pitch + Time'),\n    ]\n    fig.legend(handles=legend_items, loc='lower center', ncol=3, fontsize=16,\n               frameon=False, bbox_to_anchor=(0.5, -0.02))\n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.06)\n    plt.show()\n\n\n# test code\nds = BlobShiftDataset()\ndl = DataLoader(ds, batch_size=8)\na, c1, c2, t, s, d1, d2 = next(iter(dl)) # augment_batch(8)\nshow_triplets(a, c1, c2, t, s, d1, d2)",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#encoder-tiny-convnet-latent-vector",
    "href": "toy_factorization.html#encoder-tiny-convnet-latent-vector",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Encoder: tiny ConvNet → latent vector",
    "text": "Encoder: tiny ConvNet → latent vector\n\nclass ToyEncoder(nn.Module):\n    def __init__(self, latent_dim=16):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(1, 16, 3, padding=1), nn.GELU(),          # 16x16\n            nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.GELU(),  # 8x8\n            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.GELU(),  # 4x4\n            nn.Conv2d(64,  64, 3, padding=1), nn.GELU(),           # 4x4\n            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n            nn.Linear(64, latent_dim),\n        )\n    def forward(self, x): return self.net(x)\n\n\nenc = ToyEncoder().to(device)\nprint(f\"Params: {sum(p.numel() for p in enc.parameters()):,}\")\n\nParams: 61,264",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#loss-cosine-factorization-objective",
    "href": "toy_factorization.html#loss-cosine-factorization-objective",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Loss: cosine factorization objective",
    "text": "Loss: cosine factorization objective\n\ndef factorization_loss(z_anchor, z_crop1, z_crop2, targets):\n    \"\"\"Cosine-similarity loss on difference vectors.\n    targets: +1 (parallel), -1 (anti-parallel), 0 (orthogonal)\"\"\"\n    d1 = z_crop1 - z_anchor  # (B, D)\n    d2 = z_crop2 - z_anchor  # (B, D)\n    cos = F.cosine_similarity(d1, d2, dim=-1)  # (B,)\n    return ((cos - targets) ** 2).mean()",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#lejepa-too",
    "href": "toy_factorization.html#lejepa-too",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "LeJEPA too?",
    "text": "LeJEPA too?\nTo make this slightly less of a toy model and more like the full system, we can also turn on the LeJEPA loss (Balestriero & LeCun, 2025), which consists of attracting similar crops(/“views”) and a sigreg loss to prevent collapse.\n\ndef safe_mean(t, dim=None): \n    \"\"\"safe replacement for torch.mean( ). (Don't need it for this NB, but I use it elsewhere in my project.) \"\"\"\n    return t.mean(dim=dim) if t.numel() &gt; 0 else 0.0\n\ndef attraction_loss(z1, z2,  # embeddings of two \"views\" of the same thing (in batches)\n                    deltas=None,   # optional/TBD: info on semantic 'distance' between z1 & z2\n                    tau = 100.0):    # inverse strength of fall-off for delta distances, big=slower\n    \"How we pull similar 'views' together\"\n    if deltas is None: return safe_mean( (z1 - z2).square() )\n    delta_diag = (deltas**2).sum(dim=1)\n    delta_fac = torch.exp(-delta_diag / tau) # less attraction for more 'distant' views\n    #delta_fac = 1/(1 + delta_diag/tau)  # longer tail than exp\n    return safe_mean( (z1 - z2).square() * delta_fac.unsqueeze(-1) )\n\ndef SIGReg(x, global_step, num_slices=256):\n    \"\"\"SIGReg with Epps-Pulley statistic. x is (N, K) tensor.\"\"\"\n    device = x.device\n    g = torch.Generator(device=device).manual_seed(global_step)\n    proj_shape = (x.size(1), num_slices)\n    A = torch.randn(proj_shape, generator=g, device=device)\n    A = A / (A.norm(dim=0, keepdim=True) + 1e-10)  # normalize columns\n    \n    # Epps-Pulley statistic\n    t = torch.linspace(-5, 5, 17, device=device) # values used in LeJEPA paper, worked for SSLtoy\n    exp_f = torch.exp(-0.5 * t**2)  # theoretical CF for N(0,1)\n    x_t = (x @ A).unsqueeze(2) * t  # (N, M, T)\n    ecf = (torch.exp(1j * x_t).mean(dim=0)).abs()  # empirical CF\n    diff = (ecf - exp_f).abs().square().mul(exp_f)  # weighted L2 distance\n    #N = x.size(0)  # With respect to Yann: Don't scale by N because then if you change the batch size you have to retune lambd by hand ugh\n    T = torch.trapz(diff, t, dim=1).sum() #* N  # sum here is over num slices, not data points\n    return T\n\ndef LeJEPA(z1, z2, global_step, lambd=0.5, deltas=None): \n    \"Main LeJEPA loss function\"\n    sim = attraction_loss(z1, z2, deltas=deltas)\n    sigreg = SIGReg( torch.cat((z1, z2), dim=0), global_step ) * 1 # normalize to similar scale as sim\n    return {'loss': (1-lambd)*sim + lambd*sigreg, 'sim':sim.item(), 'sigreg':sigreg.item()}",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#training-loop",
    "href": "toy_factorization.html#training-loop",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Training loop",
    "text": "Training loop\n\ndef train(enc, n_steps=2000, batch_size=1024, lr=4e-3, log_every=100, num_workers=4, use_jepa=False, use_wandb=False):\n    opt = torch.optim.Adam(enc.parameters(), lr=lr)\n    ds = BlobShiftDataset(epoch_size=n_steps * batch_size)\n    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, num_workers=num_workers, pin_memory=(device != 'cpu'))\n    if use_wandb and HAS_WANDB:\n        wandb.init(project=\"toy-factorization\", config=dict(n_steps=n_steps, batch_size=batch_size, lr=lr, latent_dim=8))\n    losses, fact_losses, jepa_losses = [], [], []\n    for step, (a, c1, c2, t, s, d1, d2) in enumerate(tqdm(dl)):\n        a, c1, c2, t = a.to(device), c1.to(device), c2.to(device), t.to(device)\n        za, z1, z2 = enc(a), enc(c1), enc(c2)\n        fact_loss = factorization_loss(za, z1, z2, t)\n        loss = fact_loss\n        if use_jepa:\n            jepa_loss = LeJEPA(za, z1, step)['loss']  + LeJEPA(za, z2, step)['loss'] \n            loss = loss + 0.2*jepa_loss\n        opt.zero_grad(); loss.backward(); opt.step()\n        losses.append(loss.item())\n        fact_losses.append(fact_loss.item())\n        if use_jepa: jepa_losses.append(jepa_loss.item())\n        if step % log_every == 0:\n            print(f\"step {step:4d}  loss={loss.item():.4f}\")\n        if HAS_WANDB and wandb.run is not None: wandb.log({\"loss\": loss.item(), \"fact\":fact_loss.item(), \"jepa\":jepa_loss.item() if use_jepa else 0.0, \"step\": step})\n    if HAS_WANDB and wandb.run is not None: wandb.finish()\n    return losses, fact_losses, jepa_losses\n\nNOTE: with use_jepa=True, you’ll need 10,000 steps or more.\n\nlosses, fact_losses, jepa_losses = train(enc, n_steps=5000, use_wandb=True)\n\n\nwandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/shawley/.netrc.\n\nwandb: Currently logged in as: drscotthawley to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n\n\n\n\n\n\n\nTracking run with wandb version 0.24.2\n\n\nRun data is saved locally in /home/shawley/github/midi-rae/nbs/wandb/run-20260226_231124-frmov01u\n\n\nSyncing run spring-dust-17 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/drscotthawley/toy-factorization\n\n\n View run at https://wandb.ai/drscotthawley/toy-factorization/runs/frmov01u\n\n\n\n\n\nstep    0  loss=0.8981\nstep  100  loss=0.1192\nstep  200  loss=0.0714\nstep  300  loss=0.0528\nstep  400  loss=0.0453\nstep  500  loss=0.0498\nstep  600  loss=0.0390\nstep  700  loss=0.0405\nstep  800  loss=0.0360\nstep  900  loss=0.0394\nstep 1000  loss=0.0325\nstep 1100  loss=0.0308\nstep 1200  loss=0.0316\nstep 1300  loss=0.0341\nstep 1400  loss=0.0313\nstep 1500  loss=0.0307\nstep 1600  loss=0.0278\nstep 1700  loss=0.0299\nstep 1800  loss=0.0314\nstep 1900  loss=0.0279\nstep 2000  loss=0.0304\nstep 2100  loss=0.0314\nstep 2200  loss=0.0317\nstep 2300  loss=0.0290\nstep 2400  loss=0.0314\nstep 2500  loss=0.0254\nstep 2600  loss=0.0330\nstep 2700  loss=0.0263\nstep 2800  loss=0.0271\nstep 2900  loss=0.0305\nstep 3000  loss=0.0248\nstep 3100  loss=0.0311\nstep 3200  loss=0.0313\nstep 3300  loss=0.0310\nstep 3400  loss=0.0279\nstep 3500  loss=0.0265\nstep 3600  loss=0.0297\nstep 3700  loss=0.0259\nstep 3800  loss=0.0230\nstep 3900  loss=0.0221\nstep 4000  loss=0.0221\nstep 4100  loss=0.0256\nstep 4200  loss=0.0217\nstep 4300  loss=0.0201\nstep 4400  loss=0.0247\nstep 4500  loss=0.0256\nstep 4600  loss=0.0244\nstep 4700  loss=0.0219\nstep 4800  loss=0.0208\nstep 4900  loss=0.0254\n\n\n\n\n\n    Run history:\n\n\n\nloss\n█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nstep\n▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\n\nRun summary:\n\n\n\nloss\n0.02539\n\n\nstep\n4900\n\n\n\n\n\n\n View run spring-dust-17 at: https://wandb.ai/drscotthawley/toy-factorization/runs/frmov01u View project at: https://wandb.ai/drscotthawley/toy-factorizationSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20260226_231124-frmov01u/logs",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#loss-curve",
    "href": "toy_factorization.html#loss-curve",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Loss curve",
    "text": "Loss curve\n\nplt.figure(figsize=(8, 3))\nplt.plot(losses, alpha=0.3, label='raw')\nplt.loglog(np.convolve(losses, np.ones(50)/50, mode='valid'), label='smoothed')\nplt.xlabel('step'); plt.ylabel('loss'); plt.legend(); plt.title('Factorization Loss')\nplt.tight_layout(); plt.show()",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#visualization-pitch-time-difference-vectors-separate-cleanly",
    "href": "toy_factorization.html#visualization-pitch-time-difference-vectors-separate-cleanly",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Visualization: pitch & time difference vectors separate cleanly",
    "text": "Visualization: pitch & time difference vectors separate cleanly\n\n@torch.no_grad()\ndef collect_diff_vectors(enc, n=500, sz=16, max_shift=4):\n    \"\"\"Collect difference vectors, with same-sign and opposite-sign pairs from the SAME base image.\"\"\"\n    pitch_same, pitch_opp, time_same, time_opp = [], [], [], []\n    pitch_d, time_d = [], []\n    for _ in range(n):\n        img = make_blob(sz).unsqueeze(0).to(device)\n        s1 = torch.randint(1, max_shift+1, (1,)).item()\n        s2 = torch.randint(1, max_shift+1, (1,)).item()\n        while s2 == s1: s2 = torch.randint(1, max_shift+1, (1,)).item()\n        za = enc(img)\n        for dim, same_list, opp_list, d_list in [\n            (2, pitch_same, pitch_opp, pitch_d),\n            (3, time_same, time_opp, time_d)]:\n            zp1 = enc(shift_no_wrap(img, shifts=+s1, dims=dim))\n            zp2 = enc(shift_no_wrap(img, shifts=+s2, dims=dim))\n            zn1 = enc(shift_no_wrap(img, shifts=-s1, dims=dim))\n            d1, d2, d3 = zp1 - za, zp2 - za, zn1 - za\n            same_list.append(F.cosine_similarity(d1, d2, dim=-1).item())  # same sign\n            opp_list.append(F.cosine_similarity(d1, d3, dim=-1).item())   # opposite sign\n            d_list.append(d1.squeeze().cpu())\n            d_list.append(d3.squeeze().cpu())\n    pitch_d, time_d = torch.stack(pitch_d), torch.stack(time_d)\n    cos_cross = F.cosine_similarity(pitch_d[:len(time_d)], time_d[:len(pitch_d)], dim=-1).numpy()\n    return pitch_d, time_d, np.array(pitch_same), np.array(pitch_opp), np.array(time_same), np.array(time_opp), cos_cross\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 @torch.no_grad()\n      2 def collect_diff_vectors(enc, n=500, sz=16, max_shift=4):\n      3     \"\"\"Collect difference vectors, with same-sign and opposite-sign pairs from the SAME base image.\"\"\"\n      4     pitch_same, pitch_opp, time_same, time_opp = [], [], [], []\n\nNameError: name 'torch' is not defined\n\n\n\n\npitch_d, time_d, ps, po, ts, to_, cos_cross = collect_diff_vectors(enc)\nprint(f\"Collected: pitch={pitch_d.shape}, time={time_d.shape}\")\n\nCollected: pitch=torch.Size([1000, 16]), time=torch.Size([1000, 16])\n\n\n\nfrom sklearn.decomposition import PCA\n\nall_diffs = torch.cat([pitch_d, time_d], dim=0).numpy()\nlabels = ['pitch'] * len(pitch_d) + ['time'] * len(time_d)\npca = PCA(n_components=2).fit(all_diffs)\nproj = pca.transform(all_diffs)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n# PCA scatter  \nall_diffs = torch.cat([pitch_d, time_d], dim=0).numpy()\npca = PCA(n_components=2).fit(all_diffs)\nproj = pca.transform(all_diffs)\nn = len(pitch_d)\naxes[0].scatter(proj[:n, 0], proj[:n, 1], alpha=0.3, s=10, label='pitch Δ', c='blue')\naxes[0].scatter(proj[n:, 0], proj[n:, 1], alpha=0.3, s=10, label='time Δ', c='green')\naxes[0].set_xlabel('PC1'); axes[0].set_ylabel('PC2'); axes[0].legend()\naxes[0].set_title('PCA of difference vectors'); axes[0].set_aspect('equal')\n\n# Histogram with all three distributions\nax = axes[1]\nax.hist(cos_cross, bins=40, alpha=0.5, label='pitch vs time (want ≈0)', density=True)\nax.hist(np.concatenate([ps, ts]), bins=40, alpha=0.5, label='same-sign (want ≈+1)', density=True)\nax.hist(np.concatenate([po, to_]), bins=40, alpha=0.5, label='opp-sign (want ≈−1)', density=True)\nax.set_xlabel('cosine similarity'); ax.legend(); ax.set_title('Cosine similarity distributions')\nplt.tight_layout(); plt.show()\n\nprint(f\"target=+0: mean cos={cos_cross.mean():.3f}, mean |cos|={np.abs(cos_cross).mean():.3f}\")\nprint(f\"target=+1: mean cos={np.concatenate([ps,ts]).mean():.3f}, mean |cos|={np.abs(np.concatenate([ps,ts])).mean():.3f}\")\nprint(f\"target=-1: mean cos={np.concatenate([po,to_]).mean():.3f}, mean |cos|={np.abs(np.concatenate([po,to_])).mean():.3f}\")\n\n\n\n\n\n\n\n\nMean |cos(pitch,time)|: 0.126  (ideal: 0)\nMean |cos(pitch,pitch)|: 0.883  (ideal: 1)\nMean |cos(time,time)|:  0.878  (ideal: 1)",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#results-summary",
    "href": "toy_factorization.html#results-summary",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Results summary",
    "text": "Results summary\nThe factorization objective works well. After training for 5,000 steps with batch size 1024 and lr=4e-3:\n\nCross-type cosine similarity (pitch vs time): mean |cos| = 0.116, tightly clustered around 0 ✅\nSame-sign cosine similarity: mean cos = 0.954, sharp peak near +1 ✅\nOpposite-sign cosine similarity: mean cos = −0.884, peak near −1 ✅\nPositive/negative alignment angles: pitch 162.8°, time 163.2° (ideal: 180°) ✅\n\nThe PCA of difference vectors shows a clear cross shape, confirming pitch and time directions are approximately orthogonal in the learned latent space. The remaining ~17° deviation from perfect anti-parallelism is attributable to the asymmetric zero-padding introduced by shift_no_wrap (see probing section below).\n\nProbing the ±1 asymmetry\nBoth pitch-vs-pitch and time-vs-time cosine distributions show stronger correlation on the +1 side than the −1 side. This is because shift_no_wrap introduces an asymmetry: same-sign shift pairs produce crops with zero-padding on the same side (structurally more similar), while opposite-sign pairs have padding on opposite sides (more visually different). The encoder maps the more-similar same-sign pairs more consistently, yielding a tighter +1 peak and a broader −1 peak.\n\n@torch.no_grad()\ndef collect_signed_diffs(enc, n=500, sz=16, max_shift=4):\n    \"\"\"Collect diff vectors split by sign for both pitch and time.\"\"\"\n    results = {'pitch_pos': [], 'pitch_neg': [], 'time_pos': [], 'time_neg': []}\n    for _ in range(n):\n        img = make_blob(sz).unsqueeze(0).to(device)\n        s = torch.randint(1, max_shift+1, (1,)).item()\n        za = enc(img)\n        for dim, name in [(2, 'pitch'), (3, 'time')]:\n            for sign, label in [(1, 'pos'), (-1, 'neg')]:\n                zs = enc(shift_no_wrap(img, shifts=sign*s, dims=dim))\n                results[f'{name}_{label}'].append((zs - za).squeeze().cpu())\n    return {k: torch.stack(v) for k, v in results.items()}\n\ndiffs = collect_signed_diffs(enc)\n\nfor name in ['pitch', 'time']:\n    mean_pos = diffs[f'{name}_pos'].mean(dim=0)\n    mean_neg = diffs[f'{name}_neg'].mean(dim=0)\n    cos = F.cosine_similarity(mean_pos.unsqueeze(0), mean_neg.unsqueeze(0)).item()\n    angle = np.degrees(np.arccos(np.clip(cos, -1, 1)))\n    print(f\"{name}: cos(mean_pos, mean_neg) = {cos:.3f}, angle = {angle:.1f}° (ideal: 180°)\")\n\npitch: cos(mean_pos, mean_neg) = -0.955, angle = 162.8° (ideal: 180°)\ntime: cos(mean_pos, mean_neg) = -0.957, angle = 163.2° (ideal: 180°)\n\n\n\npca.explained_variance_ratio_[:2]\n\narray([0.27807263, 0.2644205 ], dtype=float32)\n\n\n\n\nSVD on shift grids\nFor each blob, we encode a full grid of (dy, dx) shifts and reshape the resulting embedding tensor for SVD. The first singular value dominates (60–80% of total), with rapid dropoff after 2–3 components — consistent with an approximately rank-1 (tensor-product) factorization. This confirms the soft orthogonality loss pushes the representation toward factored structure without enforcing it rigidly.\n\n@torch.no_grad()\ndef grid_embeddings(enc, n_blobs=50, sz=16, max_shift=4):\n    \"\"\"Encode a grid of (dy, dx) shifts for each blob.\"\"\"\n    shifts = list(range(-max_shift, max_shift+1))  # e.g. -4..+4 = 9 values\n    all_Z = []\n    for _ in range(n_blobs):\n        img = make_blob(sz).unsqueeze(0).to(device)\n        Z = []\n        for dy in shifts:\n            for dx in shifts:\n                shifted = shift_no_wrap(shift_no_wrap(img, shifts=dy, dims=2), shifts=dx, dims=3)\n                z = enc(shifted).squeeze().cpu()\n                Z.append(z)\n        all_Z.append(torch.stack(Z).reshape(len(shifts), len(shifts), -1))\n    return torch.stack(all_Z), shifts  # (n_blobs, n_dy, n_dx, latent_dim)\n\nZ, shifts = grid_embeddings(enc)\nn_dy, n_dx, latent = Z.shape[1], Z.shape[2], Z.shape[3]\nprint(f\"Z shape: {Z.shape}  (blobs, dy, dx, latent)\")\n\n# For each blob, reshape to (n_dy, n_dx*latent) and get singular values\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nall_sv = []\nfor i in range(len(Z)):\n    M = Z[i].reshape(n_dy, n_dx * latent).numpy()\n    sv = np.linalg.svd(M, compute_uv=False)\n    sv_normed = sv / sv.sum()\n    all_sv.append(sv_normed)\n    axes[0].plot(sv_normed, alpha=0.15, color='blue')\n\nmean_sv = np.stack(all_sv).mean(0)\naxes[0].plot(mean_sv, color='red', linewidth=2, label='mean')\naxes[0].set_xlabel('singular value index'); axes[0].set_ylabel('fraction of total')\naxes[0].set_title('Singular values: Z reshaped as (pitch, time·latent)')\naxes[0].legend()\n\n# Same but transposed: (n_dx, n_dy*latent)\nall_sv_t = []\nfor i in range(len(Z)):\n    M = Z[i].permute(1, 0, 2).reshape(n_dx, n_dy * latent).numpy()\n    sv = np.linalg.svd(M, compute_uv=False)\n    sv_normed = sv / sv.sum()\n    all_sv_t.append(sv_normed)\n    axes[1].plot(sv_normed, alpha=0.15, color='green')\n\nmean_sv_t = np.stack(all_sv_t).mean(0)\naxes[1].plot(mean_sv_t, color='red', linewidth=2, label='mean')\naxes[1].set_xlabel('singular value index'); axes[1].set_ylabel('fraction of total')\naxes[1].set_title('Singular values: Z reshaped as (time, pitch·latent)')\naxes[1].legend()\n\nplt.tight_layout(); plt.show()\n\nZ shape: torch.Size([50, 9, 9, 16])  (blobs, dy, dx, latent)",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#deeper-analysis-kernel-pca-and-svd",
    "href": "toy_factorization.html#deeper-analysis-kernel-pca-and-svd",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Deeper analysis: Kernel PCA and SVD",
    "text": "Deeper analysis: Kernel PCA and SVD\nThe cosine histograms confirm that the soft loss achieves the desired geometric relationships. But does the latent space admit a tensor-product structure — i.e., can embeddings be decomposed into pitch ⊗ time components?\nWe use two complementary analyses:\n\nKernel PCA with an RBF kernel (inspired by Albinet’s (2026) connection between soft constraints and eigenstructure via kernel methods). The RBF kernel can reveal nonlinear factored structure that linear PCA would miss. Results show that the first two kernel principal components cleanly separate pitch and time with orthogonal gradients, while kPC3–4 contain no factor-related structure — confirming a 2D factored subspace.\nSVD on a grid of shift embeddings: For each blob, we encode a full grid of (pitch, time) shifts and reshape as a matrix. The first singular value captures 60–80% of the total energy, with most of the remainder in just 2–3 components — indicating approximately low-rank (near tensor-product) structure in the learned embeddings.\n\n\n# kernel pca \n\nfrom sklearn.decomposition import KernelPCA\n\n@torch.no_grad()\ndef kernel_pca_analysis(enc, n_blobs=200, sz=16, max_shift=4, gamma=1.0):\n    \"\"\"Encode shifted blobs and apply kernel PCA to reveal nonlinear structure.\"\"\"\n    shifts = list(range(-max_shift, max_shift+1))\n    embeddings, dy_labels, dx_labels = [], [], []\n    for _ in range(n_blobs):\n        img = make_blob(sz).unsqueeze(0).to(device)\n        for dy in shifts:\n            for dx in shifts:\n                shifted = shift_no_wrap(shift_no_wrap(img, shifts=dy, dims=2), shifts=dx, dims=3)\n                z = enc(shifted).squeeze().cpu().numpy()\n                embeddings.append(z)\n                dy_labels.append(dy)\n                dx_labels.append(dx)\n    X = np.stack(embeddings)\n    dy_labels, dx_labels = np.array(dy_labels), np.array(dx_labels)\n\n    kpca = KernelPCA(n_components=4, kernel='rbf', gamma=gamma)\n    Z = kpca.fit_transform(X)\n\n    fig, axs = plt.subplots(2, 2, figsize=(8, 7))\n    axes = axs.flatten()\n    sc0 = axes[0].scatter(Z[:, 0], Z[:, 1], c=dy_labels, cmap='coolwarm', s=2, alpha=0.3)\n    axes[0].set_xlabel('kPC1'); axes[0].set_ylabel('kPC2')\n    axes[0].set_title('Colored by pitch shift (dy)')\n    plt.colorbar(sc0, ax=axes[0])\n\n    sc1 = axes[1].scatter(Z[:, 0], Z[:, 1], c=dx_labels, cmap='PiYG', s=2, alpha=0.3)\n    axes[1].set_xlabel('kPC1'); axes[1].set_ylabel('kPC2')\n    axes[1].set_title('Colored by time shift (dx)')\n    plt.colorbar(sc1, ax=axes[1])\n\n    sc2 = axes[2].scatter(Z[:, 2], Z[:, 3], c=dy_labels, cmap='coolwarm', s=2, alpha=0.3)\n    axes[2].set_xlabel('kPC3'); axes[2].set_ylabel('kPC4')\n    axes[2].set_title('kPC3-4, colored by pitch')\n    plt.colorbar(sc2, ax=axes[2])\n\n    sc3 = axes[3].scatter(Z[:, 2], Z[:, 3], c=dx_labels, cmap='PiYG', s=2, alpha=0.3)\n    axes[3].set_xlabel('kPC3'); axes[2].set_ylabel('kPC4')\n    axes[3].set_title('kPC3-4, colored by time')\n    plt.colorbar(sc3, ax=axes[3])\n\n    plt.tight_layout(); plt.show()\n    return kpca, Z, dy_labels, dx_labels\n\nkpca, Z, dy_lab, dx_lab = kernel_pca_analysis(enc, gamma=1.0)\n\n\n\n\n\n\n\n\n\n!uv pip install umap-learn\n\nUsing Python 3.10.12 environment at: /home/shawley/envs/midi-rae\nResolved 10 packages in 366ms                                        \nPrepared 2 packages in 38ms                                              \nInstalled 2 packages in 2ms                                 \n + pynndescent==0.6.0\n + umap-learn==0.5.11",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#does-umap-show-us-anything",
    "href": "toy_factorization.html#does-umap-show-us-anything",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Does UMAP Show Us Anything?",
    "text": "Does UMAP Show Us Anything?\nIf the nonlinear PCA with the RBF kernel is showing a clean separation, does that mean that there’s a two-dimensional subspace manifold at work? Maybe UMAP can discover this manifold and show us something interesting.\nSpoiler: No, This is pretty much a jumbled mess because the UMAP focuses on the local structure and twists everything around:\n\n# Umap \nimport umap\n\n@torch.no_grad()\ndef umap_analysis(enc, n_blobs=200, sz=16, max_shift=4):\n    shifts = list(range(-max_shift, max_shift+1))\n    embeddings, dy_labels, dx_labels = [], [], []\n    for _ in range(n_blobs):\n        img = make_blob(sz).unsqueeze(0).to(device)\n        for dy in shifts:\n            for dx in shifts:\n                shifted = shift_no_wrap(shift_no_wrap(img, shifts=dy, dims=2), shifts=dx, dims=3)\n                z = enc(shifted).squeeze().cpu().numpy()\n                embeddings.append(z)\n                dy_labels.append(dy)\n                dx_labels.append(dx)\n    X = np.stack(embeddings)\n    dy_labels, dx_labels = np.array(dy_labels), np.array(dx_labels)\n\n    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n    Z = reducer.fit_transform(X)\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    sc0 = axes[0].scatter(Z[:, 0], Z[:, 1], c=dy_labels, cmap='coolwarm', s=2, alpha=0.3)\n    axes[0].set_title('UMAP colored by pitch shift (dy)')\n    plt.colorbar(sc0, ax=axes[0])\n\n    sc1 = axes[1].scatter(Z[:, 0], Z[:, 1], c=dx_labels, cmap='PiYG', s=2, alpha=0.3)\n    axes[1].set_title('UMAP colored by time shift (dx)')\n    plt.colorbar(sc1, ax=axes[1])\n\n    plt.tight_layout(); plt.show()\n    return reducer, Z, dy_labels, dx_labels\n\nreducer, Z_umap, dy_lab, dx_lab = umap_analysis(enc)\n\n/home/shawley/envs/midi-rae/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\n\n\n\n\n\n\n\n\n\nWhat if we went to 3D instead of 2D? Would it let us see more?\nAnswer: not really.\n\nimport plotly.graph_objects as go\n\n@torch.no_grad()\ndef umap_3d_analysis(enc, n_blobs=200, sz=16, max_shift=4):\n    shifts = list(range(-max_shift, max_shift+1))\n    embeddings, dy_labels, dx_labels = [], [], []\n    for _ in range(n_blobs):\n        img = make_blob(sz).unsqueeze(0).to(device)\n        for dy in shifts:\n            for dx in shifts:\n                shifted = shift_no_wrap(shift_no_wrap(img, shifts=dy, dims=2), shifts=dx, dims=3)\n                z = enc(shifted).squeeze().cpu().numpy()\n                embeddings.append(z)\n                dy_labels.append(dy)\n                dx_labels.append(dx)\n    X = np.stack(embeddings)\n    dy_labels, dx_labels = np.array(dy_labels), np.array(dx_labels)\n\n    reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)\n    coords = reducer.fit_transform(X)\n\n    fig_pitch = go.Figure(data=[go.Scatter3d(\n        x=coords[:,0], y=coords[:,1], z=coords[:,2],\n        mode='markers',\n        marker=dict(size=2, color=dy_labels, colorscale='RdBu', opacity=0.5,\n                    colorbar=dict(title='dy')),\n        hovertext=[f\"dy={dy}, dx={dx}\" for dy, dx in zip(dy_labels, dx_labels)],\n        hoverinfo='x+y+z+text'\n    )])\n    fig_pitch.update_layout(title='UMAP 3D — colored by pitch (dy)', margin=dict(l=0, r=0, b=0, t=30))\n\n    fig_time = go.Figure(data=[go.Scatter3d(\n        x=coords[:,0], y=coords[:,1], z=coords[:,2],\n        mode='markers',\n        marker=dict(size=2, color=dx_labels, colorscale='PiYG', opacity=0.5,\n                    colorbar=dict(title='dx')),\n        hovertext=[f\"dy={dy}, dx={dx}\" for dy, dx in zip(dy_labels, dx_labels)],\n        hoverinfo='x+y+z+text'\n    )])\n    fig_time.update_layout(title='UMAP 3D — colored by time (dx)', margin=dict(l=0, r=0, b=0, t=30))\n\n    fig_pitch.show()\n    fig_time.show()\n    return coords, dy_labels, dx_labels\n\ncoords, dy_lab, dx_lab = umap_3d_analysis(enc)\n\n/home/shawley/envs/midi-rae/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#discussion",
    "href": "toy_factorization.html#discussion",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "Discussion",
    "text": "Discussion\nSo where are we going with this? The idea that factorization in pitch and time is useful but not the complete picture seems to be borne out and supported by the soft factorization method, which would allow us some level of interpretable controls or structure without forcing it in a detrimental way.\nThe fact that our augmentation scheme allows us to explicitly label the degree to which we’re Performing the augmentations is a key aspect of this method, i.e., if you were just doing other types of augmentations, you might not be able to pull this off.\nI’ll mention that when Christian Steinmetz and I looked at directions for audio effects parameters in latent space (Hawley & Steinmetz, 2023), it was….okay, but ultimately kind of disappointing. Because CLAP embeddings suck. Anyway, this work is kind of on the continuum with that. And the OpLaS paper for letting the latent space encode transformations geometrically.\n“Selling Points”: * I like flexibility of the implementation. It doesn’t require any architecture changes at all as it would if you were going to try to do the factorization by construction. You just add it as a loss term. * Furthermore, it’s tunable. You can sort of weight how strongly you want the factorization thing to take hold just by tuning a regularization parameter in the overall loss term. That’s got to be worth some attention from the machine learning community, right? LOL",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "toy_factorization.html#references",
    "href": "toy_factorization.html#references",
    "title": "Toy “Soft” Factorization Experiment",
    "section": "References",
    "text": "References\nInspiration: Franck’s series, Soft constraints and kernel methods: - F. Albinet, “(Part 1) SVD Through Variational Principles: From Geometry to the Classical Formulation,” blog post, Jan 20, 2026. https://fr.anckalbi.net/posts/svd-geometry-to-variational/ - F. Albinet, “(Part 2) Ridges and Thalwegs: The Landscape of Competing Forces — PCA through the LS-SVM lens, preparing the path to SVD and beyond,” blog post, Feb. 20, 2026. https://fr.anckalbi.net/posts/ls-svm-kernel-pca/\nMy Prior Work on Latent Encoding of Data Transformations, Translations, Rotations, Etc: - S.H. Hawley & A. Tackett, “Operational Latent Spaces (OpLaS),” AES International Symposium on AI and the Musician, https://drscotthawley.github.io/oplas/ - S.H. Hawley & C. J. Steinmetz, “Leveraging Neural Representations for Audio Manipulation,” JAES / AES Europe 2023. https://arxiv.org/abs/2304.04394\nFactored representations in symbolic music: - X. Wu, Z. Huang, K. Zhang, J. Yu, X. Tan, T. Zhang, Y. Li, Z. Wang, and L. Sun, “MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation,” arXiv:2309.10738, September 2023. - K. Chen, G. Xia, and S. Dubnov, “Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm,” arXiv:2008.01291, 2020. - H. Liang, T. Mu, and R. Cai, “PiRhDy: Learning Pitch-, Rhythm-, and Dynamics-aware Embeddings for Symbolic Music,” arXiv:2010.08091, 2020. - Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, X. Gu, and G. Xia, “MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation,” in Proc. ISMIR, 2021.\nMotif discovery and pattern repetition: - D. Meredith, K. Lemström, and G. A. Wiggins, “Algorithms for discovering repeated patterns in multidimensional representations of polyphonic music,” Journal of New Music Research, vol. 31, no. 4, pp. 321–345, 2002. - D. Meredith, “COSIATEC and SIATECCompress: Pattern discovery by geometric compression,” in MIREX, Curitiba, Brazil, 2013. - E. Cambouropoulos, M. Crochemore, C. Iliopoulos, L. Mouchard, and Y. Pinzon, “Algorithms for computing approximate repetitions in musical sequences,” International Journal of Computer Mathematics, vol. 79, no. 11, pp. 1135–1148, 2002. - C.-C. M. Yeh, Y. Zhu, L. Ulanova, N. Begum, Y. Ding, H. A. Dau, D. F. Silva, A. Mueen, and E. Keogh, “Matrix Profile I: All pairs similarity joins for time series,” in Proc. IEEE ICDM, Barcelona, Spain, 2016, pp. 1317–1322. - “SIATEC-C: Computationally Efficient Repeated Pattern Discovery in Polyphonic Music,” in Proc. ISMIR, 2022. https://archives.ismir.net/ismir2022/paper/000006.pdf\nDatasets: (The main code so far is only using POP909). - Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, X. Gu, and G. Xia, “POP909: A Pop-song Dataset for Music Arrangement Generation,” in Proc. 21st International Society for Music Information Retrieval Conference (ISMIR), Montréal, Canada, 2020, pp. 38–45. - “BPS-Motif: A Dataset for Repeated Pattern Discovery of Polyphonic Symbolic Music,” in Proc. ISMIR, 2023. https://archives.ismir.net/ismir2023/paper/000032.pdf\nRepresentation AutoEncoder (RAE) & more: - B. Zheng, N. Ma, S. Tong, and S. Xie, “Diffusion Transformers with Representation Autoencoders,” arXiv:2510.11690, 2025. - K. Didi, “The unification of representation learning and generative modelling,” https://kdidi.netlify.app/blog/ml/2025-12-31-r4g/, Dec 31, 2025.\nLeJEPA: - R. Balestriero and Y. LeCun, “LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics”, Nov. 2025, https://arxiv.org/abs/2511.08544\nFactored representations in transformers: - A. Shai, L. Amdahl-Culleton, C. L. Christensen, H. R. Bigelow, F. E. Rosas, A. B. Boyd, E. A. Alt, K. J. Ray, and P. M. Riechers, “Transformers learn factored representations,” arXiv:2602.02385, February 2026.\nFactored word embeddings: - J. Pennington, R. Socher, and C. D. Manning, “GloVe: Global Vectors for Word Representation,” in Proc. EMNLP, 2014, pp. 1532–1543. https://nlp.stanford.edu/pubs/glove.pdf\nDisentangled and geometric representations: - E. Härkönen, A. Hertzmann, J. Lehtinen, and S. Paris, “GANSpace: Discovering Interpretable GAN Controls,” in Proc. NeurIPS, 2020. - I. Higgins et al., “beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,” in Proc. ICLR, 2017. - G. Velarde, D. Meredith, and T. Weyde, “A wavelet-based approach to pattern discovery in melodies,” Springer Cham, 2016, pp. 303–333.",
    "crumbs": [
      "Toy \"Soft\" Factorization Experiment"
    ]
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "losses",
    "section": "",
    "text": "Turns out zero element tensors will yield NaN when you try to run .mean(), so…\n\n\n\n\ndef safe_mean(\n    t, dim:NoneType=None\n):\n\nsafe replacement for torch.mean( ). can’t be used as a suffix though",
    "crumbs": [
      "losses"
    ]
  },
  {
    "objectID": "losses.html#safe-mean",
    "href": "losses.html#safe-mean",
    "title": "losses",
    "section": "",
    "text": "Turns out zero element tensors will yield NaN when you try to run .mean(), so…\n\n\n\n\ndef safe_mean(\n    t, dim:NoneType=None\n):\n\nsafe replacement for torch.mean( ). can’t be used as a suffix though",
    "crumbs": [
      "losses"
    ]
  },
  {
    "objectID": "losses.html#lejepa-loss",
    "href": "losses.html#lejepa-loss",
    "title": "losses",
    "section": "LeJEPA Loss",
    "text": "LeJEPA Loss\nFor an interactive overview of LeJEPA, see https://www.scotthawley.com/ssltoy/\n\n\nSIGReg\n\ndef SIGReg(\n    x, global_step, num_slices:int=256\n):\n\nSIGReg with Epps-Pulley statistic. x is (N, K) tensor.\n\n# Test SIGReg with random embeddings\nbatch_size, embed_dim = 32, 64\nx = torch.randn(batch_size, embed_dim)\nloss = SIGReg(x, global_step=0, num_slices=256)\nprint(f\"SIGReg loss: {loss.item():.4f}\")\n\nSIGReg loss: 2.9035\n\n\n\n\n\nattraction_loss\n\ndef attraction_loss(\n    z1, z2, # embeddings of two \"views\" of the same thing (in batches)\n    deltas:NoneType=None, # optional/TBD: info on semantic 'distance' between z1 & z2\n    tau:float=100.0, # inverse strength of fall-off for delta distances, big=slower\n):\n\nHow we pull similar ‘views’ together\n\n\n\nLeJEPA\n\ndef LeJEPA(\n    z1, z2, global_step, lambd:float=0.5, deltas:NoneType=None\n):\n\nMain LeJEPA loss function\n\n# Test LeJEPA loss\nbatch_size, embed_dim = 32, 64\nz1 = torch.randn(batch_size, embed_dim)\nz2 = torch.randn(batch_size, embed_dim)\n\nloss = LeJEPA(z1, z2, global_step=0, lambd=0.5)\nprint(f\"LeJEPA loss: {loss['loss'].item():.4f}\")\nprint(f\"  Attraction: {attraction_loss(z1, z2).item():.4f}\")\nprint(f\"  SIGReg: {SIGReg(torch.cat((z1, z2), dim=0), global_step=0).item():.4f}\")\n\nLeJEPA loss: 1.8238\n  Attraction: 2.0526\n  SIGReg: 1.5950",
    "crumbs": [
      "losses"
    ]
  },
  {
    "objectID": "losses.html#masked-autoencoder-loss",
    "href": "losses.html#masked-autoencoder-loss",
    "title": "losses",
    "section": "Masked (Auto)Encoder Loss",
    "text": "Masked (Auto)Encoder Loss\n\n\ncalc_mae_loss\n\ndef calc_mae_loss(\n    recon_patches, img, enc_out, lambda_visible:float=0.1\n):\n\nBCE loss on reconstructed patches, with optional downweighting of visible patches",
    "crumbs": [
      "losses"
    ]
  },
  {
    "objectID": "losses.html#full-encoder-loss",
    "href": "losses.html#full-encoder-loss",
    "title": "losses",
    "section": "Full Encoder Loss",
    "text": "Full Encoder Loss\n\n\nanchor_loss\n\ndef anchor_loss(\n    z1, z2\n):\n\nAnchor embeddings of empty patches to the origin\n\n\n\ncalc_enc_loss\n\ndef calc_enc_loss(\n    z1, z2, global_step, deltas:NoneType=None, lambd:float=0.5, non_emptys:tuple=(None, None),\n    lambda_anchor:float=0.05\n):\n\nMain loss function for Encoder\n\n\n\ncalc_enc_loss_multiscale\n\ndef calc_enc_loss_multiscale(\n    z1, z2, # lists of embeddings at each swin hierarchy level, 0=coarsest, -1=finest\n    global_step, img_size, deltas:NoneType=None, lambd:float=0.5, non_emptys:NoneType=None, lambda_anchor:float=0.05\n):\n\nCompute encoder loss at each hierarchy level, weighted by patch overlap fraction. Levels where delta/shift exceeds patch size (resulting in zero overlap) are skipped entirely. Non-empty patch masks focus the sim/anchor losses on musically active regions.",
    "crumbs": [
      "losses"
    ]
  },
  {
    "objectID": "losses.html#adversarial-loss",
    "href": "losses.html#adversarial-loss",
    "title": "losses",
    "section": "Adversarial Loss",
    "text": "Adversarial Loss\n\n\nPatchGANDiscriminator\n\ndef PatchGANDiscriminator(\n    in_ch:int=1, base_ch:int=64, n_layers:int=3, use_spectral_norm:bool=True\n):\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "losses"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "set_seed\n\ndef set_seed(\n    seed:int=42, deterministic:bool=False\n):\n\nSet all random seeds for reproducibility\n\n\n\nsave_checkpoint\n\ndef save_checkpoint(\n    model, epoch, val_loss, cfg, optimizer:NoneType=None, save_every:int=25, n_keep:int=5, verbose:bool=True,\n    tag:str=''\n):\n\nSaves new checkpoint, keeps best & the most recent n_keep. Can loop over multiple models. (Saves separate files for each model)\n\n\n\nload_checkpoint\n\ndef load_checkpoint(\n    model, ckpt_path:str, return_all:bool=False, weights_only:bool=False, strict:bool=False\n):\n\nloads a model (and maybe other things) from a checkpoint file",
    "crumbs": [
      "utils"
    ]
  }
]